{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc5d004-5e6f-4bb3-938e-fb44f919cec3",
   "metadata": {},
   "source": [
    "# `nanoGPT`: GPT-2 Small (125M Params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd994b32-ca5f-4e00-81d3-89dbbf7b2093",
   "metadata": {},
   "source": [
    "## Install / Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c38d27-5acb-4e5d-87c8-dbb2739cf277",
   "metadata": {},
   "source": [
    "### First Time Running\n",
    "\n",
    "We need to install `ngpt` and setup the Shakespeare dataset\n",
    "\n",
    "This will need to be ran the first time you are running this notebook.\n",
    "\n",
    "Following the\n",
    "\n",
    "```python\n",
    "!python3 -m pip install nanoGPT\n",
    "```\n",
    "\n",
    "you will need to restart your runtime (Runtime -> Restart runtime)\n",
    "\n",
    "After this, you should be able to\n",
    "\n",
    "```python\n",
    ">>> import ngpt\n",
    ">>> ngpt.__file__\n",
    "'/content/nanoGPT/src/ngpt/__init__.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed59c1f-e6c9-4222-bded-5a32e4505ee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T01:41:52.728352Z",
     "iopub.status.busy": "2023-11-30T01:41:52.728214Z",
     "iopub.status.idle": "2023-11-30T01:41:52.860380Z",
     "shell.execute_reply": "2023-11-30T01:41:52.859876Z",
     "shell.execute_reply.started": "2023-11-30T01:41:52.728334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\n",
      "Has ngpt installed. Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 -c 'import ngpt; print(ngpt.__file__)' 2> '/dev/null'\n",
    "\n",
    "if [[ $? -eq 0 ]]; then\n",
    "    echo \"Has ngpt installed. Nothing to do.\"\n",
    "else\n",
    "    echo \"Does not have ngpt installed. Installing...\"\n",
    "    git clone 'https://github.com/saforem2/nanoGPT'\n",
    "    python3 nanoGPT/data/shakespeare_char/prepare.py\n",
    "    python3 -m pip install -e nanoGPT -vvv\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a8da7-72fe-4839-a14d-f01606285fc3",
   "metadata": {},
   "source": [
    "## Post Install\n",
    "\n",
    "If installed correctly, you should be able to:\n",
    "\n",
    "```python\n",
    ">>> import ngpt\n",
    ">>> ngpt.__file__\n",
    "'/path/to/nanoGPT/src/ngpt/__init__.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbf22d1-34ba-48ae-a78e-fc447fc9a0de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T01:41:53.288829Z",
     "iopub.status.busy": "2023-11-30T01:41:53.288553Z",
     "iopub.status.idle": "2023-11-30T01:41:53.370052Z",
     "shell.execute_reply": "2023-11-30T01:41:53.369574Z",
     "shell.execute_reply.started": "2023-11-30T01:41:53.288811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:41:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119m3434626787.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m7\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35m__init__.py\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ngpt\n",
    "from enrich import get_logger\n",
    "log = get_logger('jupyter')\n",
    "log.info(ngpt.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329ed94-582e-4aa8-bbe0-ba56782fe9e9",
   "metadata": {},
   "source": [
    "## Build Trainer\n",
    "\n",
    "Explicitly, we:\n",
    "\n",
    "1. `setup_torch(...)`\n",
    "2. Build `cfg: DictConfig = get_config(...)`\n",
    "3. Instnatiate `config: ExperimentConfig = instantiate(cfg)`\n",
    "4. Build `trainer = Trainer(config)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d12405a-8f88-476b-8922-a1a212adc682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T01:41:54.701382Z",
     "iopub.status.busy": "2023-11-30T01:41:54.701087Z",
     "iopub.status.idle": "2023-11-30T01:42:12.803181Z",
     "shell.execute_reply": "2023-11-30T01:42:12.801998Z",
     "shell.execute_reply.started": "2023-11-30T01:41:54.701364Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "WARNING: There was an error initializing an OpenFabrics device.\n",
      "\n",
      "  Local host:   thetagpu24\n",
      "  Local device: mlx5_0\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download font: Source Sans Pro, skipping!\n",
      "Failed to download font: Titillium WebRoboto Condensed, skipping!\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:01]\u001b[0m\u001b[33m[WARNING]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m297\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - No meta.pkl found, assuming GPT-\u001b[35m2\u001b[0m encodings\u001b[33m...\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m263\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Rescaling GAS -> GAS \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m WORLD_SIZE = \u001b[35m1\u001b[0m \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m \u001b[35m1\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m398\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Tokens per iteration: \u001b[35m12\u001b[0m,\u001b[35m288\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m430\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Using \u001b[1m<\u001b[0m\u001b[1;95mtorch.amp.autocast_mode.autocast\u001b[0m\u001b[39m object at \u001b[0m\u001b[35m0x7f6e4c363550\u001b[0m\u001b[1m>\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m187\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing from OpenAI GPT-\u001b[35m2\u001b[0m Weights: gpt2\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:02]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m225\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - loading weights from pretrained gpt: gpt2\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:02]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m234\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - forcing \u001b[3;94mvocab_size\u001b[0m=\u001b[35m50257\u001b[0m, \u001b[3;94mblock_size\u001b[0m=\u001b[35m1024\u001b[0m, \u001b[3;94mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:02]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m240\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - overriding dropout rate to \u001b[35m0.0\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:05]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m160\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - number of parameters: \u001b[35m123.\u001b[0m65M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca96dd438ad440cb2b753b7def00d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf79689e15094fa0a79850cdfa1d961f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10f628fc5a74a2888e0ca2c40a4bfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:10]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m290\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num decayed parameter tensors: \u001b[35m50\u001b[0m, with \u001b[35m124\u001b[0m,\u001b[35m318\u001b[0m,\u001b[35m464\u001b[0m parameters\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:10]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m291\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num non-decayed parameter tensors: \u001b[35m98\u001b[0m, with \u001b[35m121\u001b[0m,\u001b[35m344\u001b[0m parameters\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:10]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m297\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - using fused AdamW: \u001b[3;92mTrue\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ezpz import setup_torch\n",
    "from hydra.utils import instantiate\n",
    "from ngpt.configs import get_config\n",
    "from ngpt.trainer import Trainer\n",
    "\n",
    "os.environ['MASTER_PORT'] = '4235'\n",
    "rank = setup_torch('DDP', seed=1234)\n",
    "cfg = get_config(\n",
    "    [\n",
    "        'data=owt',              # open web text\n",
    "        'model=gpt2',            # gpt2 arch.\n",
    "        'optimizer=gpt2',\n",
    "        'train=gpt2',\n",
    "        'train.init_from=gpt2',  # init from GPT2\n",
    "        'train.max_iters=1000',\n",
    "        'train.dtype=bfloat16',\n",
    "    ]\n",
    ")\n",
    "config = instantiate(cfg)\n",
    "trainer = Trainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20e9355-fbcf-489e-afa1-f744486aec69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T01:42:17.013220Z",
     "iopub.status.busy": "2023-11-30T01:42:17.012552Z",
     "iopub.status.idle": "2023-11-30T01:42:25.205234Z",
     "shell.execute_reply": "2023-11-30T01:42:25.204771Z",
     "shell.execute_reply.started": "2023-11-30T01:42:17.013143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:25]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119m1657463709.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m3\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[1m[\u001b[0m\u001b[32m'prompt'\u001b[0m\u001b[1m]\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mquery\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:42:25]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119m1657463709.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m4\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[1m[\u001b[0m\u001b[32m'response'\u001b[0m\u001b[1m]\u001b[0m:\n",
      "\n",
      "What is a supercomputer? When did you first learn it?\n",
      "\n",
      "I used to work in an Apple Computer. It was called the \u001b[32m\"Elgin\"\u001b[0m Computer. It was the first computer that I had seen on TV. I went to college in \u001b[35m1983\u001b[0m. I was at Arizona State University and I studied computer science. I later joined a computer science program at MIT. But my first computer was the Intel Core. It was from \u001b[35m1986\u001b[0m. I went to MIT where I got my PhD and was at the lab. When I did graduate school in \u001b[35m1987\u001b[0m, I went to Stanford where I made a few jobs.\n",
      "\n",
      "Did you ever think of programming as the \u001b[32m\"new media?\"\u001b[0m Did you ever think of learning about computers as the \u001b[32m\"new medium?\"\u001b[0m\n",
      "\n",
      "The world was changing. I started to think about the Internet as a new medium. I thought of computers as different from television and movies. My research led me to the Computer World. Where did you get that idea? It was a computer science book for people who were interested only in mathematics. I did research on the Internet. I went back to the Computer World which led me to programming.\n",
      "\n",
      "Do you remember talking to people that you know about how computers work?\n",
      "\n",
      "No, I don't think I did. I did not know what the concept of computers was actually like. For example, I hadn't taught programming to students in college. I was studying computer science. I never taught programming to any students. I was going to work on work on my computer.\n",
      "\n",
      "For those of you who were there, your own computers were what started the revolution. What were your first computers like?\n",
      "\n",
      "The first computer was a little bit like a calculator. It was very simple. You could use a calculator to count values. The following was just one number: \u001b[35m10\u001b[0m*\u001b[35m10\u001b[0m. You could use it to calculate your daily expenses and you could use it to write down what your income would be based on. It was all very simple.\n",
      "\n",
      "There's a very simple history of the Internet. Computers are not just about computers. They are a very important part of the world today. I am sure you have all your documents saved. We all have our personal computers. One computer, I remember, was a computer called Microsoft Word. Some of you remember it as Microsoft Word. I can't say anything about it. I'm not going to say much more. But I remember reading about the word Microsoft Word. It was all very\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a supercomputer?\"\n",
    "outputs = trainer.evaluate(query, num_samples=1, display=False)\n",
    "log.info(\"['prompt']: '{query}'\")\n",
    "log.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5202ff3-8811-47c9-8b9d-9818d4603697",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Legend:\n",
    "\n",
    "<div style=\"text-align:left;\">\n",
    "\n",
    "|  **NAME**  |     **DESCRIPTION**          |\n",
    "|:----------:|:----------------------------:|\n",
    "|   `step`   | Current training step        |\n",
    "|   `loss`   | Loss value                   |\n",
    "|   `dt`     | Time per step (in **ms**)    |\n",
    "|   `sps`    | Samples per second           |\n",
    "|   `mtps`   | (million) Tokens per sec     |\n",
    "|   `mfu`    | Model Flops Utilization*     |\n",
    "\n",
    "*in units of A100 `bfloat16` peak FLOPS\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb8d33b-f8c9-41c6-aa61-95bdb76bf6ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T01:42:59.863828Z",
     "iopub.status.busy": "2023-11-30T01:42:59.863360Z",
     "iopub.status.idle": "2023-11-30T01:47:48.785172Z",
     "shell.execute_reply": "2023-11-30T01:47:48.784615Z",
     "shell.execute_reply.started": "2023-11-30T01:42:59.863809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465a0095309147debcb391691ec7344e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-29 19:43:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m100\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.119\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m254\u001b[0m\u001b[35m.990\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.922\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.048\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.208\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.126\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.102\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:44:20]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m200\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m2\u001b[0m\u001b[35m.961\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m228\u001b[0m\u001b[35m.112\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.384\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.054\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.364\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.126\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.102\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:44:46]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m300\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.022\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m241\u001b[0m\u001b[35m.652\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.138\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.051\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.421\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.126\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.102\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:45:12]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m400\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.002\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m231\u001b[0m\u001b[35m.064\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.328\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.053\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.537\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.126\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.102\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:45:37]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.004\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m253\u001b[0m\u001b[35m.217\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.949\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.049\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.513\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.126\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.102\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:46:03]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m600\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.071\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m239\u001b[0m\u001b[35m.040\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.183\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.051\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.571\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.126\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.102\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:46:30]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m700\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.118\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m221\u001b[0m\u001b[35m.668\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.511\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.055\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.733\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.126\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.102\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:46:56]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m800\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.218\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m248\u001b[0m\u001b[35m.641\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.022\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.049\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.715\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.126\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.102\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:47:22]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m900\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m2\u001b[0m\u001b[35m.736\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m283\u001b[0m\u001b[35m.214\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.531\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.043\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.532\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.126\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.102\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:47:48]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.201\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m270\u001b[0m\u001b[35m.756\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.693\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.045\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.423\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.126\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m3\u001b[0m\u001b[35m.102\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c0c5a-539a-462d-a9e3-0655e8e48d85",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c9350c9-6303-46bd-bcb5-70020b1d5564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T01:49:00.844745Z",
     "iopub.status.busy": "2023-11-30T01:49:00.844466Z",
     "iopub.status.idle": "2023-11-30T01:49:10.004445Z",
     "shell.execute_reply": "2023-11-30T01:49:10.002623Z",
     "shell.execute_reply.started": "2023-11-30T01:49:00.844722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-29 19:49:09]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119m1657463709.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m3\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[1m[\u001b[0m\u001b[32m'prompt'\u001b[0m\u001b[1m]\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mquery\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-29 19:49:09]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119m1657463709.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m4\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[1m[\u001b[0m\u001b[32m'response'\u001b[0m\u001b[1m]\u001b[0m:\n",
      "\n",
      "What is a supercomputer?\n",
      "\n",
      "Researchers at MIT and EPFL and the University of Southern California published a paper on July \u001b[35m17\u001b[0m in IEEE Translational Computer Graphics. The paper, titled \u001b[32m\"Processes for Supercomputers,\"\u001b[0m \u001b[32m\"is particularly interesting because it shows how computer graphics can be used with supercomputers.\"\u001b[0m\n",
      "\n",
      "Supercomputers solve many of the processing needs of large graphical full-screen displays, and there is always room to improve one's graphics. Several supercomputers have been used to prototype computer designs, and \u001b[32m\"this is one of the first supercomputers to try and develop a very high-level, powerful GPU,\"\u001b[0m says Daniel Pape. Pape and his team \u001b[32m\"are working on a GPU implementation\"\u001b[0m of processes, he says.\n",
      "\n",
      "The paper explains how supercomputers program computing using the language of higher-order models, but there is \u001b[32m\"no formal description\"\u001b[0m for what those models are. \u001b[32m\"In the computer graphics that we know, these are really very good computers working with very large screens, creating many millions of graphics and other graphics cards,\"\u001b[0m Pape says.\n",
      "\n",
      "\u001b[32m\"People are very interested in exploring the computational properties of these supercomputers because, at the moment they are not really capable of doing anything,\"\u001b[0m says Pape. \u001b[32m\"If we can develop one to a much larger scale, it could make a huge difference. If we can produce a much bigger graphics display, how could we create a lot more graphics?\"\u001b[0m\n",
      "\n",
      "Pape and his team are working on just one model, but he views the GPU model as another piece of the puzzle. \u001b[32m\"It's not only a computer, it's a big model,\"\u001b[0m he says. \u001b[32m\"It's a computer that uses the best possible architecture of the graphics.\"\u001b[0m\n",
      "\n",
      "The full paper can be found online.\u001b[1m<\u001b[0m\u001b[1;95m|endoftext|\u001b[0m\u001b[1m>\u001b[0mRepublican candidate Ted Cruz says he can cut taxes on the wealthy.\n",
      "\n",
      "In an interview with Fox News’s “Sunday Morning,” Sen. Ted Cruz said he has plans to cut taxes on the wealthy.\n",
      "\n",
      "“People you just speak to and you know, they say the tax code is unfair,” the Republican candidate said in the interview. “They say, ‘How much is it going to be?’ They say the percentage of people who’ve assets is going to be taxed.”\n",
      "\n",
      "Cruz said in the interview that he does not believe taxpayers are “in the position of their money.”\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a supercomputer?\"\n",
    "outputs = trainer.evaluate(query, num_samples=1, display=False)\n",
    "log.info(\"['prompt']: '{query}'\")\n",
    "log.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
