[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nanoGPT",
    "section": "",
    "text": "A minimal LLM implementation for research and education.\n\n\n\n\n\n\n\n\n\n\nLast Updated: 11/15/2023 @ 21:09:26\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2023,\n  author = {Foreman, Sam},\n  title = {nanoGPT},\n  date = {2023-11-15},\n  url = {https://saforem2.github.io/nanoGPT},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2023. “nanoGPT.” November 15, 2023. https://saforem2.github.io/nanoGPT."
  },
  {
    "objectID": "index.html#fa-solid-hourglass-end-last-updated",
    "href": "index.html#fa-solid-hourglass-end-last-updated",
    "title": "nanoGPT",
    "section": " Last Updated",
    "text": "Last Updated\n\n\nLast Updated: 11/15/2023 @ 16:34:29"
  },
  {
    "objectID": "quarto/ngpt-shakespeare.html",
    "href": "quarto/ngpt-shakespeare.html",
    "title": "nanoGPT",
    "section": "",
    "text": "nanoGPT\nInstall / Setup\nFirst Time Running\nPost Install\nBuild Trainer\nPrompt (prior to training)\nTrain Model\nEvaluate Model\nInstall / Setup\nFirst Time Running\nWe need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do.\nPost Install\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nBuild Trainer\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5432'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        'train.dtype=bfloat16',\n        'train.max_iters=5000',\n        'train.log_interval=250',\n        'train.eval_interval=1000',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n\n--------------------------------------------------------------------------\nWARNING: There was an error initializing an OpenFabrics device.\n\n  Local host:   thetagpu23\n  Local device: mlx5_0\n--------------------------------------------------------------------------\n2023-11-15 09:33:41.578337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n[2023-11-15 09:33:44][INFO][configs.py:263] - Rescaling GAS -&gt; GAS // WORLD_SIZE = 1 // 1\n[2023-11-15 09:33:44][INFO][configs.py:398] - Tokens per iteration: 16,384\n[2023-11-15 09:33:44][INFO][configs.py:430] - Using &lt;torch.amp.autocast_mode.autocast object at 0x7f588e33ddb0&gt;\n[2023-11-15 09:33:44][INFO][configs.py:436] - Initializing a new model from scratch\n[2023-11-15 09:33:44][INFO][trainer.py:179] - Initializing a new model from scratch\n[2023-11-15 09:33:44][INFO][model.py:160] - number of parameters: 10.65M\n[2023-11-15 09:33:45][INFO][model.py:290] - num decayed parameter tensors: 26, with 10,740,096 parameters\n[2023-11-15 09:33:45][INFO][model.py:291] - num non-decayed parameter tensors: 13, with 4,992 parameters\n[2023-11-15 09:33:46][INFO][model.py:297] - using fused AdamW: True\n\n\n[2023-11-11 01:15:48][INFO][trainer.py:179] - Initializing a new model from scratch\n\n\n[2023-11-11 01:15:48][INFO][model.py:160] - number of parameters: 10.65M\n\n\n[2023-11-11 01:15:50][INFO][model.py:290] - num decayed parameter tensors: 26, with 10,740,096 parameters\n\n\n[2023-11-11 01:15:50][INFO][model.py:291] - num non-decayed parameter tensors: 13, with 4,992 parameters\n\n\n[2023-11-11 01:15:50][INFO][model.py:297] - using fused AdamW: True\nPrompt (prior to training)\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[prompt]: \"What is a supercomputer?\"\n\n\n[response]:\n\nWhat is a supercomputer?CbqA-RN?bnss--iadmsD\nS?qJKEwssDq YMSSSFGPxxnJLDC:cvYfOy\nfiXXe3GQQvYQARdEEbbHHPnWyFp-CwBFrr;g\nWATVAcTZCWWr\ntYCCz,E-wqNbIsMbSvYVONyaQzzcs;Iaa?WOACrnMH'':dXFEQZa-PYkAvV.B.  F$J-nKnEaZ,'vpesXY&y-M.nIcMVV!GYYVVFh-UX.G&Fa?LSPrkXd3eKV?KJjSZOwSbbhwfIYaywrvRUEuuQMnnIAZS-Ja.fXrMAHB&!!eVbUFwMIkkalHbmRhwwfcj$:s\nRlVhRcaVbYcTTihITDUbbTNMHEdnOibdB-ebuiJLLS:yarlFYHHkSWxB!hbN?nVm3-&djw'BA uS,EQJP3bbWe$hs-g\n:3jEEYU\nNkLCetHH lc-IIZEBbb-at\njyNYmvffVVnERN?LnTM:yS\nsH;are$WRip!jbX'\ne\n\npyA-jbwK 'B$O& Fvvac&sEjbIretcX-H\nTrain Model\nLegend:\n\n\n\n\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization[^1](in%20units%20of%20A100%20%60bfloat16%60%20peak%20FLOPS)\ntrainer.train()\n\n{\"model_id\":\"e9728e68f7414dcf91f5c4214df8ad72\",\"version_major\":2,\"version_minor\":0}\n\n\n[2023-11-15 09:34:03][INFO][trainer.py:516] - step=250 loss=2.064 dt=27.412 sps=36.481 mtps=0.598 mfu=13.594 train_loss=4.299 val_loss=4.291\n[2023-11-15 09:34:10][INFO][trainer.py:516] - step=500 loss=1.610 dt=26.915 sps=37.153 mtps=0.609 mfu=13.619 train_loss=4.299 val_loss=4.291\n[2023-11-15 09:34:17][INFO][trainer.py:516] - step=750 loss=1.432 dt=27.775 sps=36.004 mtps=0.590 mfu=13.598 train_loss=4.299 val_loss=4.291\n[2023-11-15 09:34:24][INFO][trainer.py:516] - step=1000 loss=1.346 dt=26.781 sps=37.340 mtps=0.612 mfu=13.630 train_loss=4.299 val_loss=4.291\n[2023-11-15 09:34:28][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n[2023-11-15 09:34:28][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n[2023-11-15 09:34:28][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n[2023-11-15 09:34:35][INFO][trainer.py:516] - step=1250 loss=1.309 dt=27.473 sps=36.400 mtps=0.596 mfu=13.623 train_loss=1.271 val_loss=1.520\n[2023-11-15 09:34:42][INFO][trainer.py:516] - step=1500 loss=1.225 dt=27.261 sps=36.682 mtps=0.601 mfu=13.628 train_loss=1.271 val_loss=1.520\n[2023-11-15 09:34:49][INFO][trainer.py:516] - step=1750 loss=1.176 dt=26.890 sps=37.188 mtps=0.609 mfu=13.651 train_loss=1.271 val_loss=1.520\n[2023-11-15 09:34:56][INFO][trainer.py:516] - step=2000 loss=1.163 dt=26.727 sps=37.415 mtps=0.613 mfu=13.680 train_loss=1.271 val_loss=1.520\n[2023-11-15 09:35:00][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n[2023-11-15 09:35:00][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n[2023-11-15 09:35:00][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n[2023-11-15 09:35:07][INFO][trainer.py:516] - step=2250 loss=1.120 dt=26.733 sps=37.407 mtps=0.613 mfu=13.706 train_loss=1.052 val_loss=1.471\n[2023-11-15 09:35:14][INFO][trainer.py:516] - step=2500 loss=1.068 dt=27.096 sps=36.905 mtps=0.605 mfu=13.710 train_loss=1.052 val_loss=1.471\n[2023-11-15 09:35:21][INFO][trainer.py:516] - step=2750 loss=1.027 dt=26.879 sps=37.204 mtps=0.610 mfu=13.726 train_loss=1.052 val_loss=1.471\n[2023-11-15 09:35:27][INFO][trainer.py:516] - step=3000 loss=1.002 dt=27.375 sps=36.530 mtps=0.599 mfu=13.714 train_loss=1.052 val_loss=1.471\n[2023-11-15 09:35:32][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n[2023-11-15 09:35:32][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n[2023-11-15 09:35:32][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n[2023-11-15 09:35:39][INFO][trainer.py:516] - step=3250 loss=0.950 dt=26.866 sps=37.222 mtps=0.610 mfu=13.730 train_loss=0.864 val_loss=1.531\n[2023-11-15 09:35:45][INFO][trainer.py:516] - step=3500 loss=0.926 dt=27.330 sps=36.590 mtps=0.599 mfu=13.720 train_loss=0.864 val_loss=1.531\n[2023-11-15 09:35:52][INFO][trainer.py:516] - step=3750 loss=0.916 dt=27.203 sps=36.761 mtps=0.602 mfu=13.718 train_loss=0.864 val_loss=1.531\n[2023-11-15 09:35:59][INFO][trainer.py:516] - step=4000 loss=0.901 dt=27.394 sps=36.504 mtps=0.598 mfu=13.706 train_loss=0.864 val_loss=1.531\n[2023-11-15 09:36:03][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n[2023-11-15 09:36:03][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n[2023-11-15 09:36:03][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n[2023-11-15 09:36:10][INFO][trainer.py:516] - step=4250 loss=0.840 dt=26.814 sps=37.293 mtps=0.611 mfu=13.725 train_loss=0.703 val_loss=1.615\n[2023-11-15 09:36:17][INFO][trainer.py:516] - step=4500 loss=0.850 dt=27.402 sps=36.494 mtps=0.598 mfu=13.713 train_loss=0.703 val_loss=1.615\n[2023-11-15 09:36:24][INFO][trainer.py:516] - step=4750 loss=0.824 dt=26.811 sps=37.298 mtps=0.611 mfu=13.731 train_loss=0.703 val_loss=1.615\n[2023-11-15 09:36:30][INFO][trainer.py:516] - step=5000 loss=0.819 dt=27.435 sps=36.450 mtps=0.597 mfu=13.716 train_loss=0.703 val_loss=1.615\n\n\n[2023-11-11 01:16:27][INFO][trainer.py:516] - step=1000 loss=1.332 dt=26.899 sps=37.176 mtps=0.609 mfu=13.642 train_loss=4.299 val_loss=4.291\n\n\n[2023-11-11 01:16:34][INFO][trainer.py:516] - step=1250 loss=1.277 dt=27.229 sps=36.725 mtps=0.602 mfu=13.647 train_loss=4.299 val_loss=4.291\n\n\n[2023-11-11 01:16:40][INFO][trainer.py:516] - step=1500 loss=1.234 dt=26.878 sps=37.205 mtps=0.610 mfu=13.668 train_loss=4.299 val_loss=4.291\n\n\n[2023-11-11 01:16:47][INFO][trainer.py:516] - step=1750 loss=1.175 dt=27.460 sps=36.417 mtps=0.597 mfu=13.659 train_loss=4.299 val_loss=4.291\n\n\n[2023-11-11 01:16:54][INFO][trainer.py:516] - step=2000 loss=1.140 dt=26.889 sps=37.190 mtps=0.609 mfu=13.678 train_loss=4.299 val_loss=4.291\n\n\n[2023-11-11 01:16:58][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n\n\n[2023-11-11 01:16:58][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n\n\n[2023-11-11 01:16:58][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n\n\n[2023-11-11 01:17:05][INFO][trainer.py:516] - step=2250 loss=1.121 dt=27.308 sps=36.619 mtps=0.600 mfu=13.675 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:12][INFO][trainer.py:516] - step=2500 loss=1.067 dt=26.838 sps=37.261 mtps=0.610 mfu=13.696 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:19][INFO][trainer.py:516] - step=2750 loss=1.034 dt=27.360 sps=36.550 mtps=0.599 mfu=13.688 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:26][INFO][trainer.py:516] - step=3000 loss=1.009 dt=26.237 sps=38.114 mtps=0.624 mfu=13.740 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:33][INFO][trainer.py:516] - step=3250 loss=0.940 dt=26.991 sps=37.050 mtps=0.607 mfu=13.746 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:39][INFO][trainer.py:516] - step=3500 loss=0.947 dt=26.261 sps=38.080 mtps=0.624 mfu=13.791 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:46][INFO][trainer.py:516] - step=3750 loss=0.885 dt=37.216 sps=26.870 mtps=0.440 mfu=13.413 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:53][INFO][trainer.py:516] - step=4000 loss=0.866 dt=26.241 sps=38.108 mtps=0.624 mfu=13.492 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:57][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n\n\n[2023-11-11 01:17:57][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n\n\n[2023-11-11 01:17:57][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n\n\n[2023-11-11 01:18:04][INFO][trainer.py:516] - step=4250 loss=0.847 dt=27.228 sps=36.728 mtps=0.602 mfu=13.511 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:11][INFO][trainer.py:516] - step=4500 loss=0.835 dt=26.215 sps=38.147 mtps=0.625 mfu=13.581 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:18][INFO][trainer.py:516] - step=4750 loss=0.822 dt=26.657 sps=37.513 mtps=0.615 mfu=13.621 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:24][INFO][trainer.py:516] - step=5000 loss=0.808 dt=26.635 sps=37.544 mtps=0.615 mfu=13.658 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:31][INFO][trainer.py:516] - step=5250 loss=0.811 dt=26.267 sps=38.071 mtps=0.624 mfu=13.711 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:38][INFO][trainer.py:516] - step=5500 loss=0.769 dt=26.406 sps=37.870 mtps=0.620 mfu=13.751 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:44][INFO][trainer.py:516] - step=5750 loss=0.780 dt=26.239 sps=38.111 mtps=0.624 mfu=13.796 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:51][INFO][trainer.py:516] - step=6000 loss=0.767 dt=26.682 sps=37.478 mtps=0.614 mfu=13.813 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:55][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n\n\n[2023-11-11 01:18:55][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n\n\n[2023-11-11 01:18:56][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n\n\n[2023-11-11 01:19:02][INFO][trainer.py:516] - step=6250 loss=0.773 dt=31.104 sps=32.151 mtps=0.527 mfu=13.629 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:09][INFO][trainer.py:516] - step=6500 loss=0.759 dt=27.142 sps=36.843 mtps=0.604 mfu=13.639 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:16][INFO][trainer.py:516] - step=6750 loss=0.753 dt=26.712 sps=37.437 mtps=0.613 mfu=13.670 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:22][INFO][trainer.py:516] - step=7000 loss=0.745 dt=26.871 sps=37.215 mtps=0.610 mfu=13.690 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:29][INFO][trainer.py:516] - step=7250 loss=0.733 dt=26.266 sps=38.072 mtps=0.624 mfu=13.740 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:36][INFO][trainer.py:516] - step=7500 loss=0.723 dt=26.817 sps=37.289 mtps=0.611 mfu=13.755 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:43][INFO][trainer.py:516] - step=7750 loss=0.747 dt=26.461 sps=37.791 mtps=0.619 mfu=13.788 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:49][INFO][trainer.py:516] - step=8000 loss=0.729 dt=29.348 sps=34.074 mtps=0.558 mfu=13.679 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:53][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n\n\n[2023-11-11 01:19:53][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n\n\n[2023-11-11 01:19:54][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n\n\n[2023-11-11 01:20:01][INFO][trainer.py:516] - step=8250 loss=0.718 dt=26.464 sps=37.787 mtps=0.619 mfu=13.719 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:07][INFO][trainer.py:516] - step=8500 loss=0.705 dt=27.051 sps=36.967 mtps=0.606 mfu=13.725 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:14][INFO][trainer.py:516] - step=8750 loss=0.704 dt=26.298 sps=38.026 mtps=0.623 mfu=13.769 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:21][INFO][trainer.py:516] - step=9000 loss=0.694 dt=27.131 sps=36.858 mtps=0.604 mfu=13.766 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:27][INFO][trainer.py:516] - step=9250 loss=0.700 dt=26.291 sps=38.036 mtps=0.623 mfu=13.806 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:34][INFO][trainer.py:516] - step=9500 loss=0.668 dt=27.353 sps=36.560 mtps=0.599 mfu=13.788 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:41][INFO][trainer.py:516] - step=9750 loss=0.658 dt=26.422 sps=37.847 mtps=0.620 mfu=13.819 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:48][INFO][trainer.py:516] - step=10000 loss=0.678 dt=26.887 sps=37.192 mtps=0.609 mfu=13.823 train_loss=0.473 val_loss=1.840\nEvaluate Model\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[prompt]: \"What is a supercomputer?\"\n\n\n[response]:\n\nWhat is a supercomputer?\nHow now, now! what news?\nHave thy sons?\n\nMessenger:\nThe queen is his noble consul;\nThe man I am a lord's, and he received:\nTherefore, consider and the hand of death.\n\nSIR STEPHEN SCROOP:\nPeace, hope, my lord; I am not thy name;\nFor I have need of this cause is so long.\n\nBISHOP OF ELY:\nBelieve me, I will practise your majesty.\nBe remember thy thoughts: give me and brothers,\nAnd towards London, till I were common all.\n\nBUCKINGHAM:\nNorthumberland, so proud weighing to fight.\n\nGLOUCESTER:\nRelent, e"
  },
  {
    "objectID": "quarto/ngpt-shakespeare.html#install-setup",
    "href": "quarto/ngpt-shakespeare.html#install-setup",
    "title": "nanoGPT",
    "section": "Install / Setup",
    "text": "Install / Setup"
  },
  {
    "objectID": "quarto/ngpt-shakespeare.html#post-install",
    "href": "quarto/ngpt-shakespeare.html#post-install",
    "title": "nanoGPT",
    "section": "Post Install",
    "text": "Post Install\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'"
  },
  {
    "objectID": "quarto/ngpt-shakespeare.html#build-trainer",
    "href": "quarto/ngpt-shakespeare.html#build-trainer",
    "title": "nanoGPT",
    "section": "Build Trainer",
    "text": "Build Trainer\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)"
  },
  {
    "objectID": "quarto/ngpt-shakespeare.html#prompt-prior-to-training",
    "href": "quarto/ngpt-shakespeare.html#prompt-prior-to-training",
    "title": "nanoGPT",
    "section": "Prompt (prior to training)",
    "text": "Prompt (prior to training)"
  },
  {
    "objectID": "quarto/ngpt-shakespeare.html#train-model",
    "href": "quarto/ngpt-shakespeare.html#train-model",
    "title": "nanoGPT",
    "section": "Train Model",
    "text": "Train Model\nLegend:\n\n\n\n\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization[^1](in%20units%20of%20A100%20%60bfloat16%60%20peak%20FLOPS)"
  },
  {
    "objectID": "quarto/ngpt-shakespeare.html#evaluate-model",
    "href": "quarto/ngpt-shakespeare.html#evaluate-model",
    "title": "nanoGPT",
    "section": "Evaluate Model",
    "text": "Evaluate Model"
  },
  {
    "objectID": "quarto/ngpt-shakespeare.html#footnotes",
    "href": "quarto/ngpt-shakespeare.html#footnotes",
    "title": "nanoGPT",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin units of A100 bfloat16 peak FLOPS↩︎"
  },
  {
    "objectID": "quarto/shakespeare.html",
    "href": "quarto/shakespeare.html",
    "title": "Shakespeare",
    "section": "",
    "text": "Open In Collab\n\n\n\nnanoGPT\nInstall / Setup\nFirst Time Running\nPost Install\nBuild Trainer\nPrompt (prior to training)\nTrain Model\nEvaluate Model"
  },
  {
    "objectID": "quarto/shakespeare.html#install-setup",
    "href": "quarto/shakespeare.html#install-setup",
    "title": "Shakespeare",
    "section": "Install / Setup",
    "text": "Install / Setup\n\nFirst Time Running\nWe need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do."
  },
  {
    "objectID": "quarto/shakespeare.html#post-install",
    "href": "quarto/shakespeare.html#post-install",
    "title": "Shakespeare",
    "section": "Post Install",
    "text": "Post Install\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\n\n    The autoreload extension is already loaded. To reload it, use:\n      %reload_ext autoreload\n\n    /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py"
  },
  {
    "objectID": "quarto/shakespeare.html#build-trainer",
    "href": "quarto/shakespeare.html#build-trainer",
    "title": "Shakespeare",
    "section": "Build Trainer",
    "text": "Build Trainer\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5432'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        'train.dtype=bfloat16',\n        'train.max_iters=5000',\n        'train.log_interval=250',\n        'train.eval_interval=1000',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n\n    --------------------------------------------------------------------------\n    WARNING: There was an error initializing an OpenFabrics device.\n    \n      Local host:   thetagpu23\n      Local device: mlx5_0\n    --------------------------------------------------------------------------\n    2023-11-15 09:33:41.578337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n    [2023-11-15 09:33:44][INFO][configs.py:263] - Rescaling GAS -&gt; GAS // WORLD_SIZE = 1 // 1\n    [2023-11-15 09:33:44][INFO][configs.py:398] - Tokens per iteration: 16,384\n    [2023-11-15 09:33:44][INFO][configs.py:430] - Using &lt;torch.amp.autocast_mode.autocast object at 0x7f588e33ddb0&gt;\n    [2023-11-15 09:33:44][INFO][configs.py:436] - Initializing a new model from scratch\n    [2023-11-15 09:33:44][INFO][trainer.py:179] - Initializing a new model from scratch\n    [2023-11-15 09:33:44][INFO][model.py:160] - number of parameters: 10.65M\n    [2023-11-15 09:33:45][INFO][model.py:290] - num decayed parameter tensors: 26, with 10,740,096 parameters\n    [2023-11-15 09:33:45][INFO][model.py:291] - num non-decayed parameter tensors: 13, with 4,992 parameters\n    [2023-11-15 09:33:46][INFO][model.py:297] - using fused AdamW: True\n    [2023-11-11 01:15:48][INFO][trainer.py:179] - Initializing a new model from scratch\n    [2023-11-11 01:15:48][INFO][model.py:160] - number of parameters: 10.65M\n    [2023-11-11 01:15:50][INFO][model.py:290] - num decayed parameter tensors: 26, with 10,740,096 parameters\n    [2023-11-11 01:15:50][INFO][model.py:291] - num non-decayed parameter tensors: 13, with 4,992 parameters\n    [2023-11-11 01:15:50][INFO][model.py:297] - using fused AdamW: True"
  },
  {
    "objectID": "quarto/shakespeare.html#prompt-prior-to-training",
    "href": "quarto/shakespeare.html#prompt-prior-to-training",
    "title": "Shakespeare",
    "section": "Prompt (prior to training)",
    "text": "Prompt (prior to training)\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?CbqA-RN?bnss--iadmsD\n    S?qJKEwssDq YMSSSFGPxxnJLDC:cvYfOy\n    fiXXe3GQQvYQARdEEbbHHPnWyFp-CwBFrr;g\n    WATVAcTZCWWr\n    tYCCz,E-wqNbIsMbSvYVONyaQzzcs;Iaa?WOACrnMH'':dXFEQZa-PYkAvV.B.  F$J-nKnEaZ,'vpesXY&y-M.nIcMVV!GYYVVFh-UX.G&Fa?LSPrkXd3eKV?KJjSZOwSbbhwfIYaywrvRUEuuQMnnIAZS-Ja.fXrMAHB&!!eVbUFwMIkkalHbmRhwwfcj$:s\n    RlVhRcaVbYcTTihITDUbbTNMHEdnOibdB-ebuiJLLS:yarlFYHHkSWxB!hbN?nVm3-&djw'BA uS,EQJP3bbWe$hs-g\n    :3jEEYU\n    NkLCetHH lc-IIZEBbb-at\n    jyNYmvffVVnERN?LnTM:yS\n    sH;are$WRip!jbX'\n    e\n\n    pyA-jbwK 'B$O& Fvvac&sEjbIretcX-H\n\n\n\n\n\nTable 1: Legend\n\n\nName\nDescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1"
  },
  {
    "objectID": "quarto/shakespeare.html#train-model",
    "href": "quarto/shakespeare.html#train-model",
    "title": "Shakespeare",
    "section": "Train Model",
    "text": "Train Model\ntrainer.train()\n\n    [2023-11-15 09:34:03][INFO][trainer.py:516] - step=250 loss=2.064 dt=27.412 sps=36.481 mtps=0.598 mfu=13.594 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:10][INFO][trainer.py:516] - step=500 loss=1.610 dt=26.915 sps=37.153 mtps=0.609 mfu=13.619 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:17][INFO][trainer.py:516] - step=750 loss=1.432 dt=27.775 sps=36.004 mtps=0.590 mfu=13.598 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:24][INFO][trainer.py:516] - step=1000 loss=1.346 dt=26.781 sps=37.340 mtps=0.612 mfu=13.630 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:28][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:34:28][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:34:28][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:34:35][INFO][trainer.py:516] - step=1250 loss=1.309 dt=27.473 sps=36.400 mtps=0.596 mfu=13.623 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:34:42][INFO][trainer.py:516] - step=1500 loss=1.225 dt=27.261 sps=36.682 mtps=0.601 mfu=13.628 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:34:49][INFO][trainer.py:516] - step=1750 loss=1.176 dt=26.890 sps=37.188 mtps=0.609 mfu=13.651 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:34:56][INFO][trainer.py:516] - step=2000 loss=1.163 dt=26.727 sps=37.415 mtps=0.613 mfu=13.680 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:35:00][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:35:00][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:35:00][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:35:07][INFO][trainer.py:516] - step=2250 loss=1.120 dt=26.733 sps=37.407 mtps=0.613 mfu=13.706 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:14][INFO][trainer.py:516] - step=2500 loss=1.068 dt=27.096 sps=36.905 mtps=0.605 mfu=13.710 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:21][INFO][trainer.py:516] - step=2750 loss=1.027 dt=26.879 sps=37.204 mtps=0.610 mfu=13.726 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:27][INFO][trainer.py:516] - step=3000 loss=1.002 dt=27.375 sps=36.530 mtps=0.599 mfu=13.714 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:32][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:35:32][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:35:32][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:35:39][INFO][trainer.py:516] - step=3250 loss=0.950 dt=26.866 sps=37.222 mtps=0.610 mfu=13.730 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:35:45][INFO][trainer.py:516] - step=3500 loss=0.926 dt=27.330 sps=36.590 mtps=0.599 mfu=13.720 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:35:52][INFO][trainer.py:516] - step=3750 loss=0.916 dt=27.203 sps=36.761 mtps=0.602 mfu=13.718 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:35:59][INFO][trainer.py:516] - step=4000 loss=0.901 dt=27.394 sps=36.504 mtps=0.598 mfu=13.706 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:36:03][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:36:03][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:36:03][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:36:10][INFO][trainer.py:516] - step=4250 loss=0.840 dt=26.814 sps=37.293 mtps=0.611 mfu=13.725 train_loss=0.703 val_loss=1.615\n    [2023-11-15 09:36:17][INFO][trainer.py:516] - step=4500 loss=0.850 dt=27.402 sps=36.494 mtps=0.598 mfu=13.713 train_loss=0.703 val_loss=1.615\n    [2023-11-15 09:36:24][INFO][trainer.py:516] - step=4750 loss=0.824 dt=26.811 sps=37.298 mtps=0.611 mfu=13.731 train_loss=0.703 val_loss=1.615\n    [2023-11-15 09:36:30][INFO][trainer.py:516] - step=5000 loss=0.819 dt=27.435 sps=36.450 mtps=0.597 mfu=13.716 train_loss=0.703 val_loss=1.615\n    [2023-11-11 01:16:27][INFO][trainer.py:516] - step=1000 loss=1.332 dt=26.899 sps=37.176 mtps=0.609 mfu=13.642 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:34][INFO][trainer.py:516] - step=1250 loss=1.277 dt=27.229 sps=36.725 mtps=0.602 mfu=13.647 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:40][INFO][trainer.py:516] - step=1500 loss=1.234 dt=26.878 sps=37.205 mtps=0.610 mfu=13.668 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:47][INFO][trainer.py:516] - step=1750 loss=1.175 dt=27.460 sps=36.417 mtps=0.597 mfu=13.659 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:54][INFO][trainer.py:516] - step=2000 loss=1.140 dt=26.889 sps=37.190 mtps=0.609 mfu=13.678 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:58][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:16:58][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:16:58][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:17:05][INFO][trainer.py:516] - step=2250 loss=1.121 dt=27.308 sps=36.619 mtps=0.600 mfu=13.675 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:12][INFO][trainer.py:516] - step=2500 loss=1.067 dt=26.838 sps=37.261 mtps=0.610 mfu=13.696 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:19][INFO][trainer.py:516] - step=2750 loss=1.034 dt=27.360 sps=36.550 mtps=0.599 mfu=13.688 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:26][INFO][trainer.py:516] - step=3000 loss=1.009 dt=26.237 sps=38.114 mtps=0.624 mfu=13.740 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:33][INFO][trainer.py:516] - step=3250 loss=0.940 dt=26.991 sps=37.050 mtps=0.607 mfu=13.746 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:39][INFO][trainer.py:516] - step=3500 loss=0.947 dt=26.261 sps=38.080 mtps=0.624 mfu=13.791 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:46][INFO][trainer.py:516] - step=3750 loss=0.885 dt=37.216 sps=26.870 mtps=0.440 mfu=13.413 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:53][INFO][trainer.py:516] - step=4000 loss=0.866 dt=26.241 sps=38.108 mtps=0.624 mfu=13.492 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:57][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:17:57][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:17:57][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:18:04][INFO][trainer.py:516] - step=4250 loss=0.847 dt=27.228 sps=36.728 mtps=0.602 mfu=13.511 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:11][INFO][trainer.py:516] - step=4500 loss=0.835 dt=26.215 sps=38.147 mtps=0.625 mfu=13.581 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:18][INFO][trainer.py:516] - step=4750 loss=0.822 dt=26.657 sps=37.513 mtps=0.615 mfu=13.621 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:24][INFO][trainer.py:516] - step=5000 loss=0.808 dt=26.635 sps=37.544 mtps=0.615 mfu=13.658 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:31][INFO][trainer.py:516] - step=5250 loss=0.811 dt=26.267 sps=38.071 mtps=0.624 mfu=13.711 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:38][INFO][trainer.py:516] - step=5500 loss=0.769 dt=26.406 sps=37.870 mtps=0.620 mfu=13.751 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:44][INFO][trainer.py:516] - step=5750 loss=0.780 dt=26.239 sps=38.111 mtps=0.624 mfu=13.796 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:51][INFO][trainer.py:516] - step=6000 loss=0.767 dt=26.682 sps=37.478 mtps=0.614 mfu=13.813 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:55][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:18:55][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:18:56][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:19:02][INFO][trainer.py:516] - step=6250 loss=0.773 dt=31.104 sps=32.151 mtps=0.527 mfu=13.629 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:09][INFO][trainer.py:516] - step=6500 loss=0.759 dt=27.142 sps=36.843 mtps=0.604 mfu=13.639 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:16][INFO][trainer.py:516] - step=6750 loss=0.753 dt=26.712 sps=37.437 mtps=0.613 mfu=13.670 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:22][INFO][trainer.py:516] - step=7000 loss=0.745 dt=26.871 sps=37.215 mtps=0.610 mfu=13.690 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:29][INFO][trainer.py:516] - step=7250 loss=0.733 dt=26.266 sps=38.072 mtps=0.624 mfu=13.740 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:36][INFO][trainer.py:516] - step=7500 loss=0.723 dt=26.817 sps=37.289 mtps=0.611 mfu=13.755 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:43][INFO][trainer.py:516] - step=7750 loss=0.747 dt=26.461 sps=37.791 mtps=0.619 mfu=13.788 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:49][INFO][trainer.py:516] - step=8000 loss=0.729 dt=29.348 sps=34.074 mtps=0.558 mfu=13.679 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:53][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:19:53][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:19:54][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:20:01][INFO][trainer.py:516] - step=8250 loss=0.718 dt=26.464 sps=37.787 mtps=0.619 mfu=13.719 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:07][INFO][trainer.py:516] - step=8500 loss=0.705 dt=27.051 sps=36.967 mtps=0.606 mfu=13.725 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:14][INFO][trainer.py:516] - step=8750 loss=0.704 dt=26.298 sps=38.026 mtps=0.623 mfu=13.769 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:21][INFO][trainer.py:516] - step=9000 loss=0.694 dt=27.131 sps=36.858 mtps=0.604 mfu=13.766 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:27][INFO][trainer.py:516] - step=9250 loss=0.700 dt=26.291 sps=38.036 mtps=0.623 mfu=13.806 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:34][INFO][trainer.py:516] - step=9500 loss=0.668 dt=27.353 sps=36.560 mtps=0.599 mfu=13.788 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:41][INFO][trainer.py:516] - step=9750 loss=0.658 dt=26.422 sps=37.847 mtps=0.620 mfu=13.819 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:48][INFO][trainer.py:516] - step=10000 loss=0.678 dt=26.887 sps=37.192 mtps=0.609 mfu=13.823 train_loss=0.473 val_loss=1.840"
  },
  {
    "objectID": "quarto/shakespeare.html#evaluate-model",
    "href": "quarto/shakespeare.html#evaluate-model",
    "title": "Shakespeare",
    "section": "Evaluate Model",
    "text": "Evaluate Model\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n    How now, now! what news?\n    Have thy sons?\n\n    Messenger:\n    The queen is his noble consul;\n    The man I am a lord's, and he received:\n    Therefore, consider and the hand of death.\n\n    SIR STEPHEN SCROOP:\n    Peace, hope, my lord; I am not thy name;\n    For I have need of this cause is so long.\n\n    BISHOP OF ELY:\n    Believe me, I will practise your majesty.\n    Be remember thy thoughts: give me and brothers,\n    And towards London, till I were common all.\n\n    BUCKINGHAM:\n    Northumberland, so proud weighing to fight.\n\n    GLOUCESTER:\n    Relent, e"
  },
  {
    "objectID": "quarto/shakespeare.html#footnotes",
    "href": "quarto/shakespeare.html#footnotes",
    "title": "Shakespeare",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin units of A100 bfloat16 peak FLOPS↩︎"
  },
  {
    "objectID": "quarto/gpt2-xl.html",
    "href": "quarto/gpt2-xl.html",
    "title": "GPT-2 XL",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do."
  },
  {
    "objectID": "quarto/gpt2-xl.html#install-setup",
    "href": "quarto/gpt2-xl.html#install-setup",
    "title": "GPT-2 XL",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do."
  },
  {
    "objectID": "quarto/gpt2-xl.html#post-install",
    "href": "quarto/gpt2-xl.html#post-install",
    "title": "GPT-2 XL",
    "section": "Post Install",
    "text": "Post Install\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\n\n    The autoreload extension is already loaded. To reload it, use:\n      %reload_ext autoreload\n\n    /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py"
  },
  {
    "objectID": "quarto/gpt2-xl.html#build-trainer",
    "href": "quarto/gpt2-xl.html#build-trainer",
    "title": "GPT-2 XL",
    "section": "Build Trainer",
    "text": "Build Trainer\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5127'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nSEED = np.random.randint(2**32)\nconsole.print(f'SEED: {SEED}')\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=owt',\n        'model=gpt2_xl',\n        'optimizer=gpt2_xl',\n        'train=gpt2_xl',\n        'train.init_from=gpt2-xl',\n        'train.max_iters=100',\n        'train.dtype=bfloat16',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n\n    --------------------------------------------------------------------------\n    WARNING: There was an error initializing an OpenFabrics device.\n    \n      Local host:   thetagpu24\n      Local device: mlx5_0\n    --------------------------------------------------------------------------\n\n    SEED: 125313342\n    RANK: 0 / 0\n    [2023-11-10 17:36:01][WARNING][configs.py:298] - No meta.pkl found, assuming GPT-2 encodings...\n    [2023-11-10 17:36:01][INFO][configs.py:264] - Rescaling GAS -&gt; GAS // WORLD_SIZE = 1 // 1\n    [2023-11-10 17:36:01][INFO][configs.py:399] - Tokens per iteration: 1,024\n    [2023-11-10 17:36:01][INFO][configs.py:431] - Using &lt;torch.amp.autocast_mode.autocast object at 0x7f98e0139660&gt;\n    [2023-11-10 17:36:01][INFO][trainer.py:184] - Initializing from OpenAI GPT-2 Weights: gpt2-xl\n    2023-11-10 17:36:01.777923: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n    [2023-11-10 17:36:05,925] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n    [2023-11-10 17:36:06][INFO][model.py:225] - loading weights from pretrained gpt: gpt2-xl\n    [2023-11-10 17:36:06][INFO][model.py:234] - forcing vocab_size=50257, block_size=1024, bias=True\n    [2023-11-10 17:36:06][INFO][model.py:240] - overriding dropout rate to 0.0\n    [2023-11-10 17:36:29][INFO][model.py:160] - number of parameters: 1555.97M\n    [2023-11-10 17:36:56][INFO][model.py:290] - num decayed parameter tensors: 194, with 1,556,609,600 parameters\n    [2023-11-10 17:36:56][INFO][model.py:291] - num non-decayed parameter tensors: 386, with 1,001,600 parameters\n    [2023-11-10 17:36:56][INFO][model.py:297] - using fused AdamW: True"
  },
  {
    "objectID": "quarto/gpt2-xl.html#prompt-prior-to-training",
    "href": "quarto/gpt2-xl.html#prompt-prior-to-training",
    "title": "GPT-2 XL",
    "section": "Prompt (prior to training)",
    "text": "Prompt (prior to training)\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n    [prompt]: \"What is a supercomputer?\"\n\n\n\n\n    [response]:\n\n    What is a supercomputer? When it comes to massive computing, a supercomputer is simply a large computer system that has the ability to perform many calculations at once. This can be the result of using many different processing cores, or memory, or operating at a high clock speed. Supercomputers are often used to crack complex calculations and research problems.\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    On a larger scale, these massive computers are used to solve tough mathematical equations and solve hard scientific problems. They are very powerful enough to emulate the workings of the human brain and simulate a human intelligence in a virtual world.\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    In 1992, IBM's NeXTStep supercomputer was the largest and most powerful supercomputer in the world. It was released in 1995 and did not continue to live up to its original promises, because its capabilities were quickly surpassed by its competitors.\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia\n\n    Image credit: Wikipedia&lt;|endoftext|&gt;Editor's note: Dan De Luce is the author of \"When the Going Gets Tough: The New Survival Guide for College Students and Your Health and Well-Being.\"\n\n    College has never been more expensive. But with so many choices and so many choices of where to go, it's harder than ever for prospective students to find a college that fits their lifestyle.\n\n    This is a problem—not just because it can be a hassle to find a college that doesn't require a large amount of financial aid. It's a problem because it can be costly for students to stay in college.\n\n    So I created this list of colleges with the highest tuition where\n\n\n\n\n\nTable 1: Legend\n\n\nName\nDescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1"
  },
  {
    "objectID": "quarto/gpt2-xl.html#train-model",
    "href": "quarto/gpt2-xl.html#train-model",
    "title": "GPT-2 XL",
    "section": "Train Model",
    "text": "Train Model\ntrainer.model.module.train()\ntrainer.train()\n\n    [2023-11-10 17:41:58][INFO][trainer.py:540] - step=100 loss=2.505 dt=922.295 sps=1.084 mtps=0.001 mfu=43.897 train_loss=2.555 val_loss=2.558"
  },
  {
    "objectID": "quarto/gpt2-xl.html#evaluate-model",
    "href": "quarto/gpt2-xl.html#evaluate-model",
    "title": "GPT-2 XL",
    "section": "Evaluate Model",
    "text": "Evaluate Model\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nfrom rich.text import Text\nfrom enrich.console import get_console\nconsole = get_console()\n\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n    [prompt]: \"What is a supercomputer?\"\n    [response]:\n\n    What is a supercomputer? A supercomputer is a machine that is exponentially more powerful than previous computing models while being far more energy efficient.\n\n    What is an artificial neural network? An artificial neural network (ANN) is an order of magnitude more powerful than previous computational models, but has the same energy efficiency.\n\n    For this article I will be using a machine learning technique called Backward-Compatible Neural Networks (BCNNs) to represent the biological brain.\n\n    The BCNNs model is very similar to the neural networks utilized in deep learning, but has the added bonus of being able to 'decouple' the learning from the final results.\n\n    BCNN for Machine Learning\n\n    In order to make the transition from neural networks to BCNNs we will follow the same basic principles as we did with neural networks.\n\n    However, instead of the neurons in neural networks that represent the data being represented, BCNNs work with nodes instead. This is because the nodes are the data, while the neurons are the information.\n\n    In case you aren’t familiar with the term node, it is a symbol representing any type of data. For instance, it could be a datum in a neural network model.\n\n    Another way to think of them is as symbols.\n\n    The basic idea of nodes and connections is that a node can have many connections to other nodes, with each node linked to a connection to a larger entity.\n\n    For instance, a node might have a target, which is just a point in space. A connection might have a value, which is just a number between 0 and 1.\n\n    Something like this:\n\n    Node Value -0.1 0.1 0.1 0.1\n\n    The important thing to note, is that the value is a number between 0 and 1.\n\n    When we are given a list of data and an input, we will move forward through the data, connected nodes, and the resulting output.\n\n    In the case of neural networks, this would look like:\n\n    Neural Network\n\n    A neural network is just a collection of nodes, connected to each other through connections.\n\n    For example, let’s look at the ConvNet model from Wikipedia.\n\n    Pretty simple. It has multiple layers of neurons, with each neuron being assigned one of the above variables.\n\n    The neurons work with the data given as an input (remember, it’s a"
  },
  {
    "objectID": "quarto/shakespeare-escape.html",
    "href": "quarto/shakespeare-escape.html",
    "title": "nanoGPT",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do.\n\n\n\n\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\n\n    The autoreload extension is already loaded. To reload it, use:\n      %reload_ext autoreload\n\n\n\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\n\n\n\n\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5432'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        'train.dtype=bfloat16',\n        'train.max_iters=5000',\n        'train.log_interval=250',\n        'train.eval_interval=1000',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n&lt;pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"&gt;&lt;pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"&gt;\n--------------------------------------------------------------------------\nWARNING: There was an error initializing an OpenFabrics device.\n\n  Local host:   thetagpu23\n  Local device: mlx5_0\n--------------------------------------------------------------------------\n2023-11-15 09:33:41.578337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:263&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Rescaling GAS -&gt; GAS &lt;span style=\"color:#0A0\"&gt;/&lt;/span&gt;&lt;span style=\"color:#0A0\"&gt;/&lt;/span&gt; WORLD_SIZE = &lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt; &lt;span style=\"color:#0A0\"&gt;/&lt;/span&gt;&lt;span style=\"color:#0A0\"&gt;/&lt;/span&gt; &lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:398&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Tokens per iteration: &lt;span style=\"color:#A0A\"&gt;16&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;384&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:430&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Using &lt;b&gt;&lt;&lt;/b&gt;&lt;b&gt;&lt;span style=\"color:#F5F\"&gt;torch.amp.autocast_mode.autocast&lt;/span&gt;&lt;/b&gt;&lt;span style=\"color:#FFF\"&gt; object at &lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;0x7f588e33ddb0&lt;/span&gt;&lt;b&gt;&gt;&lt;/b&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:436&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Initializing a new model from scratch\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:179&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Initializing a new model from scratch\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:160&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - number of parameters: &lt;span style=\"color:#A0A\"&gt;10.&lt;/span&gt;65M\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:45]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:290&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - num decayed parameter tensors: &lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;, with &lt;span style=\"color:#A0A\"&gt;10&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;740&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;096&lt;/span&gt; parameters\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:45]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:291&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - num non-decayed parameter tensors: &lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;, with &lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;992&lt;/span&gt; parameters\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:46]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:297&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - using fused AdamW: &lt;i&gt;&lt;span style=\"color:#5F5\"&gt;True&lt;/span&gt;&lt;/i&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:15:48]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:179&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Initializing a new model from scratch\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:15:48]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:160&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - number of parameters: &lt;span style=\"color:#A0A\"&gt;10.&lt;/span&gt;65M\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:15:50]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:290&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - num decayed parameter tensors: &lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;, with &lt;span style=\"color:#A0A\"&gt;10&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;740&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;096&lt;/span&gt; parameters\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:15:50]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:291&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - num non-decayed parameter tensors: &lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;, with &lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;992&lt;/span&gt; parameters\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:15:50]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:297&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - using fused AdamW: &lt;i&gt;&lt;span style=\"color:#5F5\"&gt;True&lt;/span&gt;&lt;/i&gt;\n\n\n\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n[prompt]: \"What is a supercomputer?\"\n\n[response]:\n\nWhat is a supercomputer?CbqA-RN?bnss--iadmsD\nS?qJKEwssDq YMSSSFGPxxnJLDC:cvYfOy\nfiXXe3GQQvYQARdEEbbHHPnWyFp-CwBFrr;g\nWATVAcTZCWWr\ntYCCz,E-wqNbIsMbSvYVONyaQzzcs;Iaa?WOACrnMH'':dXFEQZa-PYkAvV.B.  F$J-nKnEaZ,'vpesXY&y-M.nIcMVV!GYYVVFh-UX.G&Fa?LSPrkXd3eKV?KJjSZOwSbbhwfIYaywrvRUEuuQMnnIAZS-Ja.fXrMAHB&!!eVbUFwMIkkalHbmRhwwfcj$:s\nRlVhRcaVbYcTTihITDUbbTNMHEdnOibdB-ebuiJLLS:yarlFYHHkSWxB!hbN?nVm3-&djw'BA uS,EQJP3bbWe$hs-g\n:3jEEYU\nNkLCetHH lc-IIZEBbb-at\njyNYmvffVVnERN?LnTM:yS\nsH;are$WRip!jbX'\ne\n\npyA-jbwK 'B$O& Fvvac&sEjbIretcX-H\n\n\n\n\nLegend:\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1\n\n\n\ntrainer.train()\n  0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:03]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.064&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.412&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.481&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.598&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.594&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:10]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.915&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.153&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.609&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.619&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:17]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.432&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.775&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.004&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.590&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.598&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:24]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.346&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.781&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.340&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.612&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.630&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:28]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:28]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:28]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:35]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.309&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.400&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.596&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.623&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.271&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.520&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:42]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.225&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.261&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.682&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.601&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.628&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.271&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.520&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:49]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.176&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.890&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.188&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.609&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.651&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.271&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.520&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:56]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.163&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.727&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.415&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.613&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.680&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.271&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.520&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:00]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:00]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:00]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:07]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.120&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.733&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.407&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.613&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.706&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.052&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.471&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:14]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.068&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.096&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.905&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.605&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.710&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.052&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.471&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:21]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.027&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.879&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.204&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.726&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.052&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.471&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:27]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.002&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.375&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.530&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.599&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.714&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.052&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.471&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:32]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:32]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:32]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:39]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.950&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.866&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.222&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.730&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.864&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.531&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:45]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.926&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.330&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.590&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.599&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.720&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.864&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.531&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:52]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.916&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.203&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.761&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.602&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.718&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.864&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.531&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:59]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.901&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.394&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.504&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.598&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.706&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.864&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.531&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:03]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:03]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:03]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:10]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.814&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.293&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.611&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.725&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.703&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:17]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.850&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.402&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.494&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.598&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.713&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.703&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:24]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.824&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.811&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.298&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.611&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.731&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.703&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:30]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;5000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.819&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.435&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.450&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.597&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.716&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.703&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:27]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.332&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.899&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.176&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.609&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.642&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:34]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.277&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.229&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.725&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.602&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.647&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:40]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.234&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.878&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.205&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.668&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:47]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.175&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.460&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.417&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.597&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.659&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:54]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.140&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.889&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.190&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.609&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.678&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:58]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:58]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:58]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:05]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.121&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.308&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.619&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.600&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.675&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:12]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.067&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.838&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.261&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:19]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.034&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.360&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.550&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.599&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.688&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:26]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.009&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.237&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.114&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.740&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:33]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.940&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.991&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.607&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.746&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:39]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.947&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.261&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.080&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.791&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:46]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.885&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.216&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.870&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.440&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.413&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:53]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.866&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.241&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.108&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.492&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:57]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:57]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:57]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:04]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.847&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.228&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.728&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.602&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.511&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:11]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.835&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.215&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.147&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.625&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.581&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:18]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.822&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.657&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.513&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.621&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:24]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;5000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.808&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.635&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.544&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.658&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:31]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;5250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.811&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.267&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.071&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.711&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:38]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;5500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.769&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.406&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.870&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.620&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.751&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;5750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.780&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.239&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.111&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.796&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:51]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;6000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.767&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.682&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.478&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.614&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.813&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:55]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:55]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:56]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:02]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;6250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.773&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;31&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.104&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;32&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.151&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.527&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.629&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:09]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;6500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.759&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.142&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.843&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.604&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.639&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:16]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;6750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.753&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.712&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.437&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.613&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.670&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:22]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;7000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.745&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.871&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.215&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.690&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:29]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;7250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.733&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.266&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.072&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.740&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:36]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;7500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.723&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.817&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.289&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.611&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:43]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;7750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.747&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.461&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.791&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.619&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.788&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:49]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;8000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.729&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;29&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.348&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;34&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.074&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.558&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.679&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:53]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:53]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:54]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:01]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;8250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.718&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.464&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.787&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.619&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.719&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:07]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;8500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.705&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.051&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.967&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.606&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.725&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:14]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;8750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.704&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.298&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.026&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.623&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.769&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:21]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;9000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.694&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.131&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.858&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.604&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.766&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:27]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;9250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.700&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.036&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.623&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.806&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:34]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;9500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.668&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.353&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.560&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.599&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.788&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:41]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;9750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.658&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.422&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.847&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.620&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.819&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:48]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;10000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.678&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.887&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.192&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.609&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.823&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n\n\n\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n[prompt]: \"What is a supercomputer?\"\n\n[response]:\n\nWhat is a supercomputer?\nHow now, now! what news?\nHave thy sons?\n\nMessenger:\nThe queen is his noble consul;\nThe man I am a lord's, and he received:\nTherefore, consider and the hand of death.\n\nSIR STEPHEN SCROOP:\nPeace, hope, my lord; I am not thy name;\nFor I have need of this cause is so long.\n\nBISHOP OF ELY:\nBelieve me, I will practise your majesty.\nBe remember thy thoughts: give me and brothers,\nAnd towards London, till I were common all.\n\nBUCKINGHAM:\nNorthumberland, so proud weighing to fight.\n\nGLOUCESTER:\nRelent, e"
  },
  {
    "objectID": "quarto/shakespeare-escape.html#install-setup",
    "href": "quarto/shakespeare-escape.html#install-setup",
    "title": "nanoGPT",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do."
  },
  {
    "objectID": "quarto/shakespeare-escape.html#post-install",
    "href": "quarto/shakespeare-escape.html#post-install",
    "title": "nanoGPT",
    "section": "",
    "text": "If installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\n\n    The autoreload extension is already loaded. To reload it, use:\n      %reload_ext autoreload\n\n\n\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py"
  },
  {
    "objectID": "quarto/shakespeare-escape.html#build-trainer",
    "href": "quarto/shakespeare-escape.html#build-trainer",
    "title": "nanoGPT",
    "section": "",
    "text": "Explicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5432'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        'train.dtype=bfloat16',\n        'train.max_iters=5000',\n        'train.log_interval=250',\n        'train.eval_interval=1000',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n&lt;pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"&gt;&lt;pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"&gt;\n--------------------------------------------------------------------------\nWARNING: There was an error initializing an OpenFabrics device.\n\n  Local host:   thetagpu23\n  Local device: mlx5_0\n--------------------------------------------------------------------------\n2023-11-15 09:33:41.578337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:263&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Rescaling GAS -&gt; GAS &lt;span style=\"color:#0A0\"&gt;/&lt;/span&gt;&lt;span style=\"color:#0A0\"&gt;/&lt;/span&gt; WORLD_SIZE = &lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt; &lt;span style=\"color:#0A0\"&gt;/&lt;/span&gt;&lt;span style=\"color:#0A0\"&gt;/&lt;/span&gt; &lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:398&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Tokens per iteration: &lt;span style=\"color:#A0A\"&gt;16&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;384&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:430&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Using &lt;b&gt;&lt;&lt;/b&gt;&lt;b&gt;&lt;span style=\"color:#F5F\"&gt;torch.amp.autocast_mode.autocast&lt;/span&gt;&lt;/b&gt;&lt;span style=\"color:#FFF\"&gt; object at &lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;0x7f588e33ddb0&lt;/span&gt;&lt;b&gt;&gt;&lt;/b&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:436&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Initializing a new model from scratch\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:179&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Initializing a new model from scratch\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:160&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - number of parameters: &lt;span style=\"color:#A0A\"&gt;10.&lt;/span&gt;65M\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:45]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:290&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - num decayed parameter tensors: &lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;, with &lt;span style=\"color:#A0A\"&gt;10&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;740&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;096&lt;/span&gt; parameters\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:45]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:291&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - num non-decayed parameter tensors: &lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;, with &lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;992&lt;/span&gt; parameters\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:33:46]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:297&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - using fused AdamW: &lt;i&gt;&lt;span style=\"color:#5F5\"&gt;True&lt;/span&gt;&lt;/i&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:15:48]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:179&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Initializing a new model from scratch\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:15:48]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:160&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - number of parameters: &lt;span style=\"color:#A0A\"&gt;10.&lt;/span&gt;65M\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:15:50]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:290&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - num decayed parameter tensors: &lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;, with &lt;span style=\"color:#A0A\"&gt;10&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;740&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;096&lt;/span&gt; parameters\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:15:50]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:291&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - num non-decayed parameter tensors: &lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;, with &lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;,&lt;span style=\"color:#A0A\"&gt;992&lt;/span&gt; parameters\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:15:50]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[model.py:297&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - using fused AdamW: &lt;i&gt;&lt;span style=\"color:#5F5\"&gt;True&lt;/span&gt;&lt;/i&gt;"
  },
  {
    "objectID": "quarto/shakespeare-escape.html#prompt-prior-to-training",
    "href": "quarto/shakespeare-escape.html#prompt-prior-to-training",
    "title": "nanoGPT",
    "section": "",
    "text": "query = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n[prompt]: \"What is a supercomputer?\"\n\n[response]:\n\nWhat is a supercomputer?CbqA-RN?bnss--iadmsD\nS?qJKEwssDq YMSSSFGPxxnJLDC:cvYfOy\nfiXXe3GQQvYQARdEEbbHHPnWyFp-CwBFrr;g\nWATVAcTZCWWr\ntYCCz,E-wqNbIsMbSvYVONyaQzzcs;Iaa?WOACrnMH'':dXFEQZa-PYkAvV.B.  F$J-nKnEaZ,'vpesXY&y-M.nIcMVV!GYYVVFh-UX.G&Fa?LSPrkXd3eKV?KJjSZOwSbbhwfIYaywrvRUEuuQMnnIAZS-Ja.fXrMAHB&!!eVbUFwMIkkalHbmRhwwfcj$:s\nRlVhRcaVbYcTTihITDUbbTNMHEdnOibdB-ebuiJLLS:yarlFYHHkSWxB!hbN?nVm3-&djw'BA uS,EQJP3bbWe$hs-g\n:3jEEYU\nNkLCetHH lc-IIZEBbb-at\njyNYmvffVVnERN?LnTM:yS\nsH;are$WRip!jbX'\ne\n\npyA-jbwK 'B$O& Fvvac&sEjbIretcX-H"
  },
  {
    "objectID": "quarto/shakespeare-escape.html#train-model",
    "href": "quarto/shakespeare-escape.html#train-model",
    "title": "nanoGPT",
    "section": "",
    "text": "Legend:\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1\n\n\n\ntrainer.train()\n  0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:03]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.064&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.412&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.481&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.598&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.594&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:10]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.915&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.153&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.609&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.619&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:17]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.432&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.775&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.004&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.590&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.598&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:24]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.346&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.781&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.340&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.612&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.630&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:28]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:28]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:28]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:35]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.309&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.400&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.596&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.623&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.271&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.520&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:42]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.225&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.261&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.682&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.601&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.628&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.271&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.520&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:49]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.176&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.890&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.188&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.609&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.651&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.271&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.520&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:34:56]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.163&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.727&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.415&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.613&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.680&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.271&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.520&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:00]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:00]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:00]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:07]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.120&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.733&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.407&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.613&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.706&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.052&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.471&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:14]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.068&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.096&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.905&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.605&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.710&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.052&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.471&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:21]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.027&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.879&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.204&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.726&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.052&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.471&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:27]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.002&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.375&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.530&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.599&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.714&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.052&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.471&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:32]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:32]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:32]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:39]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.950&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.866&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.222&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.730&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.864&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.531&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:45]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.926&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.330&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.590&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.599&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.720&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.864&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.531&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:52]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.916&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.203&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.761&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.602&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.718&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.864&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.531&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:35:59]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.901&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.394&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.504&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.598&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.706&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.864&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.531&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:03]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:03]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:03]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;notebooks&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:10]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.814&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.293&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.611&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.725&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.703&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:17]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.850&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.402&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.494&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.598&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.713&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.703&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:24]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.824&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.811&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.298&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.611&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.731&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.703&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-15 09:36:30]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;5000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.819&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.435&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.450&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.597&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.716&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.703&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:27]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.332&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.899&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.176&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.609&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.642&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:34]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.277&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.229&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.725&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.602&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.647&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:40]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.234&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.878&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.205&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.668&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:47]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.175&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.460&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.417&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.597&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.659&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:54]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.140&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.889&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.190&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.609&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.678&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.299&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:58]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:58]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:16:58]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:05]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.121&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.308&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.619&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.600&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.675&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:12]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.067&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.838&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.261&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:19]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;2750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.034&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.360&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.550&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.599&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.688&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:26]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.009&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.237&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.114&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.740&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:33]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.940&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.991&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.607&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.746&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:39]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.947&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.261&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.080&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.791&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:46]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;3750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.885&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.216&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.870&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.440&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.413&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:53]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.866&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.241&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.108&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.492&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.050&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.474&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:57]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:57]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:17:57]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:04]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.847&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.228&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.728&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.602&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.511&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:11]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.835&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.215&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.147&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.625&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.581&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:18]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;4750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.822&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.657&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.513&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.621&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:24]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;5000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.808&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.635&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.544&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.615&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.658&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:31]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;5250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.811&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.267&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.071&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.711&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:38]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;5500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.769&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.406&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.870&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.620&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.751&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:44]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;5750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.780&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.239&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.111&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.796&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:51]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;6000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.767&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.682&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.478&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.614&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.813&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.696&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.637&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:55]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:55]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:18:56]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:02]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;6250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.773&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;31&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.104&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;32&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.151&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.527&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.629&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:09]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;6500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.759&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.142&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.843&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.604&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.639&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:16]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;6750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.753&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.712&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.437&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.613&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.670&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:22]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;7000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.745&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.871&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.215&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.610&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.690&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:29]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;7250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.733&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.266&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.072&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.624&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.740&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:36]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;7500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.723&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.817&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.289&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.611&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:43]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;7750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.747&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.461&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.791&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.619&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.788&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:49]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;8000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.729&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;29&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.348&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;34&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.074&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.558&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.679&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.556&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.755&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:53]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:432&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving checkpoint to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:53]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:433&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Saving model to: &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;model.pth&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:19:54]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[configs.py:129&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - Appending &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;ngpt&lt;/span&gt; to &lt;span style=\"color:#0A0\"&gt;/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;checkpoints.log&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:01]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;8250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.718&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.464&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.787&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.619&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.719&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:07]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;8500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.705&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.051&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.967&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.606&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.725&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:14]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;8750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.704&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.298&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.026&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.623&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.769&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:21]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;9000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.694&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.131&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.858&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.604&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.766&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:27]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;9250&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.700&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.291&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;38&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.036&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.623&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.806&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:34]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;9500&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.668&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;27&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.353&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;36&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.560&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.599&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.788&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:41]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;9750&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.658&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.422&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.847&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.620&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.819&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;\n&lt;span style=\"color:#838383\"&gt;[2023-11-11 01:20:48]&lt;/span&gt;&lt;span style=\"color:#00A\"&gt;[INFO]&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;[trainer.py:516&lt;/span&gt;&lt;span style=\"color:#777777\"&gt;]&lt;/span&gt; - &lt;i&gt;&lt;span style=\"color:#55F\"&gt;step&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;10000&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.678&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;dt&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;26&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.887&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;sps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;37&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.192&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mtps&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.609&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;mfu&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;13&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.823&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;train_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;0&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.473&lt;/span&gt; &lt;i&gt;&lt;span style=\"color:#55F\"&gt;val_loss&lt;/span&gt;&lt;/i&gt;=&lt;span style=\"color:#A0A\"&gt;1&lt;/span&gt;&lt;span style=\"color:#A0A\"&gt;.840&lt;/span&gt;"
  },
  {
    "objectID": "quarto/shakespeare-escape.html#evaluate-model",
    "href": "quarto/shakespeare-escape.html#evaluate-model",
    "title": "nanoGPT",
    "section": "",
    "text": "query = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n[prompt]: \"What is a supercomputer?\"\n\n[response]:\n\nWhat is a supercomputer?\nHow now, now! what news?\nHave thy sons?\n\nMessenger:\nThe queen is his noble consul;\nThe man I am a lord's, and he received:\nTherefore, consider and the hand of death.\n\nSIR STEPHEN SCROOP:\nPeace, hope, my lord; I am not thy name;\nFor I have need of this cause is so long.\n\nBISHOP OF ELY:\nBelieve me, I will practise your majesty.\nBe remember thy thoughts: give me and brothers,\nAnd towards London, till I were common all.\n\nBUCKINGHAM:\nNorthumberland, so proud weighing to fight.\n\nGLOUCESTER:\nRelent, e"
  },
  {
    "objectID": "quarto/shakespeare-escape.html#footnotes",
    "href": "quarto/shakespeare-escape.html#footnotes",
    "title": "nanoGPT",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin units of A100 bfloat16 peak FLOPS↩︎"
  },
  {
    "objectID": "quarto/shakespeare-clean.html",
    "href": "quarto/shakespeare-clean.html",
    "title": "nanoGPT",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do.\n\n\n\n\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\n\n\n\n\n\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5432'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        'train.dtype=bfloat16',\n        'train.max_iters=5000',\n        'train.log_interval=250',\n        'train.eval_interval=1000',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n\n    --------------------------------------------------------------------------\n    WARNING: There was an error initializing an OpenFabrics device.\n    \n      Local host:   thetagpu23\n      Local device: mlx5_0\n    --------------------------------------------------------------------------\n    2023-11-15 09:33:41.578337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n    [2023-11-15 09:33:44][INFO][configs.py:263] - Rescaling GAS -&gt; GAS // WORLD_SIZE = 1 // 1\n    [2023-11-15 09:33:44][INFO][configs.py:398] - Tokens per iteration: 16,384\n    [2023-11-15 09:33:44][INFO][configs.py:430] - Using &lt;torch.amp.autocast_mode.autocast object at 0x7f588e33ddb0&gt;\n    [2023-11-15 09:33:44][INFO][configs.py:436] - Initializing a new model from scratch\n    [2023-11-15 09:33:44][INFO][trainer.py:179] - Initializing a new model from scratch\n    [2023-11-15 09:33:44][INFO][model.py:160] - number of parameters: 10.65M\n    [2023-11-15 09:33:45][INFO][model.py:290] - num decayed parameter tensors: 26, with 10,740,096 parameters\n    [2023-11-15 09:33:45][INFO][model.py:291] - num non-decayed parameter tensors: 13, with 4,992 parameters\n    [2023-11-15 09:33:46][INFO][model.py:297] - using fused AdamW: True\n    [2023-11-11 01:15:48][INFO][trainer.py:179] - Initializing a new model from scratch\n    [2023-11-11 01:15:48][INFO][model.py:160] - number of parameters: 10.65M\n    [2023-11-11 01:15:50][INFO][model.py:290] - num decayed parameter tensors: 26, with 10,740,096 parameters\n    [2023-11-11 01:15:50][INFO][model.py:291] - num non-decayed parameter tensors: 13, with 4,992 parameters\n    [2023-11-11 01:15:50][INFO][model.py:297] - using fused AdamW: True\n\n  \n\n\n\n\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[prompt]: \"What is a supercomputer?\"\n\n\n\n[response]:\n\nWhat is a supercomputer?CbqA-RN?bnss--iadmsD\nS?qJKEwssDq YMSSSFGPxxnJLDC:cvYfOy\nfiXXe3GQQvYQARdEEbbHHPnWyFp-CwBFrr;g\nWATVAcTZCWWr\ntYCCz,E-wqNbIsMbSvYVONyaQzzcs;Iaa?WOACrnMH'':dXFEQZa-PYkAvV.B.  F$J-nKnEaZ,'vpesXY&y-M.nIcMVV!GYYVVFh-UX.G&Fa?LSPrkXd3eKV?KJjSZOwSbbhwfIYaywrvRUEuuQMnnIAZS-Ja.fXrMAHB&!!eVbUFwMIkkalHbmRhwwfcj$:s\nRlVhRcaVbYcTTihITDUbbTNMHEdnOibdB-ebuiJLLS:yarlFYHHkSWxB!hbN?nVm3-&djw'BA uS,EQJP3bbWe$hs-g\n:3jEEYU\nNkLCetHH lc-IIZEBbb-at\njyNYmvffVVnERN?LnTM:yS\nsH;are$WRip!jbX'\ne\n\npyA-jbwK 'B$O& Fvvac&sEjbIretcX-H\n\n\n\n\n\nLegend:\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1\n\n\n\ntrainer.train()\n\n    0%|          | 0/5000 [00:00[2023-11-15 09:34:03][INFO][trainer.py:516] - step=250 loss=2.064 dt=27.412 sps=36.481 mtps=0.598 mfu=13.594 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:10][INFO][trainer.py:516] - step=500 loss=1.610 dt=26.915 sps=37.153 mtps=0.609 mfu=13.619 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:17][INFO][trainer.py:516] - step=750 loss=1.432 dt=27.775 sps=36.004 mtps=0.590 mfu=13.598 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:24][INFO][trainer.py:516] - step=1000 loss=1.346 dt=26.781 sps=37.340 mtps=0.612 mfu=13.630 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:28][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:34:28][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:34:28][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:34:35][INFO][trainer.py:516] - step=1250 loss=1.309 dt=27.473 sps=36.400 mtps=0.596 mfu=13.623 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:34:42][INFO][trainer.py:516] - step=1500 loss=1.225 dt=27.261 sps=36.682 mtps=0.601 mfu=13.628 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:34:49][INFO][trainer.py:516] - step=1750 loss=1.176 dt=26.890 sps=37.188 mtps=0.609 mfu=13.651 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:34:56][INFO][trainer.py:516] - step=2000 loss=1.163 dt=26.727 sps=37.415 mtps=0.613 mfu=13.680 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:35:00][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:35:00][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:35:00][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:35:07][INFO][trainer.py:516] - step=2250 loss=1.120 dt=26.733 sps=37.407 mtps=0.613 mfu=13.706 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:14][INFO][trainer.py:516] - step=2500 loss=1.068 dt=27.096 sps=36.905 mtps=0.605 mfu=13.710 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:21][INFO][trainer.py:516] - step=2750 loss=1.027 dt=26.879 sps=37.204 mtps=0.610 mfu=13.726 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:27][INFO][trainer.py:516] - step=3000 loss=1.002 dt=27.375 sps=36.530 mtps=0.599 mfu=13.714 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:32][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:35:32][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:35:32][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:35:39][INFO][trainer.py:516] - step=3250 loss=0.950 dt=26.866 sps=37.222 mtps=0.610 mfu=13.730 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:35:45][INFO][trainer.py:516] - step=3500 loss=0.926 dt=27.330 sps=36.590 mtps=0.599 mfu=13.720 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:35:52][INFO][trainer.py:516] - step=3750 loss=0.916 dt=27.203 sps=36.761 mtps=0.602 mfu=13.718 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:35:59][INFO][trainer.py:516] - step=4000 loss=0.901 dt=27.394 sps=36.504 mtps=0.598 mfu=13.706 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:36:03][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:36:03][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:36:03][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:36:10][INFO][trainer.py:516] - step=4250 loss=0.840 dt=26.814 sps=37.293 mtps=0.611 mfu=13.725 train_loss=0.703 val_loss=1.615\n    [2023-11-15 09:36:17][INFO][trainer.py:516] - step=4500 loss=0.850 dt=27.402 sps=36.494 mtps=0.598 mfu=13.713 train_loss=0.703 val_loss=1.615\n    [2023-11-15 09:36:24][INFO][trainer.py:516] - step=4750 loss=0.824 dt=26.811 sps=37.298 mtps=0.611 mfu=13.731 train_loss=0.703 val_loss=1.615\n    [2023-11-15 09:36:30][INFO][trainer.py:516] - step=5000 loss=0.819 dt=27.435 sps=36.450 mtps=0.597 mfu=13.716 train_loss=0.703 val_loss=1.615\n    [2023-11-11 01:16:27][INFO][trainer.py:516] - step=1000 loss=1.332 dt=26.899 sps=37.176 mtps=0.609 mfu=13.642 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:34][INFO][trainer.py:516] - step=1250 loss=1.277 dt=27.229 sps=36.725 mtps=0.602 mfu=13.647 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:40][INFO][trainer.py:516] - step=1500 loss=1.234 dt=26.878 sps=37.205 mtps=0.610 mfu=13.668 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:47][INFO][trainer.py:516] - step=1750 loss=1.175 dt=27.460 sps=36.417 mtps=0.597 mfu=13.659 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:54][INFO][trainer.py:516] - step=2000 loss=1.140 dt=26.889 sps=37.190 mtps=0.609 mfu=13.678 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:58][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:16:58][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:16:58][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:17:05][INFO][trainer.py:516] - step=2250 loss=1.121 dt=27.308 sps=36.619 mtps=0.600 mfu=13.675 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:12][INFO][trainer.py:516] - step=2500 loss=1.067 dt=26.838 sps=37.261 mtps=0.610 mfu=13.696 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:19][INFO][trainer.py:516] - step=2750 loss=1.034 dt=27.360 sps=36.550 mtps=0.599 mfu=13.688 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:26][INFO][trainer.py:516] - step=3000 loss=1.009 dt=26.237 sps=38.114 mtps=0.624 mfu=13.740 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:33][INFO][trainer.py:516] - step=3250 loss=0.940 dt=26.991 sps=37.050 mtps=0.607 mfu=13.746 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:39][INFO][trainer.py:516] - step=3500 loss=0.947 dt=26.261 sps=38.080 mtps=0.624 mfu=13.791 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:46][INFO][trainer.py:516] - step=3750 loss=0.885 dt=37.216 sps=26.870 mtps=0.440 mfu=13.413 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:53][INFO][trainer.py:516] - step=4000 loss=0.866 dt=26.241 sps=38.108 mtps=0.624 mfu=13.492 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:57][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:17:57][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:17:57][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:18:04][INFO][trainer.py:516] - step=4250 loss=0.847 dt=27.228 sps=36.728 mtps=0.602 mfu=13.511 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:11][INFO][trainer.py:516] - step=4500 loss=0.835 dt=26.215 sps=38.147 mtps=0.625 mfu=13.581 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:18][INFO][trainer.py:516] - step=4750 loss=0.822 dt=26.657 sps=37.513 mtps=0.615 mfu=13.621 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:24][INFO][trainer.py:516] - step=5000 loss=0.808 dt=26.635 sps=37.544 mtps=0.615 mfu=13.658 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:31][INFO][trainer.py:516] - step=5250 loss=0.811 dt=26.267 sps=38.071 mtps=0.624 mfu=13.711 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:38][INFO][trainer.py:516] - step=5500 loss=0.769 dt=26.406 sps=37.870 mtps=0.620 mfu=13.751 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:44][INFO][trainer.py:516] - step=5750 loss=0.780 dt=26.239 sps=38.111 mtps=0.624 mfu=13.796 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:51][INFO][trainer.py:516] - step=6000 loss=0.767 dt=26.682 sps=37.478 mtps=0.614 mfu=13.813 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:55][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:18:55][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:18:56][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:19:02][INFO][trainer.py:516] - step=6250 loss=0.773 dt=31.104 sps=32.151 mtps=0.527 mfu=13.629 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:09][INFO][trainer.py:516] - step=6500 loss=0.759 dt=27.142 sps=36.843 mtps=0.604 mfu=13.639 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:16][INFO][trainer.py:516] - step=6750 loss=0.753 dt=26.712 sps=37.437 mtps=0.613 mfu=13.670 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:22][INFO][trainer.py:516] - step=7000 loss=0.745 dt=26.871 sps=37.215 mtps=0.610 mfu=13.690 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:29][INFO][trainer.py:516] - step=7250 loss=0.733 dt=26.266 sps=38.072 mtps=0.624 mfu=13.740 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:36][INFO][trainer.py:516] - step=7500 loss=0.723 dt=26.817 sps=37.289 mtps=0.611 mfu=13.755 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:43][INFO][trainer.py:516] - step=7750 loss=0.747 dt=26.461 sps=37.791 mtps=0.619 mfu=13.788 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:49][INFO][trainer.py:516] - step=8000 loss=0.729 dt=29.348 sps=34.074 mtps=0.558 mfu=13.679 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:53][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:19:53][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:19:54][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:20:01][INFO][trainer.py:516] - step=8250 loss=0.718 dt=26.464 sps=37.787 mtps=0.619 mfu=13.719 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:07][INFO][trainer.py:516] - step=8500 loss=0.705 dt=27.051 sps=36.967 mtps=0.606 mfu=13.725 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:14][INFO][trainer.py:516] - step=8750 loss=0.704 dt=26.298 sps=38.026 mtps=0.623 mfu=13.769 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:21][INFO][trainer.py:516] - step=9000 loss=0.694 dt=27.131 sps=36.858 mtps=0.604 mfu=13.766 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:27][INFO][trainer.py:516] - step=9250 loss=0.700 dt=26.291 sps=38.036 mtps=0.623 mfu=13.806 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:34][INFO][trainer.py:516] - step=9500 loss=0.668 dt=27.353 sps=36.560 mtps=0.599 mfu=13.788 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:41][INFO][trainer.py:516] - step=9750 loss=0.658 dt=26.422 sps=37.847 mtps=0.620 mfu=13.819 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:48][INFO][trainer.py:516] - step=10000 loss=0.678 dt=26.887 sps=37.192 mtps=0.609 mfu=13.823 train_loss=0.473 val_loss=1.840\n\n  \n\n\n\n\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[prompt]: \"What is a supercomputer?\"\n\n\n\n[response]:\n\nWhat is a supercomputer?\nHow now, now! what news?\nHave thy sons?\n\nMessenger:\nThe queen is his noble consul;\nThe man I am a lord's, and he received:\nTherefore, consider and the hand of death.\n\nSIR STEPHEN SCROOP:\nPeace, hope, my lord; I am not thy name;\nFor I have need of this cause is so long.\n\nBISHOP OF ELY:\nBelieve me, I will practise your majesty.\nBe remember thy thoughts: give me and brothers,\nAnd towards London, till I were common all.\n\nBUCKINGHAM:\nNorthumberland, so proud weighing to fight.\n\nGLOUCESTER:\nRelent, e"
  },
  {
    "objectID": "quarto/shakespeare-clean.html#install-setup",
    "href": "quarto/shakespeare-clean.html#install-setup",
    "title": "nanoGPT",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do."
  },
  {
    "objectID": "quarto/shakespeare-clean.html#post-install",
    "href": "quarto/shakespeare-clean.html#post-install",
    "title": "nanoGPT",
    "section": "",
    "text": "If installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py"
  },
  {
    "objectID": "quarto/shakespeare-clean.html#build-trainer",
    "href": "quarto/shakespeare-clean.html#build-trainer",
    "title": "nanoGPT",
    "section": "",
    "text": "Explicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5432'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        'train.dtype=bfloat16',\n        'train.max_iters=5000',\n        'train.log_interval=250',\n        'train.eval_interval=1000',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n\n    --------------------------------------------------------------------------\n    WARNING: There was an error initializing an OpenFabrics device.\n    \n      Local host:   thetagpu23\n      Local device: mlx5_0\n    --------------------------------------------------------------------------\n    2023-11-15 09:33:41.578337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n    [2023-11-15 09:33:44][INFO][configs.py:263] - Rescaling GAS -&gt; GAS // WORLD_SIZE = 1 // 1\n    [2023-11-15 09:33:44][INFO][configs.py:398] - Tokens per iteration: 16,384\n    [2023-11-15 09:33:44][INFO][configs.py:430] - Using &lt;torch.amp.autocast_mode.autocast object at 0x7f588e33ddb0&gt;\n    [2023-11-15 09:33:44][INFO][configs.py:436] - Initializing a new model from scratch\n    [2023-11-15 09:33:44][INFO][trainer.py:179] - Initializing a new model from scratch\n    [2023-11-15 09:33:44][INFO][model.py:160] - number of parameters: 10.65M\n    [2023-11-15 09:33:45][INFO][model.py:290] - num decayed parameter tensors: 26, with 10,740,096 parameters\n    [2023-11-15 09:33:45][INFO][model.py:291] - num non-decayed parameter tensors: 13, with 4,992 parameters\n    [2023-11-15 09:33:46][INFO][model.py:297] - using fused AdamW: True\n    [2023-11-11 01:15:48][INFO][trainer.py:179] - Initializing a new model from scratch\n    [2023-11-11 01:15:48][INFO][model.py:160] - number of parameters: 10.65M\n    [2023-11-11 01:15:50][INFO][model.py:290] - num decayed parameter tensors: 26, with 10,740,096 parameters\n    [2023-11-11 01:15:50][INFO][model.py:291] - num non-decayed parameter tensors: 13, with 4,992 parameters\n    [2023-11-11 01:15:50][INFO][model.py:297] - using fused AdamW: True"
  },
  {
    "objectID": "quarto/shakespeare-clean.html#prompt-prior-to-training",
    "href": "quarto/shakespeare-clean.html#prompt-prior-to-training",
    "title": "nanoGPT",
    "section": "",
    "text": "query = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[prompt]: \"What is a supercomputer?\"\n\n\n\n[response]:\n\nWhat is a supercomputer?CbqA-RN?bnss--iadmsD\nS?qJKEwssDq YMSSSFGPxxnJLDC:cvYfOy\nfiXXe3GQQvYQARdEEbbHHPnWyFp-CwBFrr;g\nWATVAcTZCWWr\ntYCCz,E-wqNbIsMbSvYVONyaQzzcs;Iaa?WOACrnMH'':dXFEQZa-PYkAvV.B.  F$J-nKnEaZ,'vpesXY&y-M.nIcMVV!GYYVVFh-UX.G&Fa?LSPrkXd3eKV?KJjSZOwSbbhwfIYaywrvRUEuuQMnnIAZS-Ja.fXrMAHB&!!eVbUFwMIkkalHbmRhwwfcj$:s\nRlVhRcaVbYcTTihITDUbbTNMHEdnOibdB-ebuiJLLS:yarlFYHHkSWxB!hbN?nVm3-&djw'BA uS,EQJP3bbWe$hs-g\n:3jEEYU\nNkLCetHH lc-IIZEBbb-at\njyNYmvffVVnERN?LnTM:yS\nsH;are$WRip!jbX'\ne\n\npyA-jbwK 'B$O& Fvvac&sEjbIretcX-H"
  },
  {
    "objectID": "quarto/shakespeare-clean.html#train-model",
    "href": "quarto/shakespeare-clean.html#train-model",
    "title": "nanoGPT",
    "section": "",
    "text": "Legend:\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1\n\n\n\ntrainer.train()\n\n    0%|          | 0/5000 [00:00[2023-11-15 09:34:03][INFO][trainer.py:516] - step=250 loss=2.064 dt=27.412 sps=36.481 mtps=0.598 mfu=13.594 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:10][INFO][trainer.py:516] - step=500 loss=1.610 dt=26.915 sps=37.153 mtps=0.609 mfu=13.619 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:17][INFO][trainer.py:516] - step=750 loss=1.432 dt=27.775 sps=36.004 mtps=0.590 mfu=13.598 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:24][INFO][trainer.py:516] - step=1000 loss=1.346 dt=26.781 sps=37.340 mtps=0.612 mfu=13.630 train_loss=4.299 val_loss=4.291\n    [2023-11-15 09:34:28][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:34:28][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:34:28][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:34:35][INFO][trainer.py:516] - step=1250 loss=1.309 dt=27.473 sps=36.400 mtps=0.596 mfu=13.623 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:34:42][INFO][trainer.py:516] - step=1500 loss=1.225 dt=27.261 sps=36.682 mtps=0.601 mfu=13.628 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:34:49][INFO][trainer.py:516] - step=1750 loss=1.176 dt=26.890 sps=37.188 mtps=0.609 mfu=13.651 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:34:56][INFO][trainer.py:516] - step=2000 loss=1.163 dt=26.727 sps=37.415 mtps=0.613 mfu=13.680 train_loss=1.271 val_loss=1.520\n    [2023-11-15 09:35:00][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:35:00][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:35:00][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:35:07][INFO][trainer.py:516] - step=2250 loss=1.120 dt=26.733 sps=37.407 mtps=0.613 mfu=13.706 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:14][INFO][trainer.py:516] - step=2500 loss=1.068 dt=27.096 sps=36.905 mtps=0.605 mfu=13.710 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:21][INFO][trainer.py:516] - step=2750 loss=1.027 dt=26.879 sps=37.204 mtps=0.610 mfu=13.726 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:27][INFO][trainer.py:516] - step=3000 loss=1.002 dt=27.375 sps=36.530 mtps=0.599 mfu=13.714 train_loss=1.052 val_loss=1.471\n    [2023-11-15 09:35:32][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:35:32][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:35:32][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:35:39][INFO][trainer.py:516] - step=3250 loss=0.950 dt=26.866 sps=37.222 mtps=0.610 mfu=13.730 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:35:45][INFO][trainer.py:516] - step=3500 loss=0.926 dt=27.330 sps=36.590 mtps=0.599 mfu=13.720 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:35:52][INFO][trainer.py:516] - step=3750 loss=0.916 dt=27.203 sps=36.761 mtps=0.602 mfu=13.718 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:35:59][INFO][trainer.py:516] - step=4000 loss=0.901 dt=27.394 sps=36.504 mtps=0.598 mfu=13.706 train_loss=0.864 val_loss=1.531\n    [2023-11-15 09:36:03][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n    [2023-11-15 09:36:03][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n    [2023-11-15 09:36:03][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-15 09:36:10][INFO][trainer.py:516] - step=4250 loss=0.840 dt=26.814 sps=37.293 mtps=0.611 mfu=13.725 train_loss=0.703 val_loss=1.615\n    [2023-11-15 09:36:17][INFO][trainer.py:516] - step=4500 loss=0.850 dt=27.402 sps=36.494 mtps=0.598 mfu=13.713 train_loss=0.703 val_loss=1.615\n    [2023-11-15 09:36:24][INFO][trainer.py:516] - step=4750 loss=0.824 dt=26.811 sps=37.298 mtps=0.611 mfu=13.731 train_loss=0.703 val_loss=1.615\n    [2023-11-15 09:36:30][INFO][trainer.py:516] - step=5000 loss=0.819 dt=27.435 sps=36.450 mtps=0.597 mfu=13.716 train_loss=0.703 val_loss=1.615\n    [2023-11-11 01:16:27][INFO][trainer.py:516] - step=1000 loss=1.332 dt=26.899 sps=37.176 mtps=0.609 mfu=13.642 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:34][INFO][trainer.py:516] - step=1250 loss=1.277 dt=27.229 sps=36.725 mtps=0.602 mfu=13.647 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:40][INFO][trainer.py:516] - step=1500 loss=1.234 dt=26.878 sps=37.205 mtps=0.610 mfu=13.668 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:47][INFO][trainer.py:516] - step=1750 loss=1.175 dt=27.460 sps=36.417 mtps=0.597 mfu=13.659 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:54][INFO][trainer.py:516] - step=2000 loss=1.140 dt=26.889 sps=37.190 mtps=0.609 mfu=13.678 train_loss=4.299 val_loss=4.291\n    [2023-11-11 01:16:58][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:16:58][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:16:58][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:17:05][INFO][trainer.py:516] - step=2250 loss=1.121 dt=27.308 sps=36.619 mtps=0.600 mfu=13.675 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:12][INFO][trainer.py:516] - step=2500 loss=1.067 dt=26.838 sps=37.261 mtps=0.610 mfu=13.696 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:19][INFO][trainer.py:516] - step=2750 loss=1.034 dt=27.360 sps=36.550 mtps=0.599 mfu=13.688 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:26][INFO][trainer.py:516] - step=3000 loss=1.009 dt=26.237 sps=38.114 mtps=0.624 mfu=13.740 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:33][INFO][trainer.py:516] - step=3250 loss=0.940 dt=26.991 sps=37.050 mtps=0.607 mfu=13.746 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:39][INFO][trainer.py:516] - step=3500 loss=0.947 dt=26.261 sps=38.080 mtps=0.624 mfu=13.791 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:46][INFO][trainer.py:516] - step=3750 loss=0.885 dt=37.216 sps=26.870 mtps=0.440 mfu=13.413 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:53][INFO][trainer.py:516] - step=4000 loss=0.866 dt=26.241 sps=38.108 mtps=0.624 mfu=13.492 train_loss=1.050 val_loss=1.474\n    [2023-11-11 01:17:57][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:17:57][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:17:57][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:18:04][INFO][trainer.py:516] - step=4250 loss=0.847 dt=27.228 sps=36.728 mtps=0.602 mfu=13.511 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:11][INFO][trainer.py:516] - step=4500 loss=0.835 dt=26.215 sps=38.147 mtps=0.625 mfu=13.581 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:18][INFO][trainer.py:516] - step=4750 loss=0.822 dt=26.657 sps=37.513 mtps=0.615 mfu=13.621 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:24][INFO][trainer.py:516] - step=5000 loss=0.808 dt=26.635 sps=37.544 mtps=0.615 mfu=13.658 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:31][INFO][trainer.py:516] - step=5250 loss=0.811 dt=26.267 sps=38.071 mtps=0.624 mfu=13.711 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:38][INFO][trainer.py:516] - step=5500 loss=0.769 dt=26.406 sps=37.870 mtps=0.620 mfu=13.751 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:44][INFO][trainer.py:516] - step=5750 loss=0.780 dt=26.239 sps=38.111 mtps=0.624 mfu=13.796 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:51][INFO][trainer.py:516] - step=6000 loss=0.767 dt=26.682 sps=37.478 mtps=0.614 mfu=13.813 train_loss=0.696 val_loss=1.637\n    [2023-11-11 01:18:55][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:18:55][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:18:56][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:19:02][INFO][trainer.py:516] - step=6250 loss=0.773 dt=31.104 sps=32.151 mtps=0.527 mfu=13.629 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:09][INFO][trainer.py:516] - step=6500 loss=0.759 dt=27.142 sps=36.843 mtps=0.604 mfu=13.639 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:16][INFO][trainer.py:516] - step=6750 loss=0.753 dt=26.712 sps=37.437 mtps=0.613 mfu=13.670 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:22][INFO][trainer.py:516] - step=7000 loss=0.745 dt=26.871 sps=37.215 mtps=0.610 mfu=13.690 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:29][INFO][trainer.py:516] - step=7250 loss=0.733 dt=26.266 sps=38.072 mtps=0.624 mfu=13.740 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:36][INFO][trainer.py:516] - step=7500 loss=0.723 dt=26.817 sps=37.289 mtps=0.611 mfu=13.755 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:43][INFO][trainer.py:516] - step=7750 loss=0.747 dt=26.461 sps=37.791 mtps=0.619 mfu=13.788 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:49][INFO][trainer.py:516] - step=8000 loss=0.729 dt=29.348 sps=34.074 mtps=0.558 mfu=13.679 train_loss=0.556 val_loss=1.755\n    [2023-11-11 01:19:53][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n    [2023-11-11 01:19:53][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n    [2023-11-11 01:19:54][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n    [2023-11-11 01:20:01][INFO][trainer.py:516] - step=8250 loss=0.718 dt=26.464 sps=37.787 mtps=0.619 mfu=13.719 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:07][INFO][trainer.py:516] - step=8500 loss=0.705 dt=27.051 sps=36.967 mtps=0.606 mfu=13.725 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:14][INFO][trainer.py:516] - step=8750 loss=0.704 dt=26.298 sps=38.026 mtps=0.623 mfu=13.769 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:21][INFO][trainer.py:516] - step=9000 loss=0.694 dt=27.131 sps=36.858 mtps=0.604 mfu=13.766 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:27][INFO][trainer.py:516] - step=9250 loss=0.700 dt=26.291 sps=38.036 mtps=0.623 mfu=13.806 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:34][INFO][trainer.py:516] - step=9500 loss=0.668 dt=27.353 sps=36.560 mtps=0.599 mfu=13.788 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:41][INFO][trainer.py:516] - step=9750 loss=0.658 dt=26.422 sps=37.847 mtps=0.620 mfu=13.819 train_loss=0.473 val_loss=1.840\n    [2023-11-11 01:20:48][INFO][trainer.py:516] - step=10000 loss=0.678 dt=26.887 sps=37.192 mtps=0.609 mfu=13.823 train_loss=0.473 val_loss=1.840"
  },
  {
    "objectID": "quarto/shakespeare-clean.html#evaluate-model",
    "href": "quarto/shakespeare-clean.html#evaluate-model",
    "title": "nanoGPT",
    "section": "",
    "text": "query = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[prompt]: \"What is a supercomputer?\"\n\n\n\n[response]:\n\nWhat is a supercomputer?\nHow now, now! what news?\nHave thy sons?\n\nMessenger:\nThe queen is his noble consul;\nThe man I am a lord's, and he received:\nTherefore, consider and the hand of death.\n\nSIR STEPHEN SCROOP:\nPeace, hope, my lord; I am not thy name;\nFor I have need of this cause is so long.\n\nBISHOP OF ELY:\nBelieve me, I will practise your majesty.\nBe remember thy thoughts: give me and brothers,\nAnd towards London, till I were common all.\n\nBUCKINGHAM:\nNorthumberland, so proud weighing to fight.\n\nGLOUCESTER:\nRelent, e"
  },
  {
    "objectID": "quarto/shakespeare-clean.html#footnotes",
    "href": "quarto/shakespeare-clean.html#footnotes",
    "title": "nanoGPT",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin units of A100 bfloat16 peak FLOPS↩︎"
  },
  {
    "objectID": "quarto/shakespeare-ansi.html",
    "href": "quarto/shakespeare-ansi.html",
    "title": "nanoGPT",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do.\n\n\n\n\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\n\n\n\n\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5432'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        'train.dtype=bfloat16',\n        'train.max_iters=5000',\n        'train.log_interval=250',\n        'train.eval_interval=1000',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n--------------------------------------------------------------------------\nWARNING: There was an error initializing an OpenFabrics device.\n\n  Local host:   thetagpu23\n  Local device: mlx5_0\n--------------------------------------------------------------------------\n2023-11-15 09:33:41.578337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:263\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Rescaling GAS -&gt; GAS \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m WORLD_SIZE = \u001b[35m1\u001b[0m \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m \u001b[35m1\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:398\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Tokens per iteration: \u001b[35m16\u001b[0m,\u001b[35m384\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:430\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Using \u001b[1m&lt;\u001b[0m\u001b[1;95mtorch.amp.autocast_mode.autocast\u001b[0m\u001b[39m object at \u001b[0m\u001b[35m0x7f588e33ddb0\u001b[0m\u001b[1m&gt;\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:436\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing a new model from scratch\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:179\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing a new model from scratch\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:160\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - number of parameters: \u001b[35m10.\u001b[0m65M\n\u001b[38;2;131;131;131m[2023-11-15 09:33:45]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:290\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num decayed parameter tensors: \u001b[35m26\u001b[0m, with \u001b[35m10\u001b[0m,\u001b[35m740\u001b[0m,\u001b[35m096\u001b[0m parameters\n\u001b[38;2;131;131;131m[2023-11-15 09:33:45]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:291\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num non-decayed parameter tensors: \u001b[35m13\u001b[0m, with \u001b[35m4\u001b[0m,\u001b[35m992\u001b[0m parameters\n\u001b[38;2;131;131;131m[2023-11-15 09:33:46]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:297\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - using fused AdamW: \u001b[3;92mTrue\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:15:48]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:179\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing a new model from scratch\n\u001b[38;2;131;131;131m[2023-11-11 01:15:48]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:160\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - number of parameters: \u001b[35m10.\u001b[0m65M\n\u001b[38;2;131;131;131m[2023-11-11 01:15:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:290\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num decayed parameter tensors: \u001b[35m26\u001b[0m, with \u001b[35m10\u001b[0m,\u001b[35m740\u001b[0m,\u001b[35m096\u001b[0m parameters\n\u001b[38;2;131;131;131m[2023-11-11 01:15:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:291\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num non-decayed parameter tensors: \u001b[35m13\u001b[0m, with \u001b[35m4\u001b[0m,\u001b[35m992\u001b[0m parameters\n\u001b[38;2;131;131;131m[2023-11-11 01:15:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:297\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - using fused AdamW: \u001b[3;92mTrue\u001b[0m\n\n\n\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n[prompt]: \"What is a supercomputer?\"\n\n[response]:\n\nWhat is a supercomputer?CbqA-RN?bnss--iadmsD\nS?qJKEwssDq YMSSSFGPxxnJLDC:cvYfOy\nfiXXe3GQQvYQARdEEbbHHPnWyFp-CwBFrr;g\nWATVAcTZCWWr\ntYCCz,E-wqNbIsMbSvYVONyaQzzcs;Iaa?WOACrnMH'':dXFEQZa-PYkAvV.B.  F$J-nKnEaZ,'vpesXY&y-M.nIcMVV!GYYVVFh-UX.G&Fa?LSPrkXd3eKV?KJjSZOwSbbhwfIYaywrvRUEuuQMnnIAZS-Ja.fXrMAHB&!!eVbUFwMIkkalHbmRhwwfcj$:s\nRlVhRcaVbYcTTihITDUbbTNMHEdnOibdB-ebuiJLLS:yarlFYHHkSWxB!hbN?nVm3-&djw'BA uS,EQJP3bbWe$hs-g\n:3jEEYU\nNkLCetHH lc-IIZEBbb-at\njyNYmvffVVnERN?LnTM:yS\nsH;are$WRip!jbX'\ne\n\npyA-jbwK 'B$O& Fvvac&sEjbIretcX-H\n\n\n\n\nLegend:\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1\n\n\n\ntrainer.train()\n  0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\u001b[38;2;131;131;131m[2023-11-15 09:34:03]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m2\u001b[0m\u001b[35m.064\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.412\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.481\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.598\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.594\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:10]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.915\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.153\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:17]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.432\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.775\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.004\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.590\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.598\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:24]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.346\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.781\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.340\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.612\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.630\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:28]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:28]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:28]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:35]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.309\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.400\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.596\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.623\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.271\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.520\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:42]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.225\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.261\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.682\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.601\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.628\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.271\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.520\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:49]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.176\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.890\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.188\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.651\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.271\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.520\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:56]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.163\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.727\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.415\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.613\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.680\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.271\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.520\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:00]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:00]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:00]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:07]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.120\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.733\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.407\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.613\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.706\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.052\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.471\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:14]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.068\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.096\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.905\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.605\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.710\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.052\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.471\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:21]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.027\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.879\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.204\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.726\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.052\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.471\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.002\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.375\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.530\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.599\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.714\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.052\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.471\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:32]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:32]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:32]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:39]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.950\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.866\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.222\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.730\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.864\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.531\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:45]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.926\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.330\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.590\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.599\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.720\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.864\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.531\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:52]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.916\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.203\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.761\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.602\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.718\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.864\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.531\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:59]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.901\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.394\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.504\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.598\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.706\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.864\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.531\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:03]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:03]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:03]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:10]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.840\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.814\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.293\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.611\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.725\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.703\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.615\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:17]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.850\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.402\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.494\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.598\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.713\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.703\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.615\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:24]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.824\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.811\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.298\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.611\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.731\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.703\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.615\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:30]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.819\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.435\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.450\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.597\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.716\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.703\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.615\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.332\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.899\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.176\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.642\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:34]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.277\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.229\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.725\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.602\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.647\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:40]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.234\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.878\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.205\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.668\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:47]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.175\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.460\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.417\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.597\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.659\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:54]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.140\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.889\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.190\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.678\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:58]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:58]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:58]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:05]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.121\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.308\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.600\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.675\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:12]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.067\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.838\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.261\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:19]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.034\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.360\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.550\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.599\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.688\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:26]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.009\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.237\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.114\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.740\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:33]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.940\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.991\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.607\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.746\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:39]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.947\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.261\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.080\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.791\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:46]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.885\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.216\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.870\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.440\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.413\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.866\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.241\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.108\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.492\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:57]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:57]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:57]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:04]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.847\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.228\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.728\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.602\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.511\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:11]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.835\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.215\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.147\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.625\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.581\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:18]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.822\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.657\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.513\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.615\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.621\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:24]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.808\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.635\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.544\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.615\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.658\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:31]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.811\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.267\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.071\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.711\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:38]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.769\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.406\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.870\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.620\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.751\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.780\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.239\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.111\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.796\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:51]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.767\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.682\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.478\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.614\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.813\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:55]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:55]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:56]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:02]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.773\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m31\u001b[0m\u001b[35m.104\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m32\u001b[0m\u001b[35m.151\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.527\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.629\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:09]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.759\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.142\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.843\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.604\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.639\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:16]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.753\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.712\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.437\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.613\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.670\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:22]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.745\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.871\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.215\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.690\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:29]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.733\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.266\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.072\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.740\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:36]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.723\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.817\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.289\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.611\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.755\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:43]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.747\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.461\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.791\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.788\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:49]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.729\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m29\u001b[0m\u001b[35m.348\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m34\u001b[0m\u001b[35m.074\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.558\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.679\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:54]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.718\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.464\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.787\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.719\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:07]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.705\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.051\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.967\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.606\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.725\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:14]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.704\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.298\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.026\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.623\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.769\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:21]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.694\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.131\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.858\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.604\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.766\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.700\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.291\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.036\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.623\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.806\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:34]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.668\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.353\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.560\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.599\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.788\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:41]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.658\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.422\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.847\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.620\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.819\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:48]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m10000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.678\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.887\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.192\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.823\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\n\n\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n[prompt]: \"What is a supercomputer?\"\n\n[response]:\n\nWhat is a supercomputer?\nHow now, now! what news?\nHave thy sons?\n\nMessenger:\nThe queen is his noble consul;\nThe man I am a lord's, and he received:\nTherefore, consider and the hand of death.\n\nSIR STEPHEN SCROOP:\nPeace, hope, my lord; I am not thy name;\nFor I have need of this cause is so long.\n\nBISHOP OF ELY:\nBelieve me, I will practise your majesty.\nBe remember thy thoughts: give me and brothers,\nAnd towards London, till I were common all.\n\nBUCKINGHAM:\nNorthumberland, so proud weighing to fight.\n\nGLOUCESTER:\nRelent, e"
  },
  {
    "objectID": "quarto/shakespeare-ansi.html#install-setup",
    "href": "quarto/shakespeare-ansi.html#install-setup",
    "title": "nanoGPT",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do."
  },
  {
    "objectID": "quarto/shakespeare-ansi.html#post-install",
    "href": "quarto/shakespeare-ansi.html#post-install",
    "title": "nanoGPT",
    "section": "",
    "text": "If installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py"
  },
  {
    "objectID": "quarto/shakespeare-ansi.html#build-trainer",
    "href": "quarto/shakespeare-ansi.html#build-trainer",
    "title": "nanoGPT",
    "section": "",
    "text": "Explicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5432'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        'train.dtype=bfloat16',\n        'train.max_iters=5000',\n        'train.log_interval=250',\n        'train.eval_interval=1000',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n--------------------------------------------------------------------------\nWARNING: There was an error initializing an OpenFabrics device.\n\n  Local host:   thetagpu23\n  Local device: mlx5_0\n--------------------------------------------------------------------------\n2023-11-15 09:33:41.578337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:263\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Rescaling GAS -&gt; GAS \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m WORLD_SIZE = \u001b[35m1\u001b[0m \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m \u001b[35m1\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:398\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Tokens per iteration: \u001b[35m16\u001b[0m,\u001b[35m384\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:430\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Using \u001b[1m&lt;\u001b[0m\u001b[1;95mtorch.amp.autocast_mode.autocast\u001b[0m\u001b[39m object at \u001b[0m\u001b[35m0x7f588e33ddb0\u001b[0m\u001b[1m&gt;\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:436\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing a new model from scratch\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:179\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing a new model from scratch\n\u001b[38;2;131;131;131m[2023-11-15 09:33:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:160\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - number of parameters: \u001b[35m10.\u001b[0m65M\n\u001b[38;2;131;131;131m[2023-11-15 09:33:45]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:290\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num decayed parameter tensors: \u001b[35m26\u001b[0m, with \u001b[35m10\u001b[0m,\u001b[35m740\u001b[0m,\u001b[35m096\u001b[0m parameters\n\u001b[38;2;131;131;131m[2023-11-15 09:33:45]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:291\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num non-decayed parameter tensors: \u001b[35m13\u001b[0m, with \u001b[35m4\u001b[0m,\u001b[35m992\u001b[0m parameters\n\u001b[38;2;131;131;131m[2023-11-15 09:33:46]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:297\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - using fused AdamW: \u001b[3;92mTrue\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:15:48]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:179\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing a new model from scratch\n\u001b[38;2;131;131;131m[2023-11-11 01:15:48]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:160\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - number of parameters: \u001b[35m10.\u001b[0m65M\n\u001b[38;2;131;131;131m[2023-11-11 01:15:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:290\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num decayed parameter tensors: \u001b[35m26\u001b[0m, with \u001b[35m10\u001b[0m,\u001b[35m740\u001b[0m,\u001b[35m096\u001b[0m parameters\n\u001b[38;2;131;131;131m[2023-11-11 01:15:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:291\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num non-decayed parameter tensors: \u001b[35m13\u001b[0m, with \u001b[35m4\u001b[0m,\u001b[35m992\u001b[0m parameters\n\u001b[38;2;131;131;131m[2023-11-11 01:15:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:297\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - using fused AdamW: \u001b[3;92mTrue\u001b[0m"
  },
  {
    "objectID": "quarto/shakespeare-ansi.html#prompt-prior-to-training",
    "href": "quarto/shakespeare-ansi.html#prompt-prior-to-training",
    "title": "nanoGPT",
    "section": "",
    "text": "query = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n[prompt]: \"What is a supercomputer?\"\n\n[response]:\n\nWhat is a supercomputer?CbqA-RN?bnss--iadmsD\nS?qJKEwssDq YMSSSFGPxxnJLDC:cvYfOy\nfiXXe3GQQvYQARdEEbbHHPnWyFp-CwBFrr;g\nWATVAcTZCWWr\ntYCCz,E-wqNbIsMbSvYVONyaQzzcs;Iaa?WOACrnMH'':dXFEQZa-PYkAvV.B.  F$J-nKnEaZ,'vpesXY&y-M.nIcMVV!GYYVVFh-UX.G&Fa?LSPrkXd3eKV?KJjSZOwSbbhwfIYaywrvRUEuuQMnnIAZS-Ja.fXrMAHB&!!eVbUFwMIkkalHbmRhwwfcj$:s\nRlVhRcaVbYcTTihITDUbbTNMHEdnOibdB-ebuiJLLS:yarlFYHHkSWxB!hbN?nVm3-&djw'BA uS,EQJP3bbWe$hs-g\n:3jEEYU\nNkLCetHH lc-IIZEBbb-at\njyNYmvffVVnERN?LnTM:yS\nsH;are$WRip!jbX'\ne\n\npyA-jbwK 'B$O& Fvvac&sEjbIretcX-H"
  },
  {
    "objectID": "quarto/shakespeare-ansi.html#train-model",
    "href": "quarto/shakespeare-ansi.html#train-model",
    "title": "nanoGPT",
    "section": "",
    "text": "Legend:\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1\n\n\n\ntrainer.train()\n  0%|          | 0/5000 [00:00&lt;?, ?it/s]\n\n\n\u001b[38;2;131;131;131m[2023-11-15 09:34:03]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m2\u001b[0m\u001b[35m.064\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.412\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.481\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.598\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.594\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:10]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.915\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.153\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:17]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.432\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.775\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.004\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.590\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.598\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:24]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.346\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.781\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.340\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.612\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.630\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:28]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:28]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:28]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:35]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.309\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.400\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.596\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.623\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.271\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.520\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:42]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.225\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.261\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.682\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.601\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.628\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.271\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.520\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:49]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.176\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.890\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.188\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.651\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.271\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.520\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:34:56]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.163\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.727\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.415\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.613\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.680\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.271\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.520\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:00]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:00]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:00]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:07]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.120\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.733\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.407\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.613\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.706\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.052\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.471\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:14]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.068\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.096\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.905\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.605\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.710\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.052\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.471\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:21]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.027\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.879\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.204\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.726\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.052\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.471\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.002\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.375\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.530\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.599\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.714\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.052\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.471\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:32]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:32]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:32]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:39]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.950\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.866\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.222\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.730\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.864\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.531\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:45]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.926\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.330\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.590\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.599\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.720\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.864\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.531\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:52]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.916\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.203\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.761\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.602\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.718\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.864\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.531\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:35:59]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.901\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.394\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.504\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.598\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.706\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.864\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.531\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:03]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:03]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:03]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/\u001b[0m\u001b[35mnotebooks\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:10]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.840\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.814\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.293\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.611\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.725\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.703\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.615\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:17]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.850\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.402\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.494\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.598\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.713\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.703\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.615\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:24]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.824\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.811\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.298\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.611\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.731\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.703\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.615\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-15 09:36:30]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.819\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.435\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.450\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.597\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.716\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.703\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.615\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.332\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.899\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.176\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.642\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:34]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.277\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.229\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.725\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.602\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.647\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:40]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.234\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.878\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.205\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.668\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:47]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.175\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.460\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.417\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.597\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.659\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:54]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.140\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.889\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.190\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.678\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:58]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:58]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:16:58]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:05]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.121\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.308\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.600\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.675\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:12]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.067\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.838\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.261\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:19]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.034\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.360\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.550\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.599\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.688\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:26]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.009\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.237\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.114\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.740\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:33]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.940\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.991\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.607\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.746\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:39]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.947\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.261\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.080\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.791\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:46]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.885\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.216\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.870\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.440\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.413\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.866\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.241\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.108\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.492\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:57]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:57]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:17:57]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:04]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.847\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.228\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.728\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.602\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.511\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:11]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.835\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.215\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.147\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.625\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.581\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:18]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.822\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.657\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.513\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.615\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.621\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:24]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.808\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.635\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.544\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.615\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.658\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:31]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.811\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.267\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.071\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.711\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:38]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.769\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.406\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.870\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.620\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.751\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.780\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.239\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.111\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.796\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:51]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.767\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.682\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.478\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.614\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.813\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:55]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:55]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:18:56]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:02]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.773\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m31\u001b[0m\u001b[35m.104\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m32\u001b[0m\u001b[35m.151\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.527\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.629\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:09]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.759\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.142\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.843\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.604\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.639\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:16]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.753\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.712\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.437\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.613\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.670\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:22]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.745\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.871\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.215\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.690\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:29]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.733\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.266\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.072\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.740\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:36]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.723\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.817\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.289\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.611\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.755\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:43]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.747\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.461\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.791\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.788\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:49]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.729\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m29\u001b[0m\u001b[35m.348\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m34\u001b[0m\u001b[35m.074\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.558\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.679\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:19:54]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.718\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.464\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.787\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.719\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:07]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.705\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.051\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.967\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.606\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.725\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:14]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.704\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.298\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.026\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.623\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.769\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:21]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.694\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.131\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.858\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.604\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.766\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.700\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.291\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.036\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.623\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.806\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:34]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.668\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.353\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.560\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.599\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.788\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:41]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.658\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.422\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.847\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.620\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.819\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-11 01:20:48]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m10000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.678\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.887\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.192\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.823\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m"
  },
  {
    "objectID": "quarto/shakespeare-ansi.html#evaluate-model",
    "href": "quarto/shakespeare-ansi.html#evaluate-model",
    "title": "nanoGPT",
    "section": "",
    "text": "query = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n[prompt]: \"What is a supercomputer?\"\n\n[response]:\n\nWhat is a supercomputer?\nHow now, now! what news?\nHave thy sons?\n\nMessenger:\nThe queen is his noble consul;\nThe man I am a lord's, and he received:\nTherefore, consider and the hand of death.\n\nSIR STEPHEN SCROOP:\nPeace, hope, my lord; I am not thy name;\nFor I have need of this cause is so long.\n\nBISHOP OF ELY:\nBelieve me, I will practise your majesty.\nBe remember thy thoughts: give me and brothers,\nAnd towards London, till I were common all.\n\nBUCKINGHAM:\nNorthumberland, so proud weighing to fight.\n\nGLOUCESTER:\nRelent, e"
  },
  {
    "objectID": "quarto/shakespeare-ansi.html#footnotes",
    "href": "quarto/shakespeare-ansi.html#footnotes",
    "title": "nanoGPT",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin units of A100 bfloat16 peak FLOPS↩︎"
  },
  {
    "objectID": "quarto/gpt2-xl-jlab.html",
    "href": "quarto/gpt2-xl-jlab.html",
    "title": "nanoGPT",
    "section": "",
    "text": "Back to topCitationBibTeX citation:@online{foreman2023,\n  author = {Foreman, Sam},\n  title = {nanoGPT},\n  date = {2023-11-15},\n  url = {https://saforem2.github.io/nanoGPT},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2023. “nanoGPT.” November 15, 2023. https://saforem2.github.io/nanoGPT."
  },
  {
    "objectID": "quarto/gpt2-xl-jlab.html#install-setup",
    "href": "quarto/gpt2-xl-jlab.html#install-setup",
    "title": "nanoGPT",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do."
  },
  {
    "objectID": "quarto/gpt2-xl-jlab.html#post-install",
    "href": "quarto/gpt2-xl-jlab.html#post-install",
    "title": "nanoGPT",
    "section": "",
    "text": "If installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py"
  },
  {
    "objectID": "quarto/gpt2-xl-jlab.html#build-trainer",
    "href": "quarto/gpt2-xl-jlab.html#build-trainer",
    "title": "nanoGPT",
    "section": "",
    "text": "Explicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5127'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nSEED = np.random.randint(2**32)\nconsole.print(f'SEED: {SEED}')\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=owt',\n        'model=gpt2_xl',\n        'optimizer=gpt2_xl',\n        'train=gpt2_xl',\n        'train.init_from=gpt2-xl',\n        'train.max_iters=100',\n        'train.dtype=bfloat16',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n--------------------------------------------------------------------------\nWARNING: There was an error initializing an OpenFabrics device.\n\n  Local host:   thetagpu24\n  Local device: mlx5_0\n--------------------------------------------------------------------------\nSEED: 125313342\n\nRANK: 0 / 0\n\n\u001b[38;2;131;131;131m[2023-11-10 17:36:01]\u001b[0m\u001b[33m[WARNING]\u001b[0m\u001b[38;2;119;119;119m[configs.py:298\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - No meta.pkl found, assuming GPT-\u001b[35m2\u001b[0m encodings\u001b[33m...\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-10 17:36:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:264\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Rescaling GAS -&gt; GAS \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m WORLD_SIZE = \u001b[35m1\u001b[0m \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m \u001b[35m1\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-10 17:36:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:399\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Tokens per iteration: \u001b[35m1\u001b[0m,\u001b[35m024\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-10 17:36:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:431\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Using \u001b[1m&lt;\u001b[0m\u001b[1;95mtorch.amp.autocast_mode.autocast\u001b[0m\u001b[39m object at \u001b[0m\u001b[35m0x7f98e0139660\u001b[0m\u001b[1m&gt;\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-10 17:36:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:184\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing from OpenAI GPT-\u001b[35m2\u001b[0m Weights: gpt2-xl\n\n\n2023-11-10 17:36:01.777923: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n[2023-11-10 17:36:05,925] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\n\u001b[38;2;131;131;131m[2023-11-10 17:36:06]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:225\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - loading weights from pretrained gpt: gpt2-xl\n\u001b[38;2;131;131;131m[2023-11-10 17:36:06]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:234\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - forcing \u001b[3;94mvocab_size\u001b[0m=\u001b[35m50257\u001b[0m, \u001b[3;94mblock_size\u001b[0m=\u001b[35m1024\u001b[0m, \u001b[3;94mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-10 17:36:06]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:240\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - overriding dropout rate to \u001b[35m0.0\u001b[0m\n\u001b[38;2;131;131;131m[2023-11-10 17:36:29]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:160\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - number of parameters: \u001b[35m1555.\u001b[0m97M\n\u001b[38;2;131;131;131m[2023-11-10 17:36:56]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:290\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num decayed parameter tensors: \u001b[35m194\u001b[0m, with \u001b[35m1\u001b[0m,\u001b[35m556\u001b[0m,\u001b[35m609\u001b[0m,\u001b[35m600\u001b[0m parameters\n\u001b[38;2;131;131;131m[2023-11-10 17:36:56]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:291\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num non-decayed parameter tensors: \u001b[35m386\u001b[0m, with \u001b[35m1\u001b[0m,\u001b[35m001\u001b[0m,\u001b[35m600\u001b[0m parameters\n\u001b[38;2;131;131;131m[2023-11-10 17:36:56]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[model.py:297\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - using fused AdamW: \u001b[3;92mTrue\u001b[0m"
  },
  {
    "objectID": "quarto/gpt2-xl-jlab.html#prompt-prior-to-training",
    "href": "quarto/gpt2-xl-jlab.html#prompt-prior-to-training",
    "title": "nanoGPT",
    "section": "",
    "text": "query = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n[prompt]: \"What is a supercomputer?\"\n\n[response]:\n\nWhat is a supercomputer? When it comes to massive computing, a supercomputer is simply a large computer system that has the ability to perform many calculations at once. This can be the result of using many different processing cores, or memory, or operating at a high clock speed. Supercomputers are often used to crack complex calculations and research problems.\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nOn a larger scale, these massive computers are used to solve tough mathematical equations and solve hard scientific problems. They are very powerful enough to emulate the workings of the human brain and simulate a human intelligence in a virtual world.\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nIn 1992, IBM's NeXTStep supercomputer was the largest and most powerful supercomputer in the world. It was released in 1995 and did not continue to live up to its original promises, because its capabilities were quickly surpassed by its competitors.\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia\n\nImage credit: Wikipedia&lt;|endoftext|&gt;Editor's note: Dan De Luce is the author of \"When the Going Gets Tough: The New Survival Guide for College Students and Your Health and Well-Being.\"\n\nCollege has never been more expensive. But with so many choices and so many choices of where to go, it's harder than ever for prospective students to find a college that fits their lifestyle.\n\nThis is a problem—not just because it can be a hassle to find a college that doesn't require a large amount of financial aid. It's a problem because it can be costly for students to stay in college.\n\nSo I created this list of colleges with the highest tuition where"
  },
  {
    "objectID": "quarto/gpt2-xl-jlab.html#train-model",
    "href": "quarto/gpt2-xl-jlab.html#train-model",
    "title": "nanoGPT",
    "section": "",
    "text": "NAME\nDESCRIPTION\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops Utilization*\n\n\n\n^Logging Legend\n*in units of A100 bfloat16 peak FLOPS\ntrainer.model.module.train()\ntrainer.train()\n  0%|          | 0/100 [00:00&lt;?, ?it/s]\n\n\n\u001b[38;2;131;131;131m[2023-11-10 17:41:58]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:540\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m100\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m2\u001b[0m\u001b[35m.505\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m922\u001b[0m\u001b[35m.295\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.084\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.001\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m43\u001b[0m\u001b[35m.897\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m2\u001b[0m\u001b[35m.555\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m2\u001b[0m\u001b[35m.558\u001b[0m"
  },
  {
    "objectID": "quarto/gpt2-xl-jlab.html#evaluate-model",
    "href": "quarto/gpt2-xl-jlab.html#evaluate-model",
    "title": "nanoGPT",
    "section": "",
    "text": "query = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nfrom rich.text import Text\nfrom enrich.console import get_console\nconsole = get_console()\n\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n[prompt]: \"What is a supercomputer?\"\n\n[response]:\n\nWhat is a supercomputer? A supercomputer is a machine that is exponentially more powerful than previous computing models while being far more energy efficient.\n\nWhat is an artificial neural network? An artificial neural network (ANN) is an order of magnitude more powerful than previous computational models, but has the same energy efficiency.\n\nFor this article I will be using a machine learning technique called Backward-Compatible Neural Networks (BCNNs) to represent the biological brain.\n\nThe BCNNs model is very similar to the neural networks utilized in deep learning, but has the added bonus of being able to 'decouple' the learning from the final results.\n\nBCNN for Machine Learning\n\nIn order to make the transition from neural networks to BCNNs we will follow the same basic principles as we did with neural networks.\n\nHowever, instead of the neurons in neural networks that represent the data being represented, BCNNs work with nodes instead. This is because the nodes are the data, while the neurons are the information.\n\nIn case you aren’t familiar with the term node, it is a symbol representing any type of data. For instance, it could be a datum in a neural network model.\n\nAnother way to think of them is as symbols.\n\nThe basic idea of nodes and connections is that a node can have many connections to other nodes, with each node linked to a connection to a larger entity.\n\nFor instance, a node might have a target, which is just a point in space. A connection might have a value, which is just a number between 0 and 1.\n\nSomething like this:\n\nNode Value -0.1 0.1 0.1 0.1\n\nThe important thing to note, is that the value is a number between 0 and 1.\n\nWhen we are given a list of data and an input, we will move forward through the data, connected nodes, and the resulting output.\n\nIn the case of neural networks, this would look like:\n\nNeural Network\n\nA neural network is just a collection of nodes, connected to each other through connections.\n\nFor example, let’s look at the ConvNet model from Wikipedia.\n\nPretty simple. It has multiple layers of neurons, with each neuron being assigned one of the above variables.\n\nThe neurons work with the data given as an input (remember, it’s a"
  },
  {
    "objectID": "quarto/gpt2-medium.html",
    "href": "quarto/gpt2-medium.html",
    "title": "nanoGPT",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n\n    /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\n    Has ngpt installed. Nothing to do.\n\n\n\n\n\n\n\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\n\n    The autoreload extension is already loaded. To reload it, use:\n      %reload_ext autoreload\n\n    /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\n\n\n\n\n\n\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5127'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nSEED = np.random.randint(2**32)\nconsole.log(f'SEED: {SEED}')\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=owt',\n        'model=gpt2_medium',\n        'optimizer=gpt2_medium',\n        'train=gpt2_medium',\n        'train.dtype=bfloat16',\n        'train.max_iters=1000',\n        'train.log_interval=100',\n        'train.init_from=gpt2-medium',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n\n    --------------------------------------------------------------------------\n    WARNING: There was an error initializing an OpenFabrics device.\n\n      Local host:   thetagpu23\n      Local device: mlx5_0\n    --------------------------------------------------------------------------\n    2023-11-15 09:48:20.191135: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n    [09:48:22] SEED: 627182480\n    [2023-11-15 09:48:23][WARNING][configs.py:297] - No meta.pkl found, assuming GPT-2 encodings...\n    [2023-11-15 09:48:26][INFO][configs.py:263] - Rescaling GAS -&gt; GAS // WORLD_SIZE = 1 // 1\n    [2023-11-15 09:48:26][INFO][configs.py:398] - Tokens per iteration: 4,096\n    [2023-11-15 09:48:26][INFO][configs.py:430] - Using &lt;torch.amp.autocast_mode.autocast object at 0x7fcbf3a11930&gt;\n    [2023-11-15 09:48:26][INFO][trainer.py:187] - Initializing from OpenAI GPT-2 Weights: gpt2-medium\n    [2023-11-15 09:48:32,281] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n    [2023-11-15 09:48:49][INFO][model.py:225] - loading weights from pretrained gpt: gpt2-medium\n    [2023-11-15 09:48:49][INFO][model.py:234] - forcing vocab_size=50257, block_size=1024, bias=True\n    [2023-11-15 09:48:49][INFO][model.py:240] - overriding dropout rate to 0.0\n    [2023-11-15 09:48:55][INFO][model.py:160] - number of parameters: 353.77M\n    Downloading (…)lve/main/config.json:   0%|| 0.00/718 [00:00[2023-11-15 09:49:16][INFO][model.py:290] - num decayed parameter tensors: 98, with 354,501,632 parameters\n    [2023-11-15 09:49:16][INFO][model.py:291] - num non-decayed parameter tensors: 194, with 321,536 parameters\n    [2023-11-15 09:49:17][INFO][model.py:297] - using fused AdamW: True\n\n\n\n\n\n\nquery = \"What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic..\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n    [prompt]: \"What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic..\"\n\n    [response]:\n\n    What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic.. When I ask you to describe your computer, this is what you know: a computer that runs a quantum computer to run a computer program. There are now two supercomputers that can do quantum computations, so how can you explain quantum computing? We will get into what quantum computing is first, and then we are going to talk to you about what is a supercomputer. You will learn how to explain quantum computing to a child, and you will find out that quantum computing is what is called artificial intelligence (AI). Do you understand quantum computing?\n    Follow up – Answering the Quantum Computing Questions\n    Cognitive scientists are trying to define what is called artificial intelligence (AI). They are trying to understand what computers can do. At the heart of the argument are the following three questions:\n    What is AI and how does it apply to science?\n    What if I say, \"A computer can tell me if I am an elf, a lion, a fox, or a squirrel?\". Can I make a decision?\n    How does AI fit into the human condition?\n    Have you ever thought about what is intelligence? Have you ever wondered, \"What if I want to create a computer that can build me an AI, and that computer is smarter than me? How will the AI affect my life?\".\n    Before you even begin…\n    BECAUSE YOU HAVE NOT BEEN PURCHASED\n    This is where you may want to get an answer to a question that you have never had the opportunity to ask, and which is too big to ask right now. Think about it. Will I be able to make decisions? Will I be able to ask questions? Do I have the right to be an AI?\n    With an intelligent AI, you can have the ability to run my brain. You can have your mind, and you can control it. You can control your body, you can control your thoughts, and you can control your actions. That will make you a living being. If you are able to experience the world like the rest of us, you will have a better understanding of what it means to be alive; a living being. You will have the ability to observe, dream, and experience. All this will be possible to you if you are created as an intelligent being.\n    By creating an AI, this will be easier. It will be easy for someone to create an AI, and then run it as if\n\n\n\n\n\nTable 1: Legend\n\n\nName\nDescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1\n\n\n\n\n\n\n\ntrainer.model.module.train()\ntrainer.train()\n\n    [2023-11-15 09:50:50][INFO][trainer.py:516] - step=100 loss=2.791 dt=387.530 sps=2.580 mtps=0.011 mfu=24.642 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:51:28][INFO][trainer.py:516] - step=200 loss=2.716 dt=375.216 sps=2.665 mtps=0.011 mfu=24.722 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:52:07][INFO][trainer.py:516] - step=300 loss=2.701 dt=398.145 sps=2.512 mtps=0.010 mfu=24.649 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:52:45][INFO][trainer.py:516] - step=400 loss=2.858 dt=376.159 sps=2.658 mtps=0.011 mfu=24.722 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:53:24][INFO][trainer.py:516] - step=500 loss=2.542 dt=422.272 sps=2.368 mtps=0.010 mfu=24.512 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:54:03][INFO][trainer.py:516] - step=600 loss=2.912 dt=406.393 sps=2.461 mtps=0.010 mfu=24.410 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:54:42][INFO][trainer.py:516] - step=700 loss=2.862 dt=369.661 sps=2.705 mtps=0.011 mfu=24.552 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:55:21][INFO][trainer.py:516] - step=800 loss=2.849 dt=336.193 sps=2.974 mtps=0.012 mfu=24.938 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:56:00][INFO][trainer.py:516] - step=900 loss=2.586 dt=387.251 sps=2.582 mtps=0.011 mfu=24.910 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:56:38][INFO][trainer.py:516] - step=1000 loss=2.763 dt=373.859 sps=2.675 mtps=0.011 mfu=24.973 train_loss=2.837 val_loss=2.826\n\n\n\n\n\n\nquery = \"What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic..\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n    [prompt]: \"What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic..\"\n\n    [response]:\n\n    What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic.. Does it say that when you say \"I want to be famous\" that you should be famous as a songwriter too? Does it say that you should have a place in your life or in an organization? Is it saying that you should be regarded as a \"supercomputer\" and have access to the best software on earth?\n\n    Is it saying that you are a computer programmer?\n\n    Does it say \"you are a computer scientist\"?\n\n    Does it say \"you are a computer scientist\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer?\"\n\n     does it say \"you are a computer programmer?\"\n\n    does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer\"?\" Doesn it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer?\"\n\n    Does\n\n\n\n\n\n\ntrainer.model.module.train()\nfor iter in range(10):\n    console.rule(f'iter: {iter}')\n    trainer.train(train_iters=100)\n    query = \"What is a supercomputer?\"\n    outputs = trainer.evaluate(query, num_samples=1, display=False)\n    console.print(fr'\\[prompt]: \"{query}\"')\n    console.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n    console.rule()\n\n    ───────────────────────────────────────────────────── iter: 0 ─────────────────────────────────────────────────────\n\n    [2023-11-15 09:58:35][INFO][trainer.py:516] - step=1100 loss=2.939 dt=356.182 sps=2.808 mtps=0.011 mfu=26.810 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer consists of two parts: “a processor and a network of interconnected graphics processors, called supercomputers,” said Richard Andrews, a computer scientist at the University of Edinburgh.\n\n    Image copyright PA Image caption The first three are the biggest computers in the world, but they’re all fitted with a touchscreen\n\n    The processor is a small computer, usually a single-core one. Usually it is made of silicon and it’s the first, and most expensive, part of the complex system. The other two parts of the system are: a chip to power the processor and a memory and software stack to allow for the system to interact on the internet.\n\n    The first is the processor. It runs a complex combination of algorithms. These algorithms work to compress higher-level data, but also do some processing to understand the lower levels of the system.\n\n    According to the Oxford computer science textbook, algorithms are:\n\n    It’s a special structure which allows a system to achieve a particular level of efficiency, such as a faster processor or a faster system of atoms or molecules.\n\n    Researchers use algorithms to implement certain types of systems like computers or medicine. But a supercomputer can also work faster, or faster, to solve problems.\n\n    Image copyright PA Image caption The supercomputer in the US can run things like social networks and video games\n\n    A supercomputer has a processor inside, but there is also a memory and software to process it. This allows a system to do something faster. This is called a supercomputer’s memory.\n\n    Image copyright Getty Images Image caption Computing power has increased, but it’s also improved the power of computers in recent years\n\n    The second piece of the system is the computer network. This is a computer system that works together to perform calculations.\n\n    This is a huge system. It is where all the computers in the world - computers in different countries, continents and different time zones - work together.\n\n    The network works and converts the data on the computers into calculations. This is the core of a supercomputer.\n\n    What goes on inside a supercomputer?\n\n    Image copyright PA Image caption The supercomputer works together to work out human intelligence\n\n    It can interact with the computer network via a touchscreen.\n\n    The other pieces of the system are:\n\n    A computer system can send and receive data through a touchscreen. It is\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 1 ─────────────────────────────────────────────────────\n\n    [2023-11-15 09:59:28][INFO][trainer.py:516] - step=1200 loss=2.907 dt=400.617 sps=2.496 mtps=0.010 mfu=23.837 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    One supercomputer is a super computer that is capable of the supercomputing of supercomputers and other machines.\n\n    Where is the supercomputer on the circuit?\n\n    The supercomputer is made up of several supercomputers.\n\n    A supercomputer is the most advanced computer that has ever been built.\n\n    It is so powerful that it can do so much work.\n\n    It is the highest powered supercomputer.\n\n    It is very powerful.\n\n    It can do so much.\n\n    It is capable of doing so much.\n\n    It is supercomputer.\n\n    It is supercomputer.\n\n    It is supercomputer.\n\n    It is supercomputer.\n\n    It is supercomputer.\n\n    Supercomputer.\n\n    If you want a break-down of what a supercomputer is, read this.\n\n    You might also want to read this.\n\n    The diagram below shows that the supercomputer is connected to the circuit.\n\n    What does that diagram look like?\n\n    A pretty huge diagram, right? Well, it's a diagram that even the best supercomputer doesn't have. It is a diagram that only the best supercomputer has.\n\n    And that is why it's called a diagram.\n\n    It is called a diagram.\n\n    A diagram is an electrical diagram.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    When you see an electrical diagram it means the diagram on the circuit has an electrical diagram of it's components.\n\n    This diagram doesn't imply that the components on the circuit are connected.\n\n    It just means that the diagram on the circuit has an electrical diagram of it's components.\n\n    This diagram doesn't imply that the components on the circuit are connected.\n\n    It just means that the diagram on the circuit has an electrical diagram of it's components.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    This diagram doesn't imply that the components on the circuit are connected.\n\n    It just means that the diagram on the circuit has an electrical diagram of it\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 2 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:00:22][INFO][trainer.py:516] - step=1300 loss=2.941 dt=423.726 sps=2.360 mtps=0.010 mfu=22.537 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer? Supercomputing machines are extremely powerful and cheap. They are also cheap to upgrade, and they are incredibly easy to install - and they can handle any challenge you throw at them. You can even rent out one for a few hours in a day (after all, you need a supercomputer in the first place).\n\n    Read more about supercomputing in our article on computing history.&lt;|endoftext|&gt;An estimated 130 people were killed and at least 200 wounded by militants in the attack.\n\n    The attack started near the scene of Friday's explosions, which killed two women and a child.\n\n    A bomber had targeted a residential area next to the scene of Friday's blasts.\n\n    The suicide bomber also targeted the building in the village of Bishnagar, which has a military installation.\n\n    There's no estimate of how many people were killed or wounded.\n\n    The blasts took place near the village of Bishnagar, which has a military installation. (Photo/AP)\n\n    One of the explosions took place near the building in the village of Bishnagar, which has a military installation. (Photo/AP)\n\n    The explosions took place near the building in the village of Bishnagar, which has a military installation. (Photo/AP)&lt;|endoftext|&gt;How do you know what's happening on your computer? The days are long and the weeks are long, but once in a while, it's nice to get a peek inside the code behind what you’re doing. The difference between a good and a bad computer is usually the code behind its parts. It’s also a good sign that the developer is smart and is playing a skilled game of communication with a user. But you don’t have to play a game of communication to understand and understand how a code is actually interacting with the real world. If this sounds like a bit of a stretch, consider this: Google Chrome has evolved into a fast-paced browser focused on gaming and entertainment. Apple’s Safari was once full of the same stuff, but now it’s mostly silent.\n\n    That’s a good thing, but it doesn’t explain why Apple’s mobile operating system is so focused on gaming. To the best of my knowledge, there’s not much meaningfully different about what the developer's thinking is as it relates to the development of Safari. Maybe it’s a good thing they�\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 3 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:01:14][INFO][trainer.py:516] - step=1400 loss=2.948 dt=366.042 sps=2.732 mtps=0.011 mfu=26.088 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is an object that is built around a central processor.\n\n    Although its size and weight may not be as strong as a human body, it is still far from the feeble components of a regular computer and is the same power as a small power station.\n\n    The technology is called a supercomputer because it is capable of storing and processing huge amounts of data. If our supercomputer is not powerful enough to perform the calculations of a single human, it is equipped with the ability to run the complex software written in the instructions of the operating system.\n\n    We have managed to get a number of super computers ready for testing, and they live up to their names.\n\n    We are talking about super computers that take 10,000 times the power of the entire computer, and are equipped with a powerful combination of processors and big memory storage.\n\n    They are capable of running the complex software written in the instructions of the operating system.\n\n    You can call an IBM super computer a super computer because it has the power of all of a human being.\n\n    How well does it work?\n\n    IBM super computers are so powerful that they can run our own software. They are incredibly fast, are very long-lasting, they are able to communicate with each other and are connected with our WiFi network.\n\n    The super computer of yours is a super computer that has a massive amount of memory and power, which is very much more powerful than a human average.\n\n    That means it can run our programs, which are made of thousands of Java code.\n\n    It is equipped with the power of a human at 1,500 times their humanweight.\n\n    You can call an IBM super computer a super computer because it has the power of all of a human being.\n\n    So, if you are a human being, and you have a super computer, will you be able to run a program that people might not have seen on a super computer?\n\n    It is the same question as every other question.\n\n    We have made a number of super computers for different purposes.\n\n    We have shown to humans the power of running an ultra-fast super computer, such as a supercomputer of this kind.\n\n    It is possible to run a program written in Java that is 10,000 times the power of the entire computer.\n\n    We know that we have managed to get a number of super computers ready for testing.\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 4 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:02:07][INFO][trainer.py:516] - step=1500 loss=2.871 dt=341.233 sps=2.931 mtps=0.012 mfu=27.985 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is a computer that is able to do complex calculations faster than a human can.\n\n    A supercomputer is a computer that is able to do complex calculations faster than a human can.\n\n    A supercomputer can have more cores than a human.\n\n    A supercomputer is not a memory. It is a super computer. Once you have one, you can use it as an efficient computer.\n\n    A supercomputer can be the fastest computer on the planet.\n\n    (Click through the gallery for a larger view.)\n\n    \"We have high-performance supercomputers that are very fast and very efficient. The problem is that, because of modern computing technology, we can do multiple tasks at the same time,\" said Thomas DeLong, a computer scientist at the University of Michigan and a computer scientist at the University of California, Berkeley, who is one of the first members of the UNDP supercomputing committee.\n\n    \"This is not a supercomputer. It is a supercomputer. It is fast enough to do certain things that other computers cannot. That is the power of super computers.\"\n\n    DeLong's group has been working to upgrade super computers to get a better computing power. Their goal is to make the supercomputer that is needed for the next generation of computing on the planet -- the next generation that is computing the vast majority of the world's information -- the fastest supercomputer on the planet.\n\n    \"We have high-performance supercomputers that are very fast and very efficient. The problem is that, because of modern computing technology, we can do multiple tasks at the same time. That is the power of super computers,\" said Thomas DeLong, a computer scientist at the University of Michigan and a computer scientist at the University of California, Berkeley, who is one of the first members of the UNDP supercomputing committee. \"That is what super computers are all about.\"\n\n    Dewey said that it's so easy to see why super computers could be faster than humans -- as an example.\n\n    \"Super computers are very fast computers. If you run a program that you're fast enough, you can live with it. It's not any longer a supercomputer, but a supercomputer, which is the next evolutionary step,\" said Dewey.\n\n    Dewey said super computers are more powerful than the average human -- especially when there's more to computing.\n\n    \"The supercomputer\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 5 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:03:00][INFO][trainer.py:516] - step=1600 loss=2.997 dt=416.252 sps=2.402 mtps=0.010 mfu=22.941 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is a computer for which a large number of computers are required to run programs. A supercomputer is a computer with an operating system that is modified to reduce the number of computers required to run programs.\n\n    For example, a supercomputer is a computer with a Windows operating system that meets the standard of performance of a modern computer. This is roughly equivalent to a computer of similar size that runs an operating system that runs an operating system that doubles as a computer. This is the very computer that is needed to read and write programs and monitor them, and uses the internet to connect to a computer.\n\n    Some examples of supercomputers are:\n\n    Microsoft\n\n    Google\n\n    Apple\n\n    Apple\n\n    Another example of a computer with an operating system that also includes operating systems and applications that run on the internet is:\n\n    Microsoft\n\n    Google\n\n    Apple\n\n    Google\n\n    Apple\n\n    To communicate with a computer, a computer needs two sources of data:\n\n    data directly from the command line\n\n    data from libraries\n\n    If you have a computer with more than two accounts on the command line, you want to take advantage of the power of the three-way communication paradigm developed by the World Wide Web. If you have any other system you're running (e.g. a mobile device or laptop computer), you should see that the above is a more appropriate example of communication with a computer.\n\n    Don't forget to get some free training on how to set up an email server on your computer, and learn how to use a command line terminal or graphical user interface on your computer. That way you can do all of this without having to install any new software on your computer.\n\n    Or, if you are running a modern computer, you can download and install the free software that is needed for running an operating system on the command line from a single computer.\n\n    When you're running a modern computer, you need to use software that is capable of running a modern operating system. If you are writing a program that uses the command line, you should read the content of the installation instructions to make sure that the correct software is installed. If the installation instructions don't mention the proper path for the software you are writing, you want to be sure you have the correct software installed. If you're using an older computer (i.e. before the end of 2010), you may need to install extra software that is\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 6 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:03:53][INFO][trainer.py:516] - step=1700 loss=3.223 dt=392.194 sps=2.550 mtps=0.010 mfu=24.349 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is computer that can run instructions that are cheaper to run than the typical computer.\n\n    But how does the supercomputer perform?\n\n    In the early days of computers, supercomputers were just very powerful computers, essentially.\n\n    Then, in the early days of computers, supercomputers were just very powerful computers.\n\n    Today, computers performed three services.\n\n    It performed the simple tasks that we all perform.\n\n    It could run and read programs.\n\n    It could output an image.\n\n    It could track specific objects, such as a physical object when it was moved and moved.\n\n    It could create a software program.\n\n    And it could create a program.\n\n    At least that's what the supercomputer did.\n\n    What's one of the most common supercomputers?\n\n    You have a lot of supercomputers.\n\n    That's because supercomputers are just massive computers.\n\n    Different computers perform different tasks.\n\n    What are the most common supercomputers?\n\n    Computers are just computers.\n\n    What are the most inexpensive supercomputers?\n\n    Computers are everywhere.\n\n    What's the cost to build a supercomputer that can do the most complicated tasks?\n\n    The supercomputer is made out of three parts.\n\n    It's made of silicon (the most common material)\n\n    It's made of metal, the second most common\n\n    The last one is probably a little hard to work with that is\n\n    The first one is a lot based on silicon\n\n    Its starting price is around $50, not that expensive\n\n    The second one is the most expensive\n\n    What's the cost to build a supercomputer that can run more complicated software than today's computers?\n\n    Computers are super computers.\n\n    But what about the cost to build a supercomputer that can do more complex software than today's computers?\n\n    The price is a lot\n\n    What's the cost to build a supercomputer that can perform more complex tasks than today's computers?\n\n    The cost is just over $100\n\n    What happens if you build it to work alone?\n\n    When you build it to perform alone, you'll have a supercomputer\n\n    What's the cost of building a supercomputer that can do more complex software than today's computers?\n\n    The price of a supercomputer that can perform more complex software\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 7 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:04:45][INFO][trainer.py:516] - step=1800 loss=3.046 dt=353.265 sps=2.831 mtps=0.012 mfu=27.032 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is a computer that can control and control all of your work. It is essentially a computer that is able to do everything a computer can do. There are a few different names for a supercomputer (or supercomputer). In this article, we will focus on one that is nearly ubiquitous, the supercomputer. We will look at the specific tools and software that can be used to create these machines.\n\n    Defining a supercomputer\n\n    Intelligent and flexible\n\n    At its most basic, a supercomputer is an computer that is able to control and control all of your work, all of which happens in the real world. What you would call a computer is a computer that is able to go through your work and store what you have done. This is what most people think of as a computer, when they think of the word. That is not a computer, however, what you are really talking about is your computer.\n\n    At the end of the day, a supercomputer is a computer that is able to run your work and store what you have done.\n\n    The programming language used in a supercomputer is text based. You may have heard of a program like C programming language. In this article, I will spend a lot of time explaining how to use C programming language. I will also show you how to use Visual Studio 2016 which is just as simple as using C programming language. I will also show you how to develop an MVP of your MVP, which is also what I am talking about in this article.\n\n    Some examples of programs that might use C programming language include:\n\n    Creating a supercomputer with C coding language\n\n    Using C programming language\n\n    Using C-based programming language\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying an MVP of your MVP that is also able to control all of your work\n\n    Modifying a MVP of your MVP that is able to control all of your work\n\n    Modifying a MVP of your MVP that is able to control all\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 8 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:05:37][INFO][trainer.py:516] - step=1900 loss=3.108 dt=413.097 sps=2.421 mtps=0.010 mfu=23.116 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is not just a computer. It is an instrument that allows us to perform an algorithm. A supercomputer can be used by scientists, researchers, and computing experts. A supercomputer can be used by our lives to perform mathematical tasks such as addition and subtraction, while also being useful as a tool for fun.\n\n    What's a supercomputer like?\n\n    There is a very specific reason why super computers are super computers. Super computers are often referred to as chips, or chips with chips.\n\n    What's a supercomputer like?\n\n    Super computers are designed for deep learning. They can be used by any researcher who wants to understand how information works on a large scale. Although super computers often have a single chip, it has a huge number of chips.\n\n    What's a supercomputer like?\n\n    Super computers are designed to be used by the military. Scientists use super computers to detect and track enemy aircraft. A supercomputer can also be used to run the full search engine on a large server.\n\n    What's a supercomputer like?\n\n    Super computers are being used by academics, who are using the technology to develop tools for research. In particular, super computers have been used to check in on the behavior of the human brain.\n\n    What's a supercomputer like?\n\n    Super machines have been used to run a wide range of research, such as the Human Genome Project, where the focus is on understanding what happens in the brain.\n\n    What's a supercomputer like?\n\n    Superputers can be used by medical professionals to run an automated treatment program. A supercomputer could be used as a tool to find bugs in a patient's brain.\n\n    What's a supercomputer like?\n\n    Super computers are being used by astronomers to study the stars. A supercomputer can also be used as a tool to find bugs in a patient's brain.\n\n    What's a supercomputer like?\n\n    As the name of the video game, Supercomputer and Supercomputer. Supercomputer in the name of Super Computer and Supercomputer in the name of Supercomputer and Supercomputer.\n\n    What's a supercomputer like?\n\n    Superputers can be used by computer scientists to analyze the structure of stars. A supercomputer can also be used as a tool to find bugs in a patient's brain.\n\n    What's a supercomputer like?\n\n    Superputers can be used by computer\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 9 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:06:30][INFO][trainer.py:516] - step=2000 loss=3.133 dt=378.236 sps=2.644 mtps=0.011 mfu=25.247 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    Supercomputing is the concept of processing and storing information such as programs and images into a computer.\n\n    What kind of information can supercomputing access?\n\n    When used in supercomputing, it can access the entire computer.\n\n    What is the most powerful reason to allow supercomputing to access data?\n\n    Supercomputing can access all the data that a computer can read.\n\n    What are the most challenging aspects of supercomputing?\n\n    Most supercomputing tasks require only a basic understanding of CPU instructions.\n\n    What are the advantages of using supercomputing?\n\n    It’s a fully accessible method, which makes it accessible and easy to use.&lt;|endoftext|&gt;The Denver Broncos had a four-year contract with former Denver Broncos offensive coordinator Bill O’Brien out. The Broncos never hired Broncos offensive coordinator Bill O’Brien as Broncos offensive coordinator. O’Brien brought in Broncos offensive coordinator Mike Klis to be the Broncos offensive coordinator. Klis was a Broncos offensive coordinator for two seasons.\n\n    The Broncos hired Broncos offensive coordinator Mike Klis to be the Broncos offensive coordinator. Denver fired Bill O’Brien as Broncos offensive coordinator.\n\n     Denver had a four-year contract with Bill O’Brien out. The Broncos never hired Bill O’Brien as Broncos offensive coordinator. O’Brien brought in Broncos offensive coordinator Mike Klis to be the Broncos offensive coordinator.\n\n    In 2009, the Broncos hired Mike Klis, who was the Broncos offensive coordinator for two seasons.\n\n     Broncos coach Bill O’Brien, left, and Mike Klis, right, in the Broncos offense. In 2009, Broncos coach Bill O’Brien, left, and Mike Klis, right, in the Broncos offense.\n\n    It’s a matter of fact that Bill O’Brien brought in Peyton Manning to be the Broncos offensive coordinator. Broncos offensive coordinator Mike Klis , left, and Mike Klis, right, in the Broncos offense.\n\n    DENVER Broncos coach Bill O’Brien, left, and Mike Klis, right, were the Broncos offensive coordinators. Broncos coach Bill O’Brien, left, and Mike Klis, right.\n\n    Denver coaches Mike Klis and Mike Klis and Broncos offensive coordinator Mike Klis.\n\n    Denver coaches Mike Klis and Mike Klis, left. Denver coaches Mike Klis and Mike Klis\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "quarto/gpt2-medium.html#install-setup",
    "href": "quarto/gpt2-medium.html#install-setup",
    "title": "nanoGPT",
    "section": "",
    "text": "We need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n\n    /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\n    Has ngpt installed. Nothing to do."
  },
  {
    "objectID": "quarto/gpt2-medium.html#post-install",
    "href": "quarto/gpt2-medium.html#post-install",
    "title": "nanoGPT",
    "section": "",
    "text": "If installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\n\n    The autoreload extension is already loaded. To reload it, use:\n      %reload_ext autoreload\n\n    /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py"
  },
  {
    "objectID": "quarto/gpt2-medium.html#build-trainer",
    "href": "quarto/gpt2-medium.html#build-trainer",
    "title": "nanoGPT",
    "section": "",
    "text": "Explicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\n\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5127'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nSEED = np.random.randint(2**32)\nconsole.log(f'SEED: {SEED}')\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=owt',\n        'model=gpt2_medium',\n        'optimizer=gpt2_medium',\n        'train=gpt2_medium',\n        'train.dtype=bfloat16',\n        'train.max_iters=1000',\n        'train.log_interval=100',\n        'train.init_from=gpt2-medium',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n\n    --------------------------------------------------------------------------\n    WARNING: There was an error initializing an OpenFabrics device.\n\n      Local host:   thetagpu23\n      Local device: mlx5_0\n    --------------------------------------------------------------------------\n    2023-11-15 09:48:20.191135: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n    [09:48:22] SEED: 627182480\n    [2023-11-15 09:48:23][WARNING][configs.py:297] - No meta.pkl found, assuming GPT-2 encodings...\n    [2023-11-15 09:48:26][INFO][configs.py:263] - Rescaling GAS -&gt; GAS // WORLD_SIZE = 1 // 1\n    [2023-11-15 09:48:26][INFO][configs.py:398] - Tokens per iteration: 4,096\n    [2023-11-15 09:48:26][INFO][configs.py:430] - Using &lt;torch.amp.autocast_mode.autocast object at 0x7fcbf3a11930&gt;\n    [2023-11-15 09:48:26][INFO][trainer.py:187] - Initializing from OpenAI GPT-2 Weights: gpt2-medium\n    [2023-11-15 09:48:32,281] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n    [2023-11-15 09:48:49][INFO][model.py:225] - loading weights from pretrained gpt: gpt2-medium\n    [2023-11-15 09:48:49][INFO][model.py:234] - forcing vocab_size=50257, block_size=1024, bias=True\n    [2023-11-15 09:48:49][INFO][model.py:240] - overriding dropout rate to 0.0\n    [2023-11-15 09:48:55][INFO][model.py:160] - number of parameters: 353.77M\n    Downloading (…)lve/main/config.json:   0%|| 0.00/718 [00:00[2023-11-15 09:49:16][INFO][model.py:290] - num decayed parameter tensors: 98, with 354,501,632 parameters\n    [2023-11-15 09:49:16][INFO][model.py:291] - num non-decayed parameter tensors: 194, with 321,536 parameters\n    [2023-11-15 09:49:17][INFO][model.py:297] - using fused AdamW: True"
  },
  {
    "objectID": "quarto/gpt2-medium.html#prompt-prior-to-training",
    "href": "quarto/gpt2-medium.html#prompt-prior-to-training",
    "title": "nanoGPT",
    "section": "",
    "text": "query = \"What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic..\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n    [prompt]: \"What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic..\"\n\n    [response]:\n\n    What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic.. When I ask you to describe your computer, this is what you know: a computer that runs a quantum computer to run a computer program. There are now two supercomputers that can do quantum computations, so how can you explain quantum computing? We will get into what quantum computing is first, and then we are going to talk to you about what is a supercomputer. You will learn how to explain quantum computing to a child, and you will find out that quantum computing is what is called artificial intelligence (AI). Do you understand quantum computing?\n    Follow up – Answering the Quantum Computing Questions\n    Cognitive scientists are trying to define what is called artificial intelligence (AI). They are trying to understand what computers can do. At the heart of the argument are the following three questions:\n    What is AI and how does it apply to science?\n    What if I say, \"A computer can tell me if I am an elf, a lion, a fox, or a squirrel?\". Can I make a decision?\n    How does AI fit into the human condition?\n    Have you ever thought about what is intelligence? Have you ever wondered, \"What if I want to create a computer that can build me an AI, and that computer is smarter than me? How will the AI affect my life?\".\n    Before you even begin…\n    BECAUSE YOU HAVE NOT BEEN PURCHASED\n    This is where you may want to get an answer to a question that you have never had the opportunity to ask, and which is too big to ask right now. Think about it. Will I be able to make decisions? Will I be able to ask questions? Do I have the right to be an AI?\n    With an intelligent AI, you can have the ability to run my brain. You can have your mind, and you can control it. You can control your body, you can control your thoughts, and you can control your actions. That will make you a living being. If you are able to experience the world like the rest of us, you will have a better understanding of what it means to be alive; a living being. You will have the ability to observe, dream, and experience. All this will be possible to you if you are created as an intelligent being.\n    By creating an AI, this will be easier. It will be easy for someone to create an AI, and then run it as if\n\n\n\n\n\nTable 1: Legend\n\n\nName\nDescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization1"
  },
  {
    "objectID": "quarto/gpt2-medium.html#train-model",
    "href": "quarto/gpt2-medium.html#train-model",
    "title": "nanoGPT",
    "section": "",
    "text": "trainer.model.module.train()\ntrainer.train()\n\n    [2023-11-15 09:50:50][INFO][trainer.py:516] - step=100 loss=2.791 dt=387.530 sps=2.580 mtps=0.011 mfu=24.642 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:51:28][INFO][trainer.py:516] - step=200 loss=2.716 dt=375.216 sps=2.665 mtps=0.011 mfu=24.722 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:52:07][INFO][trainer.py:516] - step=300 loss=2.701 dt=398.145 sps=2.512 mtps=0.010 mfu=24.649 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:52:45][INFO][trainer.py:516] - step=400 loss=2.858 dt=376.159 sps=2.658 mtps=0.011 mfu=24.722 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:53:24][INFO][trainer.py:516] - step=500 loss=2.542 dt=422.272 sps=2.368 mtps=0.010 mfu=24.512 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:54:03][INFO][trainer.py:516] - step=600 loss=2.912 dt=406.393 sps=2.461 mtps=0.010 mfu=24.410 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:54:42][INFO][trainer.py:516] - step=700 loss=2.862 dt=369.661 sps=2.705 mtps=0.011 mfu=24.552 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:55:21][INFO][trainer.py:516] - step=800 loss=2.849 dt=336.193 sps=2.974 mtps=0.012 mfu=24.938 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:56:00][INFO][trainer.py:516] - step=900 loss=2.586 dt=387.251 sps=2.582 mtps=0.011 mfu=24.910 train_loss=2.837 val_loss=2.826\n    [2023-11-15 09:56:38][INFO][trainer.py:516] - step=1000 loss=2.763 dt=373.859 sps=2.675 mtps=0.011 mfu=24.973 train_loss=2.837 val_loss=2.826"
  },
  {
    "objectID": "quarto/gpt2-medium.html#evaluate-model",
    "href": "quarto/gpt2-medium.html#evaluate-model",
    "title": "nanoGPT",
    "section": "",
    "text": "query = \"What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic..\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n    [prompt]: \"What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic..\"\n\n    [response]:\n\n    What is a supercomputer? Explain like I'm a child, and speak clearly. Double check your logic.. Does it say that when you say \"I want to be famous\" that you should be famous as a songwriter too? Does it say that you should have a place in your life or in an organization? Is it saying that you should be regarded as a \"supercomputer\" and have access to the best software on earth?\n\n    Is it saying that you are a computer programmer?\n\n    Does it say \"you are a computer scientist\"?\n\n    Does it say \"you are a computer scientist\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer?\"\n\n     does it say \"you are a computer programmer?\"\n\n    does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer\"?\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer\"?\" Doesn it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer?\"\n\n    Does it say \"you are a computer programmer?\"\n\n    Does"
  },
  {
    "objectID": "quarto/gpt2-medium.html#train-a-bit-more",
    "href": "quarto/gpt2-medium.html#train-a-bit-more",
    "title": "nanoGPT",
    "section": "",
    "text": "trainer.model.module.train()\nfor iter in range(10):\n    console.rule(f'iter: {iter}')\n    trainer.train(train_iters=100)\n    query = \"What is a supercomputer?\"\n    outputs = trainer.evaluate(query, num_samples=1, display=False)\n    console.print(fr'\\[prompt]: \"{query}\"')\n    console.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n    console.rule()\n\n    ───────────────────────────────────────────────────── iter: 0 ─────────────────────────────────────────────────────\n\n    [2023-11-15 09:58:35][INFO][trainer.py:516] - step=1100 loss=2.939 dt=356.182 sps=2.808 mtps=0.011 mfu=26.810 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer consists of two parts: “a processor and a network of interconnected graphics processors, called supercomputers,” said Richard Andrews, a computer scientist at the University of Edinburgh.\n\n    Image copyright PA Image caption The first three are the biggest computers in the world, but they’re all fitted with a touchscreen\n\n    The processor is a small computer, usually a single-core one. Usually it is made of silicon and it’s the first, and most expensive, part of the complex system. The other two parts of the system are: a chip to power the processor and a memory and software stack to allow for the system to interact on the internet.\n\n    The first is the processor. It runs a complex combination of algorithms. These algorithms work to compress higher-level data, but also do some processing to understand the lower levels of the system.\n\n    According to the Oxford computer science textbook, algorithms are:\n\n    It’s a special structure which allows a system to achieve a particular level of efficiency, such as a faster processor or a faster system of atoms or molecules.\n\n    Researchers use algorithms to implement certain types of systems like computers or medicine. But a supercomputer can also work faster, or faster, to solve problems.\n\n    Image copyright PA Image caption The supercomputer in the US can run things like social networks and video games\n\n    A supercomputer has a processor inside, but there is also a memory and software to process it. This allows a system to do something faster. This is called a supercomputer’s memory.\n\n    Image copyright Getty Images Image caption Computing power has increased, but it’s also improved the power of computers in recent years\n\n    The second piece of the system is the computer network. This is a computer system that works together to perform calculations.\n\n    This is a huge system. It is where all the computers in the world - computers in different countries, continents and different time zones - work together.\n\n    The network works and converts the data on the computers into calculations. This is the core of a supercomputer.\n\n    What goes on inside a supercomputer?\n\n    Image copyright PA Image caption The supercomputer works together to work out human intelligence\n\n    It can interact with the computer network via a touchscreen.\n\n    The other pieces of the system are:\n\n    A computer system can send and receive data through a touchscreen. It is\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 1 ─────────────────────────────────────────────────────\n\n    [2023-11-15 09:59:28][INFO][trainer.py:516] - step=1200 loss=2.907 dt=400.617 sps=2.496 mtps=0.010 mfu=23.837 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    One supercomputer is a super computer that is capable of the supercomputing of supercomputers and other machines.\n\n    Where is the supercomputer on the circuit?\n\n    The supercomputer is made up of several supercomputers.\n\n    A supercomputer is the most advanced computer that has ever been built.\n\n    It is so powerful that it can do so much work.\n\n    It is the highest powered supercomputer.\n\n    It is very powerful.\n\n    It can do so much.\n\n    It is capable of doing so much.\n\n    It is supercomputer.\n\n    It is supercomputer.\n\n    It is supercomputer.\n\n    It is supercomputer.\n\n    It is supercomputer.\n\n    Supercomputer.\n\n    If you want a break-down of what a supercomputer is, read this.\n\n    You might also want to read this.\n\n    The diagram below shows that the supercomputer is connected to the circuit.\n\n    What does that diagram look like?\n\n    A pretty huge diagram, right? Well, it's a diagram that even the best supercomputer doesn't have. It is a diagram that only the best supercomputer has.\n\n    And that is why it's called a diagram.\n\n    It is called a diagram.\n\n    A diagram is an electrical diagram.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    When you see an electrical diagram it means the diagram on the circuit has an electrical diagram of it's components.\n\n    This diagram doesn't imply that the components on the circuit are connected.\n\n    It just means that the diagram on the circuit has an electrical diagram of it's components.\n\n    This diagram doesn't imply that the components on the circuit are connected.\n\n    It just means that the diagram on the circuit has an electrical diagram of it's components.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    The diagram on the circuit has an electrical diagram of it's components.\n\n    This diagram doesn't imply that the components on the circuit are connected.\n\n    It just means that the diagram on the circuit has an electrical diagram of it\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 2 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:00:22][INFO][trainer.py:516] - step=1300 loss=2.941 dt=423.726 sps=2.360 mtps=0.010 mfu=22.537 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer? Supercomputing machines are extremely powerful and cheap. They are also cheap to upgrade, and they are incredibly easy to install - and they can handle any challenge you throw at them. You can even rent out one for a few hours in a day (after all, you need a supercomputer in the first place).\n\n    Read more about supercomputing in our article on computing history.&lt;|endoftext|&gt;An estimated 130 people were killed and at least 200 wounded by militants in the attack.\n\n    The attack started near the scene of Friday's explosions, which killed two women and a child.\n\n    A bomber had targeted a residential area next to the scene of Friday's blasts.\n\n    The suicide bomber also targeted the building in the village of Bishnagar, which has a military installation.\n\n    There's no estimate of how many people were killed or wounded.\n\n    The blasts took place near the village of Bishnagar, which has a military installation. (Photo/AP)\n\n    One of the explosions took place near the building in the village of Bishnagar, which has a military installation. (Photo/AP)\n\n    The explosions took place near the building in the village of Bishnagar, which has a military installation. (Photo/AP)&lt;|endoftext|&gt;How do you know what's happening on your computer? The days are long and the weeks are long, but once in a while, it's nice to get a peek inside the code behind what you’re doing. The difference between a good and a bad computer is usually the code behind its parts. It’s also a good sign that the developer is smart and is playing a skilled game of communication with a user. But you don’t have to play a game of communication to understand and understand how a code is actually interacting with the real world. If this sounds like a bit of a stretch, consider this: Google Chrome has evolved into a fast-paced browser focused on gaming and entertainment. Apple’s Safari was once full of the same stuff, but now it’s mostly silent.\n\n    That’s a good thing, but it doesn’t explain why Apple’s mobile operating system is so focused on gaming. To the best of my knowledge, there’s not much meaningfully different about what the developer's thinking is as it relates to the development of Safari. Maybe it’s a good thing they�\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 3 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:01:14][INFO][trainer.py:516] - step=1400 loss=2.948 dt=366.042 sps=2.732 mtps=0.011 mfu=26.088 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is an object that is built around a central processor.\n\n    Although its size and weight may not be as strong as a human body, it is still far from the feeble components of a regular computer and is the same power as a small power station.\n\n    The technology is called a supercomputer because it is capable of storing and processing huge amounts of data. If our supercomputer is not powerful enough to perform the calculations of a single human, it is equipped with the ability to run the complex software written in the instructions of the operating system.\n\n    We have managed to get a number of super computers ready for testing, and they live up to their names.\n\n    We are talking about super computers that take 10,000 times the power of the entire computer, and are equipped with a powerful combination of processors and big memory storage.\n\n    They are capable of running the complex software written in the instructions of the operating system.\n\n    You can call an IBM super computer a super computer because it has the power of all of a human being.\n\n    How well does it work?\n\n    IBM super computers are so powerful that they can run our own software. They are incredibly fast, are very long-lasting, they are able to communicate with each other and are connected with our WiFi network.\n\n    The super computer of yours is a super computer that has a massive amount of memory and power, which is very much more powerful than a human average.\n\n    That means it can run our programs, which are made of thousands of Java code.\n\n    It is equipped with the power of a human at 1,500 times their humanweight.\n\n    You can call an IBM super computer a super computer because it has the power of all of a human being.\n\n    So, if you are a human being, and you have a super computer, will you be able to run a program that people might not have seen on a super computer?\n\n    It is the same question as every other question.\n\n    We have made a number of super computers for different purposes.\n\n    We have shown to humans the power of running an ultra-fast super computer, such as a supercomputer of this kind.\n\n    It is possible to run a program written in Java that is 10,000 times the power of the entire computer.\n\n    We know that we have managed to get a number of super computers ready for testing.\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 4 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:02:07][INFO][trainer.py:516] - step=1500 loss=2.871 dt=341.233 sps=2.931 mtps=0.012 mfu=27.985 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is a computer that is able to do complex calculations faster than a human can.\n\n    A supercomputer is a computer that is able to do complex calculations faster than a human can.\n\n    A supercomputer can have more cores than a human.\n\n    A supercomputer is not a memory. It is a super computer. Once you have one, you can use it as an efficient computer.\n\n    A supercomputer can be the fastest computer on the planet.\n\n    (Click through the gallery for a larger view.)\n\n    \"We have high-performance supercomputers that are very fast and very efficient. The problem is that, because of modern computing technology, we can do multiple tasks at the same time,\" said Thomas DeLong, a computer scientist at the University of Michigan and a computer scientist at the University of California, Berkeley, who is one of the first members of the UNDP supercomputing committee.\n\n    \"This is not a supercomputer. It is a supercomputer. It is fast enough to do certain things that other computers cannot. That is the power of super computers.\"\n\n    DeLong's group has been working to upgrade super computers to get a better computing power. Their goal is to make the supercomputer that is needed for the next generation of computing on the planet -- the next generation that is computing the vast majority of the world's information -- the fastest supercomputer on the planet.\n\n    \"We have high-performance supercomputers that are very fast and very efficient. The problem is that, because of modern computing technology, we can do multiple tasks at the same time. That is the power of super computers,\" said Thomas DeLong, a computer scientist at the University of Michigan and a computer scientist at the University of California, Berkeley, who is one of the first members of the UNDP supercomputing committee. \"That is what super computers are all about.\"\n\n    Dewey said that it's so easy to see why super computers could be faster than humans -- as an example.\n\n    \"Super computers are very fast computers. If you run a program that you're fast enough, you can live with it. It's not any longer a supercomputer, but a supercomputer, which is the next evolutionary step,\" said Dewey.\n\n    Dewey said super computers are more powerful than the average human -- especially when there's more to computing.\n\n    \"The supercomputer\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 5 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:03:00][INFO][trainer.py:516] - step=1600 loss=2.997 dt=416.252 sps=2.402 mtps=0.010 mfu=22.941 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is a computer for which a large number of computers are required to run programs. A supercomputer is a computer with an operating system that is modified to reduce the number of computers required to run programs.\n\n    For example, a supercomputer is a computer with a Windows operating system that meets the standard of performance of a modern computer. This is roughly equivalent to a computer of similar size that runs an operating system that runs an operating system that doubles as a computer. This is the very computer that is needed to read and write programs and monitor them, and uses the internet to connect to a computer.\n\n    Some examples of supercomputers are:\n\n    Microsoft\n\n    Google\n\n    Apple\n\n    Apple\n\n    Another example of a computer with an operating system that also includes operating systems and applications that run on the internet is:\n\n    Microsoft\n\n    Google\n\n    Apple\n\n    Google\n\n    Apple\n\n    To communicate with a computer, a computer needs two sources of data:\n\n    data directly from the command line\n\n    data from libraries\n\n    If you have a computer with more than two accounts on the command line, you want to take advantage of the power of the three-way communication paradigm developed by the World Wide Web. If you have any other system you're running (e.g. a mobile device or laptop computer), you should see that the above is a more appropriate example of communication with a computer.\n\n    Don't forget to get some free training on how to set up an email server on your computer, and learn how to use a command line terminal or graphical user interface on your computer. That way you can do all of this without having to install any new software on your computer.\n\n    Or, if you are running a modern computer, you can download and install the free software that is needed for running an operating system on the command line from a single computer.\n\n    When you're running a modern computer, you need to use software that is capable of running a modern operating system. If you are writing a program that uses the command line, you should read the content of the installation instructions to make sure that the correct software is installed. If the installation instructions don't mention the proper path for the software you are writing, you want to be sure you have the correct software installed. If you're using an older computer (i.e. before the end of 2010), you may need to install extra software that is\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 6 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:03:53][INFO][trainer.py:516] - step=1700 loss=3.223 dt=392.194 sps=2.550 mtps=0.010 mfu=24.349 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is computer that can run instructions that are cheaper to run than the typical computer.\n\n    But how does the supercomputer perform?\n\n    In the early days of computers, supercomputers were just very powerful computers, essentially.\n\n    Then, in the early days of computers, supercomputers were just very powerful computers.\n\n    Today, computers performed three services.\n\n    It performed the simple tasks that we all perform.\n\n    It could run and read programs.\n\n    It could output an image.\n\n    It could track specific objects, such as a physical object when it was moved and moved.\n\n    It could create a software program.\n\n    And it could create a program.\n\n    At least that's what the supercomputer did.\n\n    What's one of the most common supercomputers?\n\n    You have a lot of supercomputers.\n\n    That's because supercomputers are just massive computers.\n\n    Different computers perform different tasks.\n\n    What are the most common supercomputers?\n\n    Computers are just computers.\n\n    What are the most inexpensive supercomputers?\n\n    Computers are everywhere.\n\n    What's the cost to build a supercomputer that can do the most complicated tasks?\n\n    The supercomputer is made out of three parts.\n\n    It's made of silicon (the most common material)\n\n    It's made of metal, the second most common\n\n    The last one is probably a little hard to work with that is\n\n    The first one is a lot based on silicon\n\n    Its starting price is around $50, not that expensive\n\n    The second one is the most expensive\n\n    What's the cost to build a supercomputer that can run more complicated software than today's computers?\n\n    Computers are super computers.\n\n    But what about the cost to build a supercomputer that can do more complex software than today's computers?\n\n    The price is a lot\n\n    What's the cost to build a supercomputer that can perform more complex tasks than today's computers?\n\n    The cost is just over $100\n\n    What happens if you build it to work alone?\n\n    When you build it to perform alone, you'll have a supercomputer\n\n    What's the cost of building a supercomputer that can do more complex software than today's computers?\n\n    The price of a supercomputer that can perform more complex software\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 7 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:04:45][INFO][trainer.py:516] - step=1800 loss=3.046 dt=353.265 sps=2.831 mtps=0.012 mfu=27.032 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is a computer that can control and control all of your work. It is essentially a computer that is able to do everything a computer can do. There are a few different names for a supercomputer (or supercomputer). In this article, we will focus on one that is nearly ubiquitous, the supercomputer. We will look at the specific tools and software that can be used to create these machines.\n\n    Defining a supercomputer\n\n    Intelligent and flexible\n\n    At its most basic, a supercomputer is an computer that is able to control and control all of your work, all of which happens in the real world. What you would call a computer is a computer that is able to go through your work and store what you have done. This is what most people think of as a computer, when they think of the word. That is not a computer, however, what you are really talking about is your computer.\n\n    At the end of the day, a supercomputer is a computer that is able to run your work and store what you have done.\n\n    The programming language used in a supercomputer is text based. You may have heard of a program like C programming language. In this article, I will spend a lot of time explaining how to use C programming language. I will also show you how to use Visual Studio 2016 which is just as simple as using C programming language. I will also show you how to develop an MVP of your MVP, which is also what I am talking about in this article.\n\n    Some examples of programs that might use C programming language include:\n\n    Creating a supercomputer with C coding language\n\n    Using C programming language\n\n    Using C-based programming language\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying a supercomputer that is able to control all of your work\n\n    Modifying an MVP of your MVP that is also able to control all of your work\n\n    Modifying a MVP of your MVP that is able to control all of your work\n\n    Modifying a MVP of your MVP that is able to control all\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 8 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:05:37][INFO][trainer.py:516] - step=1900 loss=3.108 dt=413.097 sps=2.421 mtps=0.010 mfu=23.116 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    A supercomputer is not just a computer. It is an instrument that allows us to perform an algorithm. A supercomputer can be used by scientists, researchers, and computing experts. A supercomputer can be used by our lives to perform mathematical tasks such as addition and subtraction, while also being useful as a tool for fun.\n\n    What's a supercomputer like?\n\n    There is a very specific reason why super computers are super computers. Super computers are often referred to as chips, or chips with chips.\n\n    What's a supercomputer like?\n\n    Super computers are designed for deep learning. They can be used by any researcher who wants to understand how information works on a large scale. Although super computers often have a single chip, it has a huge number of chips.\n\n    What's a supercomputer like?\n\n    Super computers are designed to be used by the military. Scientists use super computers to detect and track enemy aircraft. A supercomputer can also be used to run the full search engine on a large server.\n\n    What's a supercomputer like?\n\n    Super computers are being used by academics, who are using the technology to develop tools for research. In particular, super computers have been used to check in on the behavior of the human brain.\n\n    What's a supercomputer like?\n\n    Super machines have been used to run a wide range of research, such as the Human Genome Project, where the focus is on understanding what happens in the brain.\n\n    What's a supercomputer like?\n\n    Superputers can be used by medical professionals to run an automated treatment program. A supercomputer could be used as a tool to find bugs in a patient's brain.\n\n    What's a supercomputer like?\n\n    Super computers are being used by astronomers to study the stars. A supercomputer can also be used as a tool to find bugs in a patient's brain.\n\n    What's a supercomputer like?\n\n    As the name of the video game, Supercomputer and Supercomputer. Supercomputer in the name of Super Computer and Supercomputer in the name of Supercomputer and Supercomputer.\n\n    What's a supercomputer like?\n\n    Superputers can be used by computer scientists to analyze the structure of stars. A supercomputer can also be used as a tool to find bugs in a patient's brain.\n\n    What's a supercomputer like?\n\n    Superputers can be used by computer\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n    ───────────────────────────────────────────────────── iter: 9 ─────────────────────────────────────────────────────\n\n    [2023-11-15 10:06:30][INFO][trainer.py:516] - step=2000 loss=3.133 dt=378.236 sps=2.644 mtps=0.011 mfu=25.247 train_loss=0.000 val_loss=0.000\n\n    [prompt]: \"What is a supercomputer?\"\n\n    [response]:\n\n    What is a supercomputer?\n\n    Supercomputing is the concept of processing and storing information such as programs and images into a computer.\n\n    What kind of information can supercomputing access?\n\n    When used in supercomputing, it can access the entire computer.\n\n    What is the most powerful reason to allow supercomputing to access data?\n\n    Supercomputing can access all the data that a computer can read.\n\n    What are the most challenging aspects of supercomputing?\n\n    Most supercomputing tasks require only a basic understanding of CPU instructions.\n\n    What are the advantages of using supercomputing?\n\n    It’s a fully accessible method, which makes it accessible and easy to use.&lt;|endoftext|&gt;The Denver Broncos had a four-year contract with former Denver Broncos offensive coordinator Bill O’Brien out. The Broncos never hired Broncos offensive coordinator Bill O’Brien as Broncos offensive coordinator. O’Brien brought in Broncos offensive coordinator Mike Klis to be the Broncos offensive coordinator. Klis was a Broncos offensive coordinator for two seasons.\n\n    The Broncos hired Broncos offensive coordinator Mike Klis to be the Broncos offensive coordinator. Denver fired Bill O’Brien as Broncos offensive coordinator.\n\n     Denver had a four-year contract with Bill O’Brien out. The Broncos never hired Bill O’Brien as Broncos offensive coordinator. O’Brien brought in Broncos offensive coordinator Mike Klis to be the Broncos offensive coordinator.\n\n    In 2009, the Broncos hired Mike Klis, who was the Broncos offensive coordinator for two seasons.\n\n     Broncos coach Bill O’Brien, left, and Mike Klis, right, in the Broncos offense. In 2009, Broncos coach Bill O’Brien, left, and Mike Klis, right, in the Broncos offense.\n\n    It’s a matter of fact that Bill O’Brien brought in Peyton Manning to be the Broncos offensive coordinator. Broncos offensive coordinator Mike Klis , left, and Mike Klis, right, in the Broncos offense.\n\n    DENVER Broncos coach Bill O’Brien, left, and Mike Klis, right, were the Broncos offensive coordinators. Broncos coach Bill O’Brien, left, and Mike Klis, right.\n\n    Denver coaches Mike Klis and Mike Klis and Broncos offensive coordinator Mike Klis.\n\n    Denver coaches Mike Klis and Mike Klis, left. Denver coaches Mike Klis and Mike Klis\n\n    ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "quarto/ngpt-shakespeare-1.html",
    "href": "quarto/ngpt-shakespeare-1.html",
    "title": "nanoGPT",
    "section": "",
    "text": "nanoGPT\nInstall / Setup\nFirst Time Running\nPost Install\nBuild Trainer\nPrompt (prior to training)\nTrain Model\nEvaluate Model\nInstall / Setup\nFirst Time Running\nWe need to install ngpt and setup the Shakespeare dataset\nThis will need to be ran the first time you are running this notebook.\nFollowing the\n!python3 -m pip install nanoGPT\nyou will need to restart your runtime (Runtime -&gt; Restart runtime)\nAfter this, you should be able to\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/content/nanoGPT/src/ngpt/__init__.py'\n%%bash\n\npython3 -c 'import ngpt; print(ngpt.__file__)' 2&gt; '/dev/null'\n\nif [[ $? -eq 0 ]]; then\n    echo \"Has ngpt installed. Nothing to do.\"\nelse\n    echo \"Does not have ngpt installed. Installing...\"\n    git clone 'https://github.com/saforem2/nanoGPT'\n    python3 nanoGPT/data/shakespeare_char/prepare.py\n    python3 -m pip install -e nanoGPT -vvv\nfi\n\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nHas ngpt installed. Nothing to do.\nPost Install\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'\n%load_ext autoreload\n%autoreload 2\n\nimport ngpt\nfrom rich import print\nprint(ngpt.__file__)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\nBuild Trainer\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)\nimport os\nimport numpy as np\nfrom ezpz import setup_torch\nfrom hydra.utils import instantiate\nfrom ngpt.configs import get_config, PROJECT_ROOT\nfrom ngpt.trainer import Trainer\nfrom enrich.console import get_console\n\nconsole = get_console()\nHF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\nHF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n\nos.environ['MASTER_PORT'] = '5432'\nos.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n\nrank = setup_torch('DDP', seed=1234)\ncfg = get_config(\n    [\n        'data=shakespeare',\n        'model=shakespeare',\n        'optimizer=shakespeare',\n        'train=shakespeare',\n        'train.dtype=bfloat16',\n        'train.max_iters=5000',\n        'train.log_interval=250',\n        'train.eval_interval=1000',\n    ]\n)\nconfig = instantiate(cfg)\ntrainer = Trainer(config)\n\n--------------------------------------------------------------------------\nWARNING: There was an error initializing an OpenFabrics device.\n\n  Local host:   thetagpu23\n  Local device: mlx5_0\n--------------------------------------------------------------------------\n2023-11-15 09:33:41.578337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n[2023-11-15 09:33:44][INFO][configs.py:263] - Rescaling GAS -&gt; GAS // WORLD_SIZE = 1 // 1\n[2023-11-15 09:33:44][INFO][configs.py:398] - Tokens per iteration: 16,384\n[2023-11-15 09:33:44][INFO][configs.py:430] - Using &lt;torch.amp.autocast_mode.autocast object at 0x7f588e33ddb0&gt;\n[2023-11-15 09:33:44][INFO][configs.py:436] - Initializing a new model from scratch\n[2023-11-15 09:33:44][INFO][trainer.py:179] - Initializing a new model from scratch\n[2023-11-15 09:33:44][INFO][model.py:160] - number of parameters: 10.65M\n[2023-11-15 09:33:45][INFO][model.py:290] - num decayed parameter tensors: 26, with 10,740,096 parameters\n[2023-11-15 09:33:45][INFO][model.py:291] - num non-decayed parameter tensors: 13, with 4,992 parameters\n[2023-11-15 09:33:46][INFO][model.py:297] - using fused AdamW: True\n\n\n[2023-11-11 01:15:48][INFO][trainer.py:179] - Initializing a new model from scratch\n\n\n[2023-11-11 01:15:48][INFO][model.py:160] - number of parameters: 10.65M\n\n\n[2023-11-11 01:15:50][INFO][model.py:290] - num decayed parameter tensors: 26, with 10,740,096 parameters\n\n\n[2023-11-11 01:15:50][INFO][model.py:291] - num non-decayed parameter tensors: 13, with 4,992 parameters\n\n\n[2023-11-11 01:15:50][INFO][model.py:297] - using fused AdamW: True\nPrompt (prior to training)\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[prompt]: \"What is a supercomputer?\"\n\n\n[response]:\n\nWhat is a supercomputer?CbqA-RN?bnss--iadmsD\nS?qJKEwssDq YMSSSFGPxxnJLDC:cvYfOy\nfiXXe3GQQvYQARdEEbbHHPnWyFp-CwBFrr;g\nWATVAcTZCWWr\ntYCCz,E-wqNbIsMbSvYVONyaQzzcs;Iaa?WOACrnMH'':dXFEQZa-PYkAvV.B.  F$J-nKnEaZ,'vpesXY&y-M.nIcMVV!GYYVVFh-UX.G&Fa?LSPrkXd3eKV?KJjSZOwSbbhwfIYaywrvRUEuuQMnnIAZS-Ja.fXrMAHB&!!eVbUFwMIkkalHbmRhwwfcj$:s\nRlVhRcaVbYcTTihITDUbbTNMHEdnOibdB-ebuiJLLS:yarlFYHHkSWxB!hbN?nVm3-&djw'BA uS,EQJP3bbWe$hs-g\n:3jEEYU\nNkLCetHH lc-IIZEBbb-at\njyNYmvffVVnERN?LnTM:yS\nsH;are$WRip!jbX'\ne\n\npyA-jbwK 'B$O& Fvvac&sEjbIretcX-H\nTrain Model\nLegend:\n\n\n\n\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization[^1](in%20units%20of%20A100%20%60bfloat16%60%20peak%20FLOPS)\ntrainer.train()\n\n{\"model_id\":\"e9728e68f7414dcf91f5c4214df8ad72\",\"version_major\":2,\"version_minor\":0}\n\n\n[2023-11-15 09:34:03][INFO][trainer.py:516] - step=250 loss=2.064 dt=27.412 sps=36.481 mtps=0.598 mfu=13.594 train_loss=4.299 val_loss=4.291\n[2023-11-15 09:34:10][INFO][trainer.py:516] - step=500 loss=1.610 dt=26.915 sps=37.153 mtps=0.609 mfu=13.619 train_loss=4.299 val_loss=4.291\n[2023-11-15 09:34:17][INFO][trainer.py:516] - step=750 loss=1.432 dt=27.775 sps=36.004 mtps=0.590 mfu=13.598 train_loss=4.299 val_loss=4.291\n[2023-11-15 09:34:24][INFO][trainer.py:516] - step=1000 loss=1.346 dt=26.781 sps=37.340 mtps=0.612 mfu=13.630 train_loss=4.299 val_loss=4.291\n[2023-11-15 09:34:28][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n[2023-11-15 09:34:28][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n[2023-11-15 09:34:28][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n[2023-11-15 09:34:35][INFO][trainer.py:516] - step=1250 loss=1.309 dt=27.473 sps=36.400 mtps=0.596 mfu=13.623 train_loss=1.271 val_loss=1.520\n[2023-11-15 09:34:42][INFO][trainer.py:516] - step=1500 loss=1.225 dt=27.261 sps=36.682 mtps=0.601 mfu=13.628 train_loss=1.271 val_loss=1.520\n[2023-11-15 09:34:49][INFO][trainer.py:516] - step=1750 loss=1.176 dt=26.890 sps=37.188 mtps=0.609 mfu=13.651 train_loss=1.271 val_loss=1.520\n[2023-11-15 09:34:56][INFO][trainer.py:516] - step=2000 loss=1.163 dt=26.727 sps=37.415 mtps=0.613 mfu=13.680 train_loss=1.271 val_loss=1.520\n[2023-11-15 09:35:00][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n[2023-11-15 09:35:00][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n[2023-11-15 09:35:00][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n[2023-11-15 09:35:07][INFO][trainer.py:516] - step=2250 loss=1.120 dt=26.733 sps=37.407 mtps=0.613 mfu=13.706 train_loss=1.052 val_loss=1.471\n[2023-11-15 09:35:14][INFO][trainer.py:516] - step=2500 loss=1.068 dt=27.096 sps=36.905 mtps=0.605 mfu=13.710 train_loss=1.052 val_loss=1.471\n[2023-11-15 09:35:21][INFO][trainer.py:516] - step=2750 loss=1.027 dt=26.879 sps=37.204 mtps=0.610 mfu=13.726 train_loss=1.052 val_loss=1.471\n[2023-11-15 09:35:27][INFO][trainer.py:516] - step=3000 loss=1.002 dt=27.375 sps=36.530 mtps=0.599 mfu=13.714 train_loss=1.052 val_loss=1.471\n[2023-11-15 09:35:32][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n[2023-11-15 09:35:32][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n[2023-11-15 09:35:32][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n[2023-11-15 09:35:39][INFO][trainer.py:516] - step=3250 loss=0.950 dt=26.866 sps=37.222 mtps=0.610 mfu=13.730 train_loss=0.864 val_loss=1.531\n[2023-11-15 09:35:45][INFO][trainer.py:516] - step=3500 loss=0.926 dt=27.330 sps=36.590 mtps=0.599 mfu=13.720 train_loss=0.864 val_loss=1.531\n[2023-11-15 09:35:52][INFO][trainer.py:516] - step=3750 loss=0.916 dt=27.203 sps=36.761 mtps=0.602 mfu=13.718 train_loss=0.864 val_loss=1.531\n[2023-11-15 09:35:59][INFO][trainer.py:516] - step=4000 loss=0.901 dt=27.394 sps=36.504 mtps=0.598 mfu=13.706 train_loss=0.864 val_loss=1.531\n[2023-11-15 09:36:03][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks\n[2023-11-15 09:36:03][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks/model.pth\n[2023-11-15 09:36:03][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/notebooks to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n[2023-11-15 09:36:10][INFO][trainer.py:516] - step=4250 loss=0.840 dt=26.814 sps=37.293 mtps=0.611 mfu=13.725 train_loss=0.703 val_loss=1.615\n[2023-11-15 09:36:17][INFO][trainer.py:516] - step=4500 loss=0.850 dt=27.402 sps=36.494 mtps=0.598 mfu=13.713 train_loss=0.703 val_loss=1.615\n[2023-11-15 09:36:24][INFO][trainer.py:516] - step=4750 loss=0.824 dt=26.811 sps=37.298 mtps=0.611 mfu=13.731 train_loss=0.703 val_loss=1.615\n[2023-11-15 09:36:30][INFO][trainer.py:516] - step=5000 loss=0.819 dt=27.435 sps=36.450 mtps=0.597 mfu=13.716 train_loss=0.703 val_loss=1.615\n\n\n[2023-11-11 01:16:27][INFO][trainer.py:516] - step=1000 loss=1.332 dt=26.899 sps=37.176 mtps=0.609 mfu=13.642 train_loss=4.299 val_loss=4.291\n\n\n[2023-11-11 01:16:34][INFO][trainer.py:516] - step=1250 loss=1.277 dt=27.229 sps=36.725 mtps=0.602 mfu=13.647 train_loss=4.299 val_loss=4.291\n\n\n[2023-11-11 01:16:40][INFO][trainer.py:516] - step=1500 loss=1.234 dt=26.878 sps=37.205 mtps=0.610 mfu=13.668 train_loss=4.299 val_loss=4.291\n\n\n[2023-11-11 01:16:47][INFO][trainer.py:516] - step=1750 loss=1.175 dt=27.460 sps=36.417 mtps=0.597 mfu=13.659 train_loss=4.299 val_loss=4.291\n\n\n[2023-11-11 01:16:54][INFO][trainer.py:516] - step=2000 loss=1.140 dt=26.889 sps=37.190 mtps=0.609 mfu=13.678 train_loss=4.299 val_loss=4.291\n\n\n[2023-11-11 01:16:58][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n\n\n[2023-11-11 01:16:58][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n\n\n[2023-11-11 01:16:58][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n\n\n[2023-11-11 01:17:05][INFO][trainer.py:516] - step=2250 loss=1.121 dt=27.308 sps=36.619 mtps=0.600 mfu=13.675 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:12][INFO][trainer.py:516] - step=2500 loss=1.067 dt=26.838 sps=37.261 mtps=0.610 mfu=13.696 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:19][INFO][trainer.py:516] - step=2750 loss=1.034 dt=27.360 sps=36.550 mtps=0.599 mfu=13.688 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:26][INFO][trainer.py:516] - step=3000 loss=1.009 dt=26.237 sps=38.114 mtps=0.624 mfu=13.740 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:33][INFO][trainer.py:516] - step=3250 loss=0.940 dt=26.991 sps=37.050 mtps=0.607 mfu=13.746 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:39][INFO][trainer.py:516] - step=3500 loss=0.947 dt=26.261 sps=38.080 mtps=0.624 mfu=13.791 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:46][INFO][trainer.py:516] - step=3750 loss=0.885 dt=37.216 sps=26.870 mtps=0.440 mfu=13.413 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:53][INFO][trainer.py:516] - step=4000 loss=0.866 dt=26.241 sps=38.108 mtps=0.624 mfu=13.492 train_loss=1.050 val_loss=1.474\n\n\n[2023-11-11 01:17:57][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n\n\n[2023-11-11 01:17:57][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n\n\n[2023-11-11 01:17:57][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n\n\n[2023-11-11 01:18:04][INFO][trainer.py:516] - step=4250 loss=0.847 dt=27.228 sps=36.728 mtps=0.602 mfu=13.511 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:11][INFO][trainer.py:516] - step=4500 loss=0.835 dt=26.215 sps=38.147 mtps=0.625 mfu=13.581 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:18][INFO][trainer.py:516] - step=4750 loss=0.822 dt=26.657 sps=37.513 mtps=0.615 mfu=13.621 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:24][INFO][trainer.py:516] - step=5000 loss=0.808 dt=26.635 sps=37.544 mtps=0.615 mfu=13.658 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:31][INFO][trainer.py:516] - step=5250 loss=0.811 dt=26.267 sps=38.071 mtps=0.624 mfu=13.711 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:38][INFO][trainer.py:516] - step=5500 loss=0.769 dt=26.406 sps=37.870 mtps=0.620 mfu=13.751 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:44][INFO][trainer.py:516] - step=5750 loss=0.780 dt=26.239 sps=38.111 mtps=0.624 mfu=13.796 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:51][INFO][trainer.py:516] - step=6000 loss=0.767 dt=26.682 sps=37.478 mtps=0.614 mfu=13.813 train_loss=0.696 val_loss=1.637\n\n\n[2023-11-11 01:18:55][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n\n\n[2023-11-11 01:18:55][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n\n\n[2023-11-11 01:18:56][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n\n\n[2023-11-11 01:19:02][INFO][trainer.py:516] - step=6250 loss=0.773 dt=31.104 sps=32.151 mtps=0.527 mfu=13.629 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:09][INFO][trainer.py:516] - step=6500 loss=0.759 dt=27.142 sps=36.843 mtps=0.604 mfu=13.639 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:16][INFO][trainer.py:516] - step=6750 loss=0.753 dt=26.712 sps=37.437 mtps=0.613 mfu=13.670 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:22][INFO][trainer.py:516] - step=7000 loss=0.745 dt=26.871 sps=37.215 mtps=0.610 mfu=13.690 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:29][INFO][trainer.py:516] - step=7250 loss=0.733 dt=26.266 sps=38.072 mtps=0.624 mfu=13.740 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:36][INFO][trainer.py:516] - step=7500 loss=0.723 dt=26.817 sps=37.289 mtps=0.611 mfu=13.755 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:43][INFO][trainer.py:516] - step=7750 loss=0.747 dt=26.461 sps=37.791 mtps=0.619 mfu=13.788 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:49][INFO][trainer.py:516] - step=8000 loss=0.729 dt=29.348 sps=34.074 mtps=0.558 mfu=13.679 train_loss=0.556 val_loss=1.755\n\n\n[2023-11-11 01:19:53][INFO][trainer.py:432] - Saving checkpoint to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt\n\n\n[2023-11-11 01:19:53][INFO][trainer.py:433] - Saving model to: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.pth\n\n\n[2023-11-11 01:19:54][INFO][configs.py:129] - Appending /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt to /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/checkpoints.log\n\n\n[2023-11-11 01:20:01][INFO][trainer.py:516] - step=8250 loss=0.718 dt=26.464 sps=37.787 mtps=0.619 mfu=13.719 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:07][INFO][trainer.py:516] - step=8500 loss=0.705 dt=27.051 sps=36.967 mtps=0.606 mfu=13.725 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:14][INFO][trainer.py:516] - step=8750 loss=0.704 dt=26.298 sps=38.026 mtps=0.623 mfu=13.769 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:21][INFO][trainer.py:516] - step=9000 loss=0.694 dt=27.131 sps=36.858 mtps=0.604 mfu=13.766 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:27][INFO][trainer.py:516] - step=9250 loss=0.700 dt=26.291 sps=38.036 mtps=0.623 mfu=13.806 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:34][INFO][trainer.py:516] - step=9500 loss=0.668 dt=27.353 sps=36.560 mtps=0.599 mfu=13.788 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:41][INFO][trainer.py:516] - step=9750 loss=0.658 dt=26.422 sps=37.847 mtps=0.620 mfu=13.819 train_loss=0.473 val_loss=1.840\n\n\n[2023-11-11 01:20:48][INFO][trainer.py:516] - step=10000 loss=0.678 dt=26.887 sps=37.192 mtps=0.609 mfu=13.823 train_loss=0.473 val_loss=1.840\nEvaluate Model\nquery = \"What is a supercomputer?\"\noutputs = trainer.evaluate(query, num_samples=1, display=False)\nconsole.print(fr'\\[prompt]: \"{query}\"')\nconsole.print(\"\\[response]:\\n\\n\" + fr\"{outputs['0']['raw']}\")\n\n[prompt]: \"What is a supercomputer?\"\n\n\n[response]:\n\nWhat is a supercomputer?\nHow now, now! what news?\nHave thy sons?\n\nMessenger:\nThe queen is his noble consul;\nThe man I am a lord's, and he received:\nTherefore, consider and the hand of death.\n\nSIR STEPHEN SCROOP:\nPeace, hope, my lord; I am not thy name;\nFor I have need of this cause is so long.\n\nBISHOP OF ELY:\nBelieve me, I will practise your majesty.\nBe remember thy thoughts: give me and brothers,\nAnd towards London, till I were common all.\n\nBUCKINGHAM:\nNorthumberland, so proud weighing to fight.\n\nGLOUCESTER:\nRelent, e"
  },
  {
    "objectID": "quarto/ngpt-shakespeare-1.html#install-setup",
    "href": "quarto/ngpt-shakespeare-1.html#install-setup",
    "title": "nanoGPT",
    "section": "Install / Setup",
    "text": "Install / Setup"
  },
  {
    "objectID": "quarto/ngpt-shakespeare-1.html#post-install",
    "href": "quarto/ngpt-shakespeare-1.html#post-install",
    "title": "nanoGPT",
    "section": "Post Install",
    "text": "Post Install\nIf installed correctly, you should be able to:\n&gt;&gt;&gt; import ngpt\n&gt;&gt;&gt; ngpt.__file__\n'/path/to/nanoGPT/src/ngpt/__init__.py'"
  },
  {
    "objectID": "quarto/ngpt-shakespeare-1.html#build-trainer",
    "href": "quarto/ngpt-shakespeare-1.html#build-trainer",
    "title": "nanoGPT",
    "section": "Build Trainer",
    "text": "Build Trainer\nExplicitly, we:\n\nsetup_torch(...)\nBuild cfg: DictConfig = get_config(...)\nInstnatiate config: ExperimentConfig = instantiate(cfg)\nBuild trainer = Trainer(config)"
  },
  {
    "objectID": "quarto/ngpt-shakespeare-1.html#prompt-prior-to-training",
    "href": "quarto/ngpt-shakespeare-1.html#prompt-prior-to-training",
    "title": "nanoGPT",
    "section": "Prompt (prior to training)",
    "text": "Prompt (prior to training)"
  },
  {
    "objectID": "quarto/ngpt-shakespeare-1.html#train-model",
    "href": "quarto/ngpt-shakespeare-1.html#train-model",
    "title": "nanoGPT",
    "section": "Train Model",
    "text": "Train Model\nLegend:\n\n\n\n\n\n\n\nname\ndescription\n\n\n\n\nstep\nCurrent training step\n\n\nloss\nLoss value\n\n\ndt\nTime per step (in ms)\n\n\nsps\nSamples per second\n\n\nmtps\n(million) Tokens per sec\n\n\nmfu\nModel Flops utilization[^1](in%20units%20of%20A100%20%60bfloat16%60%20peak%20FLOPS)"
  },
  {
    "objectID": "quarto/ngpt-shakespeare-1.html#evaluate-model",
    "href": "quarto/ngpt-shakespeare-1.html#evaluate-model",
    "title": "nanoGPT",
    "section": "Evaluate Model",
    "text": "Evaluate Model"
  },
  {
    "objectID": "quarto/gpt2-xl.html#footnotes",
    "href": "quarto/gpt2-xl.html#footnotes",
    "title": "GPT-2 XL",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin units of A100 bfloat16 peak FLOPS↩︎"
  },
  {
    "objectID": "quarto/gpt2-medium.html#footnotes",
    "href": "quarto/gpt2-medium.html#footnotes",
    "title": "nanoGPT",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin units of A100 bfloat16 peak FLOPS↩︎"
  },
  {
    "objectID": "quarto/shakespeare.html#contents",
    "href": "quarto/shakespeare.html#contents",
    "title": "Shakespeare",
    "section": "",
    "text": "Open In Collab\n\n\n\nnanoGPT\nInstall / Setup\nFirst Time Running\nPost Install\nBuild Trainer\nPrompt (prior to training)\nTrain Model\nEvaluate Model"
  }
]